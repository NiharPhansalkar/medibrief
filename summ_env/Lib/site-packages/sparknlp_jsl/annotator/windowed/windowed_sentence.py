from sparknlp_jsl.common import *

class WindowedParams:

    windowSize = Param(Params._dummy(), "windowSize", "size of the sliding window", typeConverter = TypeConverters.toInt)

    def setWindowSize(self, value):
        """Sets size of the sliding window

        Parameters
        ----------
        value : int
                size of the sliding window
        """
        return self._set(windowSize = value)

    glueString = Param(Params._dummy(), "glueString", "string to use to join the neighboring elements together", typeConverter = TypeConverters.toString)

    def setGlueString(self, value):
        """Sets string to use to join the neighboring elements together

        Parameters
        ----------
        value : string
                string to use to join the neighboring elements together
        """
        return self._set(glueString = value)


class WindowedSentenceModel(AnnotatorModelInternal, WindowedParams):
    """The `WindowedSentenceModel` class is used to combine a series of sentences based on specific window configurations.
       Returns the joined results after windowing its inputs

    ===========================================  ======================
    Input Annotation types                       Output Annotation type
    ===========================================  ======================
    ``DOCUMENT``                                    ``DOCUMENT``
    ===========================================  ======================

    Parameters
    ----------
    windowSize
        size of the sliding window
    glueString
        string to use to join the neighboring elements together

    Examples
    --------

    >>> import sparknlp
    >>> from sparknlp.base import *
    >>> from sparknlp_jsl.common import *
    >>> from sparknlp.annotator import *
    >>> from sparknlp.training import *
    >>> import sparknlp_jsl
    >>> from sparknlp_jsl.base import *
    >>> from sparknlp_jsl.annotator import *
    >>> from pyspark.ml import Pipeline
    >>>   documentAssembler = DocumentAssembler()\
    ...       .setInputCol("text")\
    ...       .setOutputCol("document")
    >>>    sentenceDetector = SentenceDetector()\
    ...       .setInputCols("document")\
    ...       .setOutputCol("sentence")
    >>>    windowedSentence = WindowedSentenceModel()\
    ...       .setWindowSize(1)\
    ...       .setInputCols("sentence")\
    ...       .setOutputCol("five")\
    ...       .setGlueString(":::")\

    >>>   flattener = Flattener()\
    ...        .setInputCols("five")

    >>>   pipeline = Pipeline(stages=[documentAssembler,sentenceDetector,windowedSentence,flattener])

    >>>   data = spark.createDataFrame([["A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years.Two weeks prior to presentation respiratory tract infection.She was on for HTG .  She had been of presentation . examination benign with no  or rigidity ."]]).toDF("text")

    >>>    model = pipeline.fit(data).transform(data)
    >>>    model.show(truncate=False)

    """

    inputAnnotatorTypes = [AnnotatorType.DOCUMENT]
    outputAnnotatorType = AnnotatorType.DOCUMENT

    def __init__(self, classname = "com.johnsnowlabs.nlp.annotators.windowed.WindowedSentenceModel", java_model = None):
        super(WindowedSentenceModel, self).__init__(classname = classname, java_model = java_model)
