#  Copyright 2017-2022 John Snow Labs
#
#  Licensed under the Apache License, Version 2.0 (the "License");
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
#  limitations under the License.
"""Contains classes for MultiChunk2Doc."""
from sparknlp_jsl.common import *
from sparknlp_jsl.annotator.white_black_list_params import WhiteBlackListParams


class MultiChunk2Doc(AnnotatorModelInternal, WhiteBlackListParams):
    """MultiChunk2Doc annotator merges a given chunks to create a document.
    During document creation, a specific whitelist and blacklist filter can be applied, and case sensitivity can be adjusted.
    See Also: :class:`sparknlp_jsl.annotator.WhiteBlackListParams` for filtering options.

    Additionally, specified prefix and suffix texts can be placed before and after the merged chunks in the resulting document.
    And a separator can be placed between the chunks.

    Converts ``CHUNK`` type annotations into ``DOCUMENT`` type

        ====================== ======================
        Input Annotation types Output Annotation type
        ====================== ======================
        ``CHUNK``              ``DOCUMENT``
        ====================== ======================

    Examples
    --------
    >>> import sparknlp
    >>> from sparknlp.base import *
    >>> from sparknlp.annotator import *
    >>> from sparknlp_jsl.annotator import *
    >>> from pyspark.ml import Pipeline
    >>> documentAssembler = DocumentAssembler().setInputCol("text").setOutputCol("document")
    >>> sentenceDetector = SentenceDetectorDLModel.pretrained("sentence_detector_dl_healthcare", "en", "clinical/models") \\
    ...     .setInputCols("document").setOutputCol("sentence")
    >>> tokenizer = Tokenizer().setInputCols("sentence").setOutputCol("token")
    >>> embeddings = WordEmbeddingsModel.pretrained("embeddings_clinical", "en", "clinical/models") \\
    ...     .setInputCols("sentence", "token").setOutputCol("embeddings")
    >>> clinical_ner = MedicalNerModel.pretrained("ner_clinical_large_langtest", "en", "clinical/models") \\
    ...     .setInputCols(["sentence", "token", "embeddings"]).setOutputCol("ner")
    >>> ner_converter = NerConverterInternal().setInputCols(["sentence", "token", "ner"]).setOutputCol("ner_chunk")
    >>> multi_chunk2_doc = MultiChunk2Doc() \\
    ...     .setInputCols(["ner_chunk"]) \\
    ...     .setOutputCol("new_document") \\
    ...     .setWhiteList(["test"]) \\
    ...     .setCaseSensitive(False) \\
    ...     .setPrefix("<") \\
    ...     .setSeparator("><") \\
    ...     .setSuffix(">")
    >>> data = spark.createDataFrame([["A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation " + \\
    ...     "and subsequent type two diabetes mellitus (T2DM), " + \\
    ...     "one prior episode of HTG-induced pancreatitis three years prior to presentation, " + \\
    ...     "and associated with an acute hepatitis, presented with a one-week history of polyuria, poor appetite, and vomiting. " + \\
    ...     "She was on metformin, glipizide, and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG. " + \\
    ...     "She had been on dapagliflozin for six months at the time of presentation. " + \\
    ...     "Physical examination on presentation was significant for dry oral mucosa ; " + \\
    ...     "significantly , her abdominal examination was benign with no tenderness, guarding, or rigidity."]]) \\
    ...     .toDF("text")
    >>> pipeline = Pipeline() \\
    ...     .setStages([documentAssembler, sentenceDetector, tokenizer, embeddings, clinical_ner, ner_converter, multi_chunk2_doc]) \\
    ...     .fit(data)
    >>> result = pipeline.transform(data)
    >>> result.selectExpr("new_document").show(truncate=False)

    +------------------------------------------------------------------------------------------------------------+
    |new_document                                                                                                |
    +------------------------------------------------------------------------------------------------------------+
    |[{document, 0, 48, <Physical examination><her abdominal examination>, {document -> 0, chunk_count -> 2}, []}]|
    +------------------------------------------------------------------------------------------------------------+

    """
    name = "MultiChunk2Doc"
    inputAnnotatorTypes = [AnnotatorType.CHUNK]
    outputAnnotatorType = AnnotatorType.DOCUMENT

    separator = Param(Params._dummy(), "separator",
                      "Separator to add between the chunks",
                      typeConverter=TypeConverters.toString)
    prefix = Param(Params._dummy(), "prefix",
                   "Prefix to add to the result",
                   typeConverter=TypeConverters.toString)
    suffix = Param(Params._dummy(), "suffix",
                   "Suffix to add to the result",
                   typeConverter=TypeConverters.toString)

    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.MultiChunk2Doc", java_model=None):
        super(MultiChunk2Doc, self).__init__(
            classname=classname,
            java_model=java_model
        )

    def setSeparator(self, value: str):
        """Sets the separator to add between the chunks. Default: ",".

        Parameters
        ----------
        value : str
            the separator to add between the chunks. Default: ",".
        """
        return self._set(separator=value)

    def setPrefix(self, value: str):
        """Sets the prefix to add to the result. Default: "".

        Parameters
        ----------
        value : str
            the prefix to add to the result. Default: "".
        """
        return self._set(prefix=value)

    def setSuffix(self, value: str):
        """Sets the suffix to add to the result. Default: "".

        Parameters
        ----------
        value : str
            the suffix to add to the result. Default: "".
        """
        return self._set(suffix=value)
