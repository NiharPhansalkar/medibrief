from sparknlp_jsl.annotator.handle_exception_params import HandleExceptionParams
from sparknlp_jsl.common import *


class SentenceResolverParams(HasCaseSensitiveProperties, HandleExceptionParams):
    """Common interface for the Sentence Resolver family.

    Parameters
    ----------

    distanceFunction : str
        What distance function to use for Word Mover's Distance (WMD). 
        Either 'EUCLIDEAN' or 'COSINE'.
    neighbours : int
        Number of neighbours to consider in the KNN algorithmm query to calculate
        Word Mover's Distance (WMD).
    threshold : float
        Threshold value for the distance calculated in the search tree.
        This represents the maximum distance between the query and the
        nearest neighbour. Neighbours with a distance greater than the
        threshold will not be returned.
    confidenceFunction : str
        What function to use to calculate confidence. Either 'INVERSE' or 'SOFTMAX'.
    missAsEmpty : bool
        Whether or not to return an empty annotation on unmatched chunks.
    returnResolvedTextEmbeddings : bool
        Whether to include embeddings for resolved text embeddings.(Default : False)
    datasetInfo
        Descriptive information about the dataset being used.
    """
    inputAnnotatorTypes = [AnnotatorType.SENTENCE_EMBEDDINGS]

    distanceFunction = Param(Params._dummy(), "distanceFunction",
                             "What distance function to use for Word Mover's Distance (WMD). Either 'EUCLIDEAN' or 'COSINE'", TypeConverters.toString)
    neighbours = Param(Params._dummy(), "neighbours",
                       "Number of neighbours to consider in the KNN query to calculate Word Mover's Distance (WMD)", TypeConverters.toInt)
    threshold = Param(Params._dummy(), "threshold", "Threshold value for the last distance calculated",
                      TypeConverters.toFloat)
    confidenceFunction = Param(Params._dummy(), "confidenceFunction",
                               "what function to use to calculate confidence: Either 'INVERSE' or 'SOFTMAX'.",
                               typeConverter=TypeConverters.toString)
    missAsEmpty = Param(Params._dummy(), "missAsEmpty",
                        "whether or not to return an empty annotation on unmatched chunks",
                        typeConverter=TypeConverters.toBoolean)
    returnResolvedTextEmbeddings = Param(Params._dummy(), "returnResolvedTextEmbeddings",
                                         "whether to include embeddings for resolved text candidates.  If set to true, embeddings will be included; if set to false, embeddings will be excluded.(Default : False) ",
                                         typeConverter=TypeConverters.toBoolean)
    datasetInfo = Param(Params._dummy(), "datasetInfo",
                               "Descriptive information about the dataset being used",
                               typeConverter=TypeConverters.toString)


    def setDistanceFunction(self, dist:str):
        """Sets distance function to use for Word Mover's Distance (WMD): 'EUCLIDEAN' or 'COSINE'.

        Parameters
        ----------
        dist : str
            Value that selects what distance function to use for WMD: 'EUCLIDEAN' or 'COSINE'.
        """
        return self._set(distanceFunction=dist)

    def setNeighbours(self, k:int):
        """Sets the number of neighbours to consider in the KNN algorithmn.

        Parameters
        ----------
        k : int
            Number of neighbours to consider in the KNN query to calculate Word Mover's Distance (WMD).
        """
        return self._set(neighbours=k)

    def setThreshold(self, thres:float):
        """Sets Threshold value for the last distance calculated.

        Parameters
        ----------
        thres : float
            Threshold value for the last distance calculated.
        """
        return self._set(threshold=thres)

    def setConfidenceFunction(self, conf_function:str):
        """What function to use to calculate confidence: Either 'INVERSE' or 'SOFTMAX'.

        Parameters
        ----------
        conf_function : str
            What function to use to calculate confidence: Either 'INVERSE' or 'SOFTMAX'.
        """
        return self._set(confidenceFunction=conf_function)

    def setMissAsEmpty(self, value:bool):
        """Sets whether or not to return an empty annotation on unmatched chunks.

        Parameters
        ----------
        value : bool
            whether or not to return an empty annotation on unmatched chunks.
        """
        return self._set(missAsEmpty=value)

    def setReturnResolvedTextEmbeddings(self, value:bool):
        """Sets whether to include embeddings for resolved text candidates.

        If set to true, embeddings will be included; if set to false, embeddings will be excluded.
        (Default : False)

        Parameters
        ----------
        value : bool
            Whether to include embeddings for resolved text embeddings.
        """
        return self._set(returnResolvedTextEmbeddings=value)

    def setDatasetInfo(self, info:str):
        """Sets descriptive information about the dataset being used.

        Parameters
        ----------
        info : str
            Descriptive information about the dataset being used.
        """
        return self._set(datasetInfo=info)


class SentenceEntityResolverApproach(AnnotatorApproachInternal, SentenceResolverParams, HasEngine):
    """Trains a SentenceEntityResolverModel.

    The SentenceEntityResolverModel maps sentence embeddings to entities in a knowledge base.
    To train a custom model, you need to provide a dataset with the following columns:

    - label: Entity name
    - chunk: Occurrence of the entity in the text, without standartization
    - sentence_embeddings: Sentence embeddings from, e.g., the BertSentenceEmbeddings annotator.
    
    Optionally, you can also provide the ``aux_label`` column, containing auxiliary label which
    maps resolved entities to additional labels. If you have ground truth of the knowledge base
    entities, setting this column will help the model to learn better.

    To continue the training of an already trained model, you can use the ``pretrainedModelPath``
    parameter. This will load the pretrained model and continue the training process.
    To override the codes in the pretrained model, set the ``overrideExistingCodes`` parameter
    to ``True``. To define a list of unwanted labels in the pretrained model, set the
    ``dropCodesList`` parameter with the list of labels to ignore.
    
    You can find pretrained Sentence Embeddings (using BERT or other flavour) in the
    `NLP Models Hub <https://nlp.johnsnowlabs.com/models?task=Embeddings>`_.

    ========================================= ======================
    Input Annotation types                    Output Annotation type
    ========================================= ======================
    ``SENTENCE_EMBEDDINGS``                    ``ENTITY``
    ========================================= ======================

    Parameters
    ----------
    labelCol: str
        Column name for the value we are trying to resolve. Usually this contains the entity ID
        in the knowledge base (e.g., the ICD-10 code).
    normalizedCol: str
        Column name for the original, normalized description
    pretrainedModelPath: str
        Path to an already trained SentenceEntityResolverModel.
        This pretrained model will be used as a starting point
        for training the new one. The path can be a local file path,
        a distributed file path (HDFS, DBFS), or a cloud storage (S3).
    overrideExistingCodes: bool
        Whether to override the existing codes with new data while continue the training from a pretrained model. Default value is false(keep all the codes).
    returnCosineDistances: bool
        Extract Cosine Distances. True or False
    aux_label_col: str
        Auxiliary label which maps resolved entities to additional labels
    useAuxLabel: bool
        Whether to use the auxiliary column or not. Default value is False.
    overrideExistingCodes: bool
        Whether to override the codes present in a pretrained model with new codes when the training process begins with a pretrained model
    dropCodesList: list
        A list of codes in a pretrained model that will be omitted when the training process begins with a pretrained model.

    Examples
    --------

    Let  ``data``  be a spark DataFrame with the required columns:
        - conceptId: Concept ID of the knowledge base (SNOMED). Will be used as `label` column.
        - ground_truth: Ground truth of the conceptId present in the knowledge base. Can be used as auxiliary column.
        - concept_name: Chunk of text identified in the text data that we want to map to the knowledge base.
    
    +----------------+----------------------+----------------------+
    | conceptId      | ground_truth         | concept_name         |
    +----------------+----------------------+----------------------+
    | 108367008      | Dislocation of joint | Dislocation of joint |
    +----------------+----------------------+----------------------+
    |3384011000036100|      Arthrotec       | Arthrotec            |
    +----------------+----------------------+----------------------+
    | 166717003      | Serum creatinine     | Serum creatinine     |
    +----------------+----------------------+----------------------+
    |3877011000036101| Lipitor              | Lipitor              |
    +----------------+----------------------+----------------------+
    | 402234004      | Foot eczema          | Foot eczema          |
    +----------------+----------------------+----------------------+

    Then, we can train a SentenceEntityResolverModel as follows:

    >>> documentAssembler = nlp.DocumentAssembler().setInputCol("text").setOutputCol("document")
    >>> sentenceDetector = nlp.SentenceDetector().setInputCols(["document"]).setOutputCol("sentence")
    >>> tokenizer = nlp.Tokenizer().setInputCols(["sentence"]).setOutputCol("token")
    >>> bertEmbeddings = (
    ...     nlp.BertSentenceEmbeddings.pretrained("sent_biobert_pubmed_base_cased")
    ...     .setInputCols(["sentence"])
    ...     .setOutputCol("embeddings")
    ... )
    >>> data_pipeline = nlp.Pipeline(stages=[
    ...    documentAssembler,
    ...    sentenceDetector,
    ...    bertEmbeddings,
    ... ])
    >>> data_processing_model = data_pipeline.fit(data)
    >>> prepared_data = data_processing_model.transform(data)
    >>> bertExtractor = (
    ...     medical.SentenceEntityResolverApproach()
    ...     .setNeighbours(25)
    ...     .setThreshold(1000)
    ...     .setInputCols(["bert_embeddings"])
    ...     .setNormalizedCol("normalized_text")
    ...     .setLabelCol("label")
    ...     .setOutputCol("snomed_code")
    ...     .setDistanceFunction("EUCLIDIAN")
    ...     .setCaseSensitive(False)
    ...     .setUseAuxLabel(True)
    ...     .setAuxLabelCol("ground_truth")
    ...     )
    >>> snomedModel = bertExtractor.fit(prepared_data)
    """
    inputAnnotatorTypes = [AnnotatorType.SENTENCE_EMBEDDINGS]
    outputAnnotatorType = AnnotatorType.ENTITY

    labelCol = Param(Params._dummy(), "labelCol", "Column name for the value we are trying to resolve",
                     typeConverter=TypeConverters.toString)
    normalizedCol = Param(Params._dummy(), "normalizedCol", "Column name for the original, normalized description",
                          typeConverter=TypeConverters.toString)

    pretrainedModelPath = Param(Params._dummy(), "pretrainedModelPath",
                                "Path to an already trained SentenceEntityResolverModel, which is used as a starting point for training the new model.",
                                typeConverter=TypeConverters.toString)

    overrideExistingCodes = Param(Params._dummy(), "overrideExistingCodes",
                                  "Whether to override the existing codes with new data while continue the training from a pretrained model. Default value is false(keep all the codes).",
                                  typeConverter=TypeConverters.toBoolean)

    dropCodesList = Param(Params._dummy(), "dropCodesList",
                          "List of codes in a pretrained model to leave out when continue training with new data.",
                          typeConverter=TypeConverters.toListString)

    returnCosineDistances = Param(Params._dummy(), "returnCosineDistances",
                                  "Extract Cosine Distances. TRUE or False",
                                  typeConverter=TypeConverters.toBoolean)
    aux_label_col = Param(Params._dummy(), "aux_label_col",
                          "Auxiliary label which maps resolved entities to additional labels",
                          typeConverter=TypeConverters.toString)

    useAuxLabel = Param(Params._dummy(), "useAuxLabel",
                        "Use AuxLabel Col or not",
                        typeConverter=TypeConverters.toBoolean)

    def setUseAuxLabel(self, name:bool):
        """Sets Use AuxLabel Col or not.

        Parameters
        ----------
        name : bool
            Use AuxLabel Col or not.
        """
        return self._set(useAuxLabel=name)

    def setAuxLabelCol(self, name:str):
        """Sets auxiliary label which maps resolved entities to additional labels

        Parameters
        ----------
        name : str
            Auxiliary label which maps resolved entities to additional labels
        """
        return self._set(aux_label_col=name)

    def setExtractCosineDistances(self, name:bool):
        """Extract Cosine Distances. True or False.

        Parameters
        ----------
        name : bool
            Extract Cosine Distances. True or False
        """
        return self._set(returnCosineDistances=name)

    def setLabelCol(self, name:str):
        """Sets column name for the value we are trying to resolve

        Parameters
        ----------
        name : str
            Column name for the value we are trying to resolve
        """
        return self._set(labelCol=name)

    def setNormalizedCol(self, name:str):
        """Sets column name for the original, normalized description

        Parameters
        ----------
        name : str
            Column name for the original, normalized description
        """
        return self._set(normalizedCol=name)

    def setPretrainedModelPath(self, path:str):
        """Sets path to an already trained SentenceEntityResolverModel.

        Parameters
        ----------
        path : str
            Path to an already trained SentenceEntityResolverModel, which is used as a starting point for training the new model.
        """
        return self._set(pretrainedModelPath=path)

    def setOverrideExistingCodes(self, value:bool):
        """Sets whether to override the existing codes with new data while continue the training from a pretrained model.

        Parameters
        ----------
        value : bool
            Whether to override the existing codes with new data while continue the training from a pretrained model. Default value is false(keep all the codes).
        """
        return self._set(overrideExistingCodes=value)

    def setDropCodesList(self, value:list):
        """Sets list of codes to leave out when continue training with new data.

        Parameters
        ----------
        value : list
            List of codes in a pretrained model to leave out when continue training with new data.
        """
        return self._set(dropCodesList=value)

    def _create_model(self, java_model:str):
        """Creates the model using the java model.

        Parameters
        ----------
            java_model : str
                The name of the java model.
        """
        return SentenceEntityResolverModel(java_model=java_model)

    @keyword_only
    def __init__(self):
        super(SentenceEntityResolverApproach, self).__init__(
            classname="com.johnsnowlabs.nlp.annotators.resolution.SentenceEntityResolverApproach")
        self._setDefault(labelCol="code", normalizedCol="code", distanceFunction="EUCLIDEAN", neighbours=500,
                         threshold=5, missAsEmpty=True, returnCosineDistances=True)


class SentenceEntityResolverModel(AnnotatorModelInternal, HasEmbeddingsProperties, HasStorageModel, SentenceResolverParams,
                                  HasEngine):
    """Extracts entities from sentence embeddings and resolves them to a particular ontology / curated dataset.
    
    Transforms a dataset with Input Annotation type SENTENCE_EMBEDDINGS, coming from e.g.
    [BertSentenceEmbeddings](/docs/en/transformers#bertsentenceembeddings)
    and returns the normalized entity for a particular trained ontology / curated dataset.
    (e.g. ICD-10, RxNorm, SNOMED etc.).

    For a list of pretrained models, please see the `NLP Models Hub <https://nlp.johnsnowlabs.com/models?task=Entity+Resolution>`_.

    ========================================= ======================
    Input Annotation types                    Output Annotation type
    ========================================= ======================
    ``SENTENCE_EMBEDDINGS``                   ``ENTITY``
    ========================================= ======================

    Parameters
    ----------
    returnCosineDistances: bool
        Whether to extract Cosine Distances. Either True or False.
    useAuxLabel: bool
        Whether to use the auxiliary column or not. Either True or False
    searchTree: StructFeature_HadoopFix
        An encapsulated instance of the SerializableKDTree class.
        The search tree is used to find the
        nearest neighbours considering the distance function by using the
        multidimensional binary search tree approch.

        This parameter is used internally by the SentenceEntityResolverModel,
        and usually the user don't need to set it.

        Reference:
        > Jon Louis Bentley. 1975.
        > Multidimensional binary search trees used for associative searching.
        > Commun. ACM 18, 9 (Sept. 1975), 509–517.
        > https://doi.org/10.1145/361002.361007

    Examples
    --------

    >>> documentAssembler = nlp.DocumentAssembler().setInputCol("text").setOutputCol("document")
    >>> sentenceDetector = nlp.SentenceDetector().setInputCols(["document"]).setOutputCol("sentence")
    >>> tokenizer = nlp.Tokenizer().setInputCols(["sentence"]).setOutputCol("token")
    >>> bertEmbeddings = (
    ...     nlp.BertSentenceEmbeddings.pretrained("sent_biobert_pubmed_base_cased")
    ...     .setInputCols(["sentence"])
    ...     .setOutputCol("embeddings")
    ... )
    >>> bertExtractor = (
    ...     medical.SentenceEntityResolverModel
    ...     .pretrained("sbertresolve_icd10cm_slim_billable_hcc_med", "en", "clinical/models")
    ...     .setInputCols(["embeddings"])
    ...     .setOutputCol("icd10_code")
    ...     .setDistanceFunction("EUCLIDEAN")
    ... )
    
    >>> icd10ExtractorPipeline = nlp.Pipeline(stages=[
    ...    documentAssembler,
    ...    sentenceDetector,
    ...    bertEmbeddings,
    ...    bertExtractor
    ... ])
    
    >>> empty_data = spark.createDataFrame([[""]]).toDF("text")
    >>> icd10Model = icd10ExtractorPipeline.fit(empty_data)
    >>> results = icd10Model.transform(data)
    """
    inputAnnotatorTypes = [AnnotatorType.SENTENCE_EMBEDDINGS]
    outputAnnotatorType = AnnotatorType.ENTITY

    name = "SentenceEntityResolverModel"
    returnCosineDistances = Param(Params._dummy(), "returnCosineDistances",
                                  "Extract Cosine Distances. TRUE or False",
                                  typeConverter=TypeConverters.toBoolean)

    useAuxLabel = Param(Params._dummy(), "useAuxLabel",
                        "Use AuxLabel Col or not",
                        typeConverter=TypeConverters.toBoolean)
    searchTree = Param(Params._dummy(), "searchTree", "Search tree for resolution", TypeConverters.identity)

    ##TODO add SetreturnCosineDistances.

    def setUseAuxLabel(self, name: bool):
        """Sets Use AuxLabel Col or not.

        Parameters
        ----------
        name : bool
            Use AuxLabel Col or not.
        """
        return self._set(useAuxLabel=name)


    def setSearchTree(self, search_tree):
        """Sets the search tree to use.

        Usualy the user don't need to set this parameter, as it uses an internal
        encapsulated search tree based on KDTree.

        Parameters
        ----------
        search_tree: SerializableKDTree
            The search tree to use. Should be an instance of SerializableKDTree.
        """
        return self._set(searchTree=search_tree)

    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.resolution.SentenceEntityResolverModel",
                 java_model=None):
        super(SentenceEntityResolverModel, self).__init__(
            classname=classname,
            java_model=java_model
        )

    @staticmethod
    def pretrained(name="sbiobertresolve_icd10cm_augmented_billable_hcc", lang="en", remote_loc="clinical/models"):
        """Downloads and loads a pretrained model.

        Parameters
        ----------
        name : str, optional
            Name of the pretrained model, by default "sbiobertresolve_icd10cm_augmented_billable_hcc"
        lang : str, optional
            Language of the pretrained model, by default "en"
        remote_loc : str, optional
            Optional remote address of the resource, by default None. Will use
            Spark NLPs repositories otherwise.

        Returns
        -------
        SentenceEntityResolverModel
            The restored model
        """
        from sparknlp_jsl.pretrained import InternalResourceDownloader
        return InternalResourceDownloader.downloadModel(SentenceEntityResolverModel, name, lang, remote_loc,
                                                        j_dwn='InternalsPythonResourceDownloader')
