from sparknlp_jsl.annotator.handle_exception_params import HandleExceptionParams
from sparknlp_jsl.common import *
from sparknlp.internal import ExtendedJavaWrapper


class _RelationExtractionDLModelLoader(ExtendedJavaWrapper,HasEngine):
    """Wrapper for the RelationExtractionDLModelLoader class.

    Notes:
        This class is not meant to be instantiated directly.
        Instead, use the :py:method:`RelationExtractionDLModel.load` or
        :py:method:`RelationExtractionDLModel.loadSavedModel` methods.
    """
    def __init__(self, path, jspark):
        super(_RelationExtractionDLModelLoader, self).__init__(
            "com.johnsnowlabs.nlp.annotators.re.RelationExtractionDLModel.loadSavedModel", path, jspark)


class RelationExtractionDLModel(AnnotatorModelInternal,HandleExceptionParams):
    """Extracts and classifies instances of relations between named entities.

    In contrast with RelationExtractionModel, RelationExtractionDLModel is based on BERT.
    For pretrained models please see the `NLP Models Hub <https://nlp.johnsnowlabs.com/models?task=Relation+Extraction>`_.

    ==========================================  ======================
    Input Annotation types                      Output Annotation type
    ==========================================  ======================
    ``CHUNK, DOCUMENT``                         ``CATEGORY``
    ==========================================  ======================

    Parameters
    ----------
    predictionThreshold
        Minimal activation of the target unit to encode a new relation instance. Default is 0.5.
    batchSize
        Number of relations to process at once. Default is 10.
    customLabels
        List of custom labels to use.

    Examples
    --------

    >>> import sparknlp
    >>> from sparknlp.base import *
    >>> from sparknlp_jsl.common import *
    >>> from sparknlp.annotator import *
    >>> from sparknlp.training import *
    >>> import sparknlp_jsl
    >>> from sparknlp_jsl.base import *
    >>> from sparknlp_jsl.annotator import *
    >>> from pyspark.ml import Pipeline
    >>> documentAssembler = DocumentAssembler() \\
    ...   .setInputCol("text") \\
    ...   .setOutputCol("document")
    ...
    >>> tokenizer = Tokenizer() \\
    ...   .setInputCols(["document"]) \\
    ...   .setOutputCol("tokens")
    ...
    >>> embedder = WordEmbeddingsModel \\
    ...   .pretrained("embeddings_clinical", "en", "clinical/models") \\
    ...   .setInputCols(["document", "tokens"]) \\
    ...   .setOutputCol("embeddings")
    ...
    >>> posTagger = PerceptronModel \\
    ...   .pretrained("pos_clinical", "en", "clinical/models") \\
    ...   .setInputCols(["document", "tokens"]) \\
    ...   .setOutputCol("posTags")
    ...
    >>> nerTagger = MedicalNerModel \\
    ...   .pretrained("ner_events_clinical", "en", "clinical/models") \\
    ...   .setInputCols(["document", "tokens", "embeddings"]) \\
    ...   .setOutputCol("ner_tags")
    ...
    >>> nerConverter = NerConverter() \\
    ...   .setInputCols(["document", "tokens", "ner_tags"]) \\
    ...   .setOutputCol("nerChunks")
    ...
    >>> depencyParser = DependencyParserModel \\
    ...   .pretrained("dependency_conllu", "en") \\
    ...   .setInputCols(["document", "posTags", "tokens"]) \\
    ...   .setOutputCol("dependencies")
    ...
    >>> relationPairs = [
    ...   "direction-external_body_part_or_region",
    ...   "external_body_part_or_region-direction",
    ...   "direction-internal_organ_or_component",
    ...   "internal_organ_or_component-direction"
    ... ]
    >>> re_ner_chunk_filter = RENerChunksFilter()\\
    ...   .setInputCols(["ner_chunks", "dependencies"])\\
    ...   .setOutputCol("re_ner_chunks")\\
    ...   .setMaxSyntacticDistance(4)\\
    ...   .setRelationPairs(["internal_organ_or_component-direction"])
    ...
    >>> re_model = RelationExtractionDLModel.pretrained("redl_bodypart_direction_biobert", "en", "clinical/models") \\
    ...     .setInputCols(["re_ner_chunks", "sentences"]) \\
    ...     .setOutputCol("relations") \\
    ...     .setPredictionThreshold(0.5)
    ...
    >>> pipeline = Pipeline(stages=[
    ...     documentAssembler,
    ...     tokenizer,
    ...     embedder,
    ...     posTagger,
    ...     nerTagger,
    ...     nerConverter,
    ...     depencyParser,
    ...     re_ner_chunk_filter ,
    ...     re_model])

    >>> model = pipeline.fit(trainData)
    >>> data = spark.createDataFrame([["MRI demonstrated infarction in the upper brain stem , left cerebellum and  right basil ganglia"]]).toDF("text")
    >>> result = pipeline.fit(data).transform(data)
    ...
    >>> result.selectExpr("explode(relations) as relations")\\
    ...  .select(
    ...    "relations.metadata.chunk1",
    ...    "relations.metadata.entity1",
    ...    "relations.metadata.chunk2",
    ...    "relations.metadata.entity2",
    ...    "relations.result"
    ...  )\\
    ...  .where("result != 0")\\
    ...  .show(truncate=False)
    ...
    ... # Show results
    ... result.selectExpr("explode(relations) as relations") \\
    ...   .select(
    ...      "relations.metadata.chunk1",
    ...      "relations.metadata.entity1",
    ...      "relations.metadata.chunk2",
    ...      "relations.metadata.entity2",
    ...      "relations.result"
    ...   ).where("result != 0") \\
    ...   .show(truncate=False)
    +------+---------+-------------+---------------------------+------+
    |chunk1|entity1  |chunk2       |entity2                    |result|
    +------+---------+-------------+---------------------------+------+
    |upper |Direction|brain stem   |Internal_organ_or_component|1     |
    |left  |Direction|cerebellum   |Internal_organ_or_component|1     |
    |right |Direction|basil ganglia|Internal_organ_or_component|1     |
    +------+---------+-------------+---------------------------+------+
    """
    inputAnnotatorTypes = [AnnotatorType.CHUNK, AnnotatorType.DOCUMENT]
    outputAnnotatorType = AnnotatorType.CATEGORY

    name = "RelationExtractionDLModel"

    predictionThreshold = Param(Params._dummy(), "predictionThreshold",
                                "Minimal activation of the target unit to encode a new relation instance",
                                TypeConverters.toFloat)
    batchSize = Param(Params._dummy(), "batchSize", "Number of relations to process at once", TypeConverters.toInt)

    classes = Param(Params._dummy(), "classes", "Categorization classes", TypeConverters.toListString)
    
    customLabels = Param(Params._dummy(), "customLabels",
                         "Custom relation labels",
                         TypeConverters.identity)
    relationPairsCaseSensitive = Param(Params._dummy(), "relationPairsCaseSensitive", "Determines whether relation pairs are case sensitive",
                                       TypeConverters.toBoolean)

    def setPredictionThreshold(self, threshold: float):
        """Sets minimal activation of the target unit to encode a new relation instance. Default is 0.5.

        Parameters
        ----------
        threshold : float
           Minimal activation of the target unit to encode a new relation instance. Default is 0.5.
        """
        return self._set(predictionThreshold=threshold)

    # TODO set up this value
    def setCaseSensitive(self, value: bool):
        """Sets case sensitivity.

        Parameters
        ----------
        value : bool
            True if case sensitive, False otherwise
        """
        return self._set(caseSensitive=value)

    def setBatchSize(self, value: int):
        """Sets number of relations to process at once

        Parameters
        ----------
        value : int
           Number of relations to process at once
        """
        return self._set(batchSize=value)

    def setCustomLabels(self, labels: dict):
        """Sets custom relation labels

        Parameters
        ----------
        labels : dict[str, str]
            Dictionary which maps old to new labels
        """
        self._call_java("setCustomLabels",labels)
        return self

    def setRelationTypePerPair(self, relationTypePairs):
        """Set the list of entity pairs allowed for a given relation

        Parameters
        ----------
        relationTypePairs : dict[str, list[str]]

        """

        self._call_java("setRelationTypePerPair", relationTypePairs)
        return self

    def getRelationTypePerPair(self):
        """
        Return the list of entity pairs allowed for a given relation
        """

        rel_pairs = self._call_java("getRelationTypePerPairStr")
        rel_pairs_dict = {}
        for t in rel_pairs.split("\n"):
            parts = t.split(":")
            if len(parts) == 2:
                rel, pairs = parts
                pairs = pairs.split(",")
                rel_pairs_dict[rel] = pairs

        return rel_pairs_dict

    def setRelationPairsCaseSensitive(self, value:bool):
        """Sets the case sensitivity of relation pairs
        Parameters
        ----------
        value : bool
            whether relation pairs are case sensitive
        """
        return self._set(relationPairsCaseSensitive=value)

    def getClasses(self):
        """Returns labels used during training.
        """
        return self._call_java("getClasses")

    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.re.RelationExtractionDLModel",
                 java_model=None):
        super(RelationExtractionDLModel, self).__init__(
            classname=classname,
            java_model=java_model
        )
        self._setDefault(
            batchSize=10
        )

    @staticmethod
    def loadSavedModel(folder, spark_session):
        """Load a pre-trained RelationExtractionDLModel.

        Args:
            folder (str): Folder containing the pre-trained model.
            spark_session (SparkSession): SparkSession.

        Returns:
            RelationExtractionDLModel: The pre-trained RelationExtractionDLModel.
        """
        jModel = _RelationExtractionDLModelLoader(folder, spark_session._jsparkSession)._java_obj
        return RelationExtractionDLModel(java_model=jModel)

    @staticmethod
    def pretrained(name="redl_ade_biobert", lang="en", remote_loc="clinical/models"):
        """Download a pre-trained RelationExtractionDLModel.

        Args:
            name (str): Name of the pre-trained model, by default "redl_ade_biobert"
            lang (str): Language of the pre-trained model, by default "en"
            remote_loc (str): Remote location of the pre-trained model. If None, use the open-source location. Other values are "clinical/models", "finance/models", or "legal/models".

        Returns:
            RelationExtractionDLModel: A pre-trained RelationExtractionDLModel.
        """
        from sparknlp_jsl.pretrained import InternalResourceDownloader
        return InternalResourceDownloader.downloadModel(RelationExtractionDLModel, name, lang, remote_loc,
                                                        j_dwn='InternalsPythonResourceDownloader')
