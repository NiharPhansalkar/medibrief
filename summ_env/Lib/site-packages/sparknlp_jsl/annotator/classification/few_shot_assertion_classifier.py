from sparknlp_jsl.common import *
from sparknlp_jsl.annotator.generic_classifier.generic_classifier import GenericClassifierModel, GenericClassifierApproach
from sparknlp_jsl.utils.licensed_annotator_type import InternalAnnotatorType
from sparknlp.common import *

class FewShotAssertionClassifierApproach(GenericClassifierApproach):
    """Trains a model to classify assertions based on general purpose few shot assertion embeddings.


    ======================  ======================
    Input Annotation types  Output Annotation type
    ======================  ======================
    ``SENTENCE_EMBEDDINGS`` ``ASSERTION``
    ======================  ======================
    """

    inputAnnotatorTypes = [AnnotatorType.SENTENCE_EMBEDDINGS]
    outputAnnotatorType = InternalAnnotatorType.ASSERTION

    name = "FewShotAssertionClassifierApproach"

    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.classification.FewShotAssertionClassifierApproach"):
        super(FewShotAssertionClassifierApproach, self).__init__(classname=classname)

class FewShotAssertionClassifierModel(GenericClassifierModel, HasStorageRef):
    """
     FewShotAssertionClassifierModel does assertion classification using can run large (LLMS based)
     few shot classifiers based on the SetFit approach.

     ==========================================  ======================
     Input Annotation types                      Output Annotation type
     ==========================================  ======================
     ``SENTENCE_EMBEDDING``                     ``ASSERTION``
     ==========================================  ======================

     Parameters
     ----------
     batchSize
         Batch size
     Examples
     --------
    >>> document_assembler = sparknlp.DocumentAssembler()\
    ...     .setInputCol("text")\
    ...     .setOutputCol("document")
    ...
    >>> sentence_detector = SentenceDetector()\
    ...    .setInputCol("document")\
    ...    .setOutputCol("sentence")
    ...
    >>> tokenizer = Tokenizer()\
    ...    .setInputCols(["sentence"])\
    ...    .setOutputCol("token")
    ...
    >>> embeddings = WordEmbeddingsModel.pretrained("embeddings_clinical", "en", "clinical/models")\
    ...    .setInputCols(["sentence", "token"])\
    ...    .setOutputCol("embeddings") \
    ...    .setCaseSensitive(False)
    ...
    >>> ner = MedicalNerModel.pretrained("ner_jsl", "en", "clinical/models") \
    ...    .setInputCols(["sentence", "token", "embeddings"]) \
    ...    .setOutputCol("ner")
    ...
    >>> ner_converter = NerConverter()\
    ...    .setInputCols(["sentence", "token", "ner"])\
    ...    .setWhiteList("Disease_Syndrome_Disorder", "Hypertension")\
    ...    .setOutputCol("ner_chunk")
    ...
    >>> few_shot_assertion_classifier = FewShotAssertionClassifierModel().pretrained()\
    ...     .setInputCols(["sentence", "ner_chunk"])\
    ...     .setOutputCol("assertion")
    ...
    >>> data = spark.createDataFrame(
    ...     [["Includes hypertension and chronic obstructive pulmonary disease."]]
    ...     ).toDF("text")
    ...
    >>> results = sparknlp.base.Pipeline() \
    ...     .setStages([
    ...         document_assembler, sentence_detector, tokenizer, embeddings, ner, ner_converter,
    ...         few_shot_assertion_classifier]) \
    ...     .fit(data) \
    ...     .transform(data) \
    ...
    >>> results\
    ...     .selectExpr("assertion.result", "assertion.metadata.chunk", "assertion.metadata.confidence")\
    ...     .show()

    +--------+----------------------------+-----------+
    |  result|                       chunk| confidence|
    +--------+----------------------------+-----------+
    | present|                hypertension|        1.0|
    |  absent| arteriovenous malformations|        1.0|
    +--------+----------------------------+-----------+
     """

    inputAnnotatorTypes = [AnnotatorType.SENTENCE_EMBEDDINGS]
    outputAnnotatorType = InternalAnnotatorType.ASSERTION

    name = "FewShotAssertionClassifierModel"

    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.classification.FewShotAssertionClassifierModel",
                 java_model=None):
        super(FewShotAssertionClassifierModel, self).__init__(
            classname=classname,
            java_model=java_model
        )

    @staticmethod
    def loadSavedModel(folder, spark_session, has_differentiable_head=False):
        """Loads a locally saved model.

        Parameters
        ----------
        folder : str
            Folder of the saved model
        spark_session : pyspark.sql.SparkSession
            The current SparkSession
        has_differentiable_head: bool
            A flag indicating whether the classifier is differentiable

        Returns
        -------
        LargeFewShotClassifierModel
            The restored model
        """
        from sparknlp_jsl.internal import _FewShotAssertionClassifierModelLoader
        jModel = _FewShotAssertionClassifierModelLoader(folder,
                                                    spark_session._jsparkSession,
                                                    has_differentiable_head)._java_obj
        return FewShotAssertionClassifierModel(java_model=jModel)

    @staticmethod
    def pretrained(name="assertion_fewshotclassifier", lang="en", remote_loc="clinical/models"):
        """Downloads and loads a pretrained model.

        Parameters
        ----------
        name : str, optional
            Name of the pretrained model, by default "assertion_fewshotclassifier"
        lang : str, optional
            Language of the pretrained model, by default "en"
        remote_loc : str, optional
            Optional remote address of the resource, by default None. Will use
            Spark NLPs repositories otherwise.

        Returns
        -------
        FewShotAssertionClassifierModel
            The restored model
        """
        from sparknlp_jsl.pretrained import InternalResourceDownloader
        return InternalResourceDownloader.downloadModel(FewShotAssertionClassifierModel, name, lang, remote_loc,
                                                        j_dwn='InternalsPythonResourceDownloader')