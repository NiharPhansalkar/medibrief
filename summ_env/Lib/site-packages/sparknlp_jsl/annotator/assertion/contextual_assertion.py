from sparknlp_jsl.annotator.handle_exception_params import HandleExceptionParams
from sparknlp_jsl.common import *
from sparknlp_jsl.utils.licensed_annotator_type import InternalAnnotatorType

from sparknlp.base import *


class ContextualAssertion(AnnotatorModelInternal, HandleExceptionParams):
    """An annotator model for contextual assertion analysis.

      This model identifies  contextual cues within text data, such as negation, uncertainty, and assertion. It is used
      clinical assertion detection, etc. It annotates text chunks with assertions based on configurable rules,
      prefix and suffix patterns, and exception patterns.

    ========================================= ======================
    Input Annotation types                    Output Annotation type
    ========================================= ======================
    ``DOCUMENT, TOKEN, CHUNK``                    ``ASSERTION``
    ========================================= ======================

    Parameters
    ----------

    caseSensitive
        Whether to use case sensitive when matching values
    prefixAndSuffixMatch
        Whether to match both prefix and suffix to annotate the hit
    prefixKeywords
        Prefix keywords to match
    suffixKeywords
        Suffix keywords to match
    exceptionKeywords
        Exception keywords not to match
    prefixRegexPatterns
        Prefix regex patterns to match
    suffixRegexPatterns
        Suffix regex pattern to match
    exceptionRegexPatterns
        Exception regex pattern not to match
    scopeWindow
        The scope window of the assertion expression
    assertion
        Assertion to match
    includeChunkToScope
        Whether to include chunk to scope when matching values
    scopeWindowDelimiters
        Delimiters used to limit the scope window.
    confidenceCalculationDirection
        Direction of confidence calculation. Accepted values are "left", "right", "both". Default is "left"


    Examples
    --------

    >>> import sparknlp
    >>> from sparknlp.base import *
    >>> from sparknlp.annotator import *
    >>> import sparknlp_jsl
    >>> from sparknlp_jsl.annotator import *
    >>> from pyspark.ml import Pipeline


    >>> documentAssembler = DocumentAssembler() \
    ...   .setInputCol("text") \
    ...   .setOutputCol("document")
    ...
    >>> sentenceDetector = SentenceDetector() \
    ...   .setInputCols(["document"]) \
    ...   .setOutputCol("sentence")
    ...
    >>> tokenizer = Tokenizer() \
    ...   .setInputCols(["sentence"]) \
    ...   .setOutputCol("token")

    >>> word_embeddings = WordEmbeddingsModel \
    ...        .pretrained("embeddings_clinical", "en", "clinical/models") \
    ...        .setInputCols(["sentence", "token"]) \
    ...        .setOutputCol("embeddings")

    >>>    clinical_ner = MedicalNerModel \
    ...        .pretrained("ner_clinical", "en", "clinical/models") \
    ...        .setInputCols(["sentence", "token", "embeddings"]) \
    ...        .setOutputCol("ner")

    >>>     ner_converter = NerConverter() \
    ...         .setInputCols(["sentence", "token", "ner"]) \
    ...         .setOutputCol("ner_chunk")

    Define the ContextualAssertion model:

    >>> data = spark.createDataFrame([["No kidney injury reported. No abnormal rashes or ulcers. Patient might not have liver disease."]]).toDF("text")

    >>> contextual_assertion = ContextualAssertion() \
    ...        .setInputCols(["sentence", "token", "ner_chunk"]) \
    ...        .setOutputCol("assertion") \
    ...        .setPrefixKeywords(["no", "not"]) \
    ...        .setSuffixKeywords(["unlikely","negative"]) \
    ...        .setPrefixRegexPatterns(["\\b(no|without|denies|never|none|free of|not include)\\b"]) \
    ...        .setSuffixRegexPatterns(["\\b(free of|negative for|absence of|not|rule out)\\b"]) \
    ...        .setExceptionKeywords(["without"]) \
    ...        .setExceptionRegexPatterns(["\\b(not clearly)\\b"]) \
    ...        .addPrefixKeywords(["negative for","negative"]) \
    ...        .addSuffixKeywords(["absent","neither"]) \
    ...        .setCaseSensitive(False) \
    ...        .setPrefixAndSuffixMatch(False) \
    ...        .setAssertion("absent") \
    ...        .setScopeWindow([2, 2])\

    >>> flattener = Flattener() \
    ...            .setInputCols("assertion") \
    ...            .setExplodeSelectedFields({"assertion": ["result",
    ...                                                     "metadata.ner_chunk as ner_chunk",
    ...                                                     "metadata.ner_label as ner_label"]})

    >>> pipeline = Pipeline(stages=[
    ...     documentAssembler,
    ...     sentenceDetector,
    ...     tokenizer,
    ...     contextual_assertion,
    ...   ])

    >>> result = pipeline.fit(data).transform(data)
    >>> result.show(truncate=False)

    +----------------+---------------+---------+
    |assertion_result|ner_chunk      |ner_label|
    +----------------+---------------+---------+
    |absent          |kidney injury  |PROBLEM  |
    |absent          |abnormal rashes|PROBLEM  |
    |absent          |liver disease  |PROBLEM  |
    +----------------+---------------+---------+

"""
    inputAnnotatorTypes = [AnnotatorType.DOCUMENT, AnnotatorType.TOKEN, AnnotatorType.CHUNK]
    outputAnnotatorType = InternalAnnotatorType.ASSERTION

    name = "ContextualAssertion"

    caseSensitive = Param(Params._dummy(),
                          "caseSensitive",
                          "Whether to use case sensitive when matching values",
                          typeConverter=TypeConverters.toBoolean)

    prefixAndSuffixMatch = Param(Params._dummy(),
                                 "prefixAndSuffixMatch",
                                 "Whether to match both prefix and suffix to annotate the hit",
                                 typeConverter=TypeConverters.toBoolean)

    scopeWindow = Param(Params._dummy(),
                        "scopeWindow",
                        "The scope window of the assertion expression",
                         typeConverter= TypeConverters.toListInt)

    assertion = Param(Params._dummy(),
                      "assertion",
                      "Assertion to match",
                      typeConverter=TypeConverters.toString)

    includeChunkToScope = Param(Params._dummy(),
                                "includeChunkToScope",
                                "Whether to include chunk to scope when matching values",
                                typeConverter=TypeConverters.toBoolean)

    scopeWindowDelimiters = Param(Params._dummy(),
                                  "scopeWindowDelimiters",
                                  "Delimiters used to limit the scope window.",
                                  typeConverter=TypeConverters.toListString)
    confidenceCalculationDirection = Param(
        Params._dummy(),
        "confidenceCalculationDirection",
        "Direction of confidence calculation.",
        typeConverter=TypeConverters.toString
    )


    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.assertion.context.ContextualAssertion", java_model=None):
        super(ContextualAssertion, self).__init__(
            classname=classname,
            java_model=java_model
        )
        self._setDefault(
            scopeWindow=[-1, -1]
           )

    def setCaseSensitive(self, value):
        """Sets whether to use case sensitive when matching values.
           Default is False

        Parameters
        ----------
        value : bool
            Whether to use case sensitive when matching values
        """
        return self._set(caseSensitive=value)

    def setPrefixAndSuffixMatch(self, value):
        """Sets whether to match both prefix and suffix to annotate the hit.
           Default is False

        Parameters
        ----------
        value : bool
            Whether to match both prefix and suffix to annotate the hit
        """
        return self._set(prefixAndSuffixMatch=value)

    def setScopeWindow(self, value):
        """Set the scope window of the assertion. The scope window is defined by two non-negative integers except (-1,-1).
           The first integer is the number of tokens to the left of the chunk, and the second integer is the number of tokens
           to the right of the chunk. Default is (-1, -1) which means the whole sentence

        Parameters
        ----------
        value : [int, int]
            Left and right offset if the scope window. Offsets must be non-negative values
        """
        assert (type(value) is list)
        assert (len(value) == 2)
        assert ((value[0] >= 0 and value[1] >= 0) or (value[0] == -1 and value[1] == -1))

        return self._set(scopeWindow=value)

    def setPrefixKeywords(self, value: list):
        """Set the prefix keywords to look for before chunk.
           Defaults are "no", "not", "never", "without", "absent",
           "neither", "nor", "denies", "free of", "lack of", "unremarkable for", "ruled out", "rule out", "declined", "denied"

        Parameters
        ----------
        value : list
            Prefix keywords to match
        """
        self._call_java("setPrefixKeywords", value)
        return self

    def setSuffixKeywords(self, value: list):
        """Set the suffix keywords to look for after chunk.
           Defaults are "not detected", "not demonstrate", "not appear",
           "not had", "was ruled out", "were ruled out", "are ruled out", "is ruled out", "unlikely", "not developed",
           "not present", "not associated with", "not had", "free from", "resolved"

        Parameters
        ----------
        value : list
            Suffix keywords to match
        """
        self._call_java("setSuffixKeywords", value)
        return self

    def setExceptionKeywords(self, value: list):
        """Set the exception patterns not to be searched for.
           Defaults are "not only", "not necessarily", "not need",
           "not certain if", "not clearly", "not likely", "not cause", "not extend", "not always", "not only", "not yet",
           "not otherwise", "not exclude"

        Parameters
        ----------
        value : list
            Exception keywords not to match
        """
        self._call_java("setExceptionKeywords", value)
        return self

    def setPrefixRegexPatterns(self, value: list):
        """Sets the prefix regex pattern to match
           Default is empty list.

        Parameters
        ----------
        value : list
            Prefix regex patterns to match
        """
        self._call_java("setPrefixRegexPatterns", value)
        return self

    def setSuffixRegexPatterns(self, value: list):
        """Sets the suffix regex pattern to match
           Default is empty list

        Parameters
        ----------
        value : list
            Suffix regex patterns to match
        """
        self._call_java("setSuffixRegexPatterns", value)
        return self

    def setExceptionRegexPatterns(self, value: list):
        """Sets the exception regex pattern not to match
           Default is empty list

        Parameters
        ----------
        value : list
            Exception regex patterns not to match
        """
        self._call_java("setExceptionRegexPatterns", value)
        return self

    def setAssertion(self, value):
        """Sets the assertion to match.
           Default is "absent"

        Parameters
        ----------
        value : str
            Assertion to match
        """
        return self._set(assertion=value)

    def addPrefixKeywords(self, value: list):
        """Adds the keywords to match

        Parameters
        ----------
        value : list
            Prefix keywords to match
        """
        try:
            prefix_keywords = list(self._call_java("getPrefixKeywords"))
        except KeyError:
            prefix_keywords = []
        prefix_keywords.extend(value)
        return self.setPrefixKeywords(prefix_keywords)

    def addSuffixKeywords(self, value: list):
        """Adds the keywords to match

        Parameters
        ----------
        value : list
            Suffix keywords to match
        """
        try:
            suffix_keywords = list(self._call_java("getSuffixKeywords"))
        except KeyError:
            suffix_keywords = []
        suffix_keywords.extend(value)
        return self.setSuffixKeywords(suffix_keywords)

    def setIncludeChunkToScope(self, value):
        """Sets whether to include chunk to scope when matching values
           Default is False

        Parameters
        ----------
        value : bool
            Whether to include chunk to scope when matching values
        """
        return self._set(includeChunkToScope=value)

    def setScopeWindowDelimiters(self, value: list):
        """Set delimiters used to limit the scope window.

        Parameters
        ----------
        value : List[str]
            Delimiters used to limit the scope window.
        """
        return self._set(scopeWindowDelimiters=value)

    def setConfidenceCalculationDirection(self, value):
        """Sets Direction of confidence calculation.
           If `left`, the confidence is calculated based on the distance of the found regex or keyword in left side of the sentence from a chunk.
           If `right`, the confidence is calculated based on the distance of the found regex or keyword in right side of the sentence from a chunk.
           If `both`, the confidence is calculated based on the minimum distance of the found regex or keyword in both sides of the sentence from a chunk.
           Default is "left"

        Parameters
        ----------
        value : str
            Direction of confidence calculation.
        """
        return self._set(confidenceCalculationDirection=value)


    @staticmethod
    def pretrained(name="contextual_assertion_absent", lang="en", remote_loc="clinical/models"):
        """Download a pre-trained ContextualAssertion.

        Parameters
        ----------
        name : str
            Name of the pre-trained model, by default "contextual_assertion_absent"
        lang : str
            Language of the pre-trained model, by default "en"
        remote_loc : str
            Remote location of the pre-trained model. If None, use the
            open-source location. Other values are "clinical/models",
            "finance/models", or "legal/models".

        Returns
        -------
        ContextualAssertion
            A pre-trained ContextualAssertion
        """
        from sparknlp_jsl.pretrained import InternalResourceDownloader

        return InternalResourceDownloader.downloadModel(ContextualAssertion, name, lang, remote_loc,
                                                        j_dwn='InternalsPythonResourceDownloader')