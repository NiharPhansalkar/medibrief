import pyspark.sql.types as T
import pyspark.sql.functions as F
from pyspark.ml import PipelineModel
from pyspark.sql import DataFrame

import sparknlp
from sparknlp.internal import ExtendedJavaWrapper

from ._tf_graph_builders.graph_builders import TFGraphBuilderFactory
from ._tf_graph_builders_1x.graph_builders import TFGraphBuilderFactory as TFGraphBuilderFactory1x


class AnnotationToolJsonReader(ExtendedJavaWrapper):
    """Reads and process the exported json file from NLP Lab.

    Reader class that parses relevant information exported from NLP Lab into different
    formats. The reader can be used to create a training dataset for training assertion
    status (using the `generateAssertionTrainSet` method) or NER models (in the CoNLL format
    using the `generateConll` method).

    To generate the assertion data, the following attributes need to be specified when
    instantiating the class:

    - `assertion_labels`: The assertion labels to use.
    - `excluded_labels`: The assertions labels that are excluded for the training dataset
        creation (can be an empty list).

    Examples:
        >>> from sparknlp_jsl.training import AnnotationToolJsonReader
        >>> assertion_labels = ["AsPresent","Absent"]
        >>> excluded_labels = ["Treatment"]
        >>> rdr = AnnotationToolJsonReader(
        ...     assertion_labels=assertion_labels,
        ...     excluded_labels=excluded_labels,
        ...     )
        >>> path = "annotations.json"
        >>> df = rdr.readDataset(spark, path)
        >>> df.show(5)
        +-----+-------+--------------------+-------------+--------------------+---------------+------------+--------------------+---------------+---------+--------------------+--------------------+--------------------+--------------------+
        |title|task_id|                text|completion_id|     completion_date|completion_user|ground_truth|          tool_chunk|assertion_label|relations|            document|            sentence|               token|           ner_label|
        +-----+-------+--------------------+-------------+--------------------+---------------+------------+--------------------+---------------+---------+--------------------+--------------------+--------------------+--------------------+
        | null|     18|\n229937784\nFIH\...|        18001|2022-03-04T13:19:...|        aleksei|        true|[{chunk, 136, 142...|             []|       []|[{document, 0, 37...|[{document, 1, 79...|[{token, 1, 9, 22...|[{named_entity, 1...|
        | null|     65|\n305038693\nFIH\...|        65001|2022-03-03T08:31:...|        aleksei|        true|[{chunk, 1, 9, 30...|             []|       []|[{document, 0, 17...|[{document, 1, 81...|[{token, 1, 9, 30...|[{named_entity, 1...|
        | null|     21|\n844970557 RWH\n...|        21001|2022-03-04T13:00:...|        aleksei|        true|[{chunk, 1, 13, 8...|             []|       []|[{document, 0, 28...|[{document, 1, 25...|[{token, 1, 9, 84...|[{named_entity, 1...|
        | null|     97|\n914783811\nFIH\...|        97001|2022-03-02T12:07:...|        aleksei|        true|[{chunk, 1, 9, 91...|             []|       []|[{document, 0, 90...|[{document, 1, 11...|[{token, 1, 9, 91...|[{named_entity, 1...|
        | null|     84|\n733882247\nFIH\...|        84001|2022-03-02T15:01:...|        aleksei|        true|[{chunk, 1, 9, 73...|             []|       []|[{document, 0, 77...|[{document, 1, 10...|[{token, 1, 9, 73...|[{named_entity, 1...|
        +-----+-------+--------------------+-------------+--------------------+---------------+------------+--------------------+---------------+---------+--------------------+--------------------+--------------------+--------------------+
        only showing top 5 rows
        >>> assertion_df = rdr.generateAssertionTrainSet(df)
        >>> rdr.generateConll(df, "annotations.conll")

    """

    def __init__(self, pipeline_model:str=None, assertion_labels:list=None, excluded_labels:list=None, cleanup_mode:str="disabled",
                 split_chars:list=None, context_chars:list=None, scheme:str="IOB", min_chars_tol:int=2, align_chars_tol:int=1,
                 merge_overlapping:bool=True, SDDLPath:str=""):
        """Inits the AnnotationToolJsonReader class.

        Args:
            pipeline_model (str): The pipeline model that is used to create the documents
                the sentences and the tokens. That pipeline model needs to have a DocumentAssembler,
                SentenceDetector, and Tokenizer stages. The input column of the document assembler
                needs to be named a 'text'. The output of the sentence detector needs to be named
                a 'sentence'. The output of the tokenizer needs to be named a 'token'. If `None`,
                a default pipeline will be created with the DocumentAssembler, SentenceDetector, and
                Tokenizer stages.
            assertion_labels (list): The assertions labels are used for the training dataset
                creation.
            excluded_labels (list): The assertions labels that are excluded for the training dataset
                creation.
            cleanup_mode (str): The clean mode that is used in the DocumentAssembler transformer.
            split_chars (list): The split chars that are used in the default tokenizer.
            context_chars (list): The context chars that are used in the default tokenizer.
            scheme (str): The schema that will use to create the IOB_tagger. Either IOB or BIOES.
            merge_overlapping (bool): Whether merge the chunks when they are in the same position.
            min_chars_tol (int): The minimum number of characters that a token must have to be
                considered valid. Should be greather than zero. Defaults to 2.
            align_chars_tol (int): Tolerance for the difference in number of chars of the
                tokens when compared to the substring obtained by subsetting the token
                with the begin and end indexes. Should be greather than zero. Defaults to 1.
            SDDLPath (str): The path to the `SentenceDetectorDLModel` annotator to be loaded from
                disk. If empty string, a default `SentenceDetector` annotator will be used.

        """
        # TODO: Improve documentation of merge_overlapping (make examples) and align_chars_tol (what does it do?)
        if assertion_labels is None:
            assertion_labels = []
        if excluded_labels is None:
            excluded_labels = []
        if type(pipeline_model) == PipelineModel:
            if scheme is None:
                scheme = "IOB"
            if min_chars_tol is None:
                min_chars_tol = 2
            if align_chars_tol is None:
                align_chars_tol = 1
            if merge_overlapping is None:
                merge_overlapping = True

            super(AnnotationToolJsonReader, self).__init__("com.johnsnowlabs.nlp.training.AnnotationToolJsonReader",
                                                           pipeline_model._to_java(), assertion_labels, excluded_labels,
                                                           scheme, min_chars_tol, align_chars_tol, merge_overlapping)
        else:
            if split_chars is None:
                split_chars = [" "]
            if context_chars is None:
                context_chars = [".", ",", ";", ":", "!", "?", "*", "-", "(", ")", "\"", "'"]

            super(AnnotationToolJsonReader, self).__init__("com.johnsnowlabs.nlp.training.AnnotationToolJsonReader",
                                                           assertion_labels, excluded_labels, cleanup_mode,
                                                           split_chars, context_chars, scheme,
                                                           min_chars_tol, align_chars_tol, merge_overlapping, SDDLPath)

    def readDataset(self, spark, path: str):
        """Reads the exported JSON file into a spark data frame.

        Args:
            spark: Spark session.
            path (str): Path to the exported file.

        Returns:
            DataFrame: Spark DataFrame containing the exported data.
        """
        # ToDo Replace with std pyspark
        jSession = spark._jsparkSession

        jdf = self._java_obj.readDataset(jSession, path)
        return self.getDataFrame(spark, jdf)

    def generateAssertionTrainSet(self, df, sentenceCol="sentence", assertionCol="assertion_label"):
        """Generates assertion training data at token level.

        Using information from the sentence and assertion labels, this method generates a
        training data set with the following columns:
        - text: sentence text
        - target: the token text
        - label: the assertion label
        - start: start position of the token
        - end: end position of the token

        The tokens are identified internally with the constraints from the
        `min_chars_tol` and `align_chars_tol` parameters.

        Args:
            df (DataFrame): Data Frame containing the sentences and assertion labels.
            sentenceCol (str, optional): Column name containing the sentence annotation.
            Defaults to "sentence".
            assertionCol (str, optional): Column name containing the assertion annotation.
            Defaults to "assertion_label".

        Returns:
            DataFrame: data frame with the training data.
        """

        jdf = self._java_obj.generateAssertionTrainSet(df._jdf, sentenceCol, assertionCol)
        session = df.sparkSession if hasattr(df, "sparkSession") else df.sql_ctx
        return DataFrame(jdf, session)

    def generatePlainAssertionTrainSet(self, df, taskColumn: str = "task_id", tokenCol: str = "token",
                                       nerLabel: str = "ner_label", assertion_label: str = "assertion_label"):
        """Generates assertion training data at chunk level.

        Using information from the sentence, task id (from NLP Lab), ner label, and 
        assertion labels, this method generates a training data set with the
        following columns:
        - sentence: sentence text
        - begin: start position of the token
        - end: end position of the token
        - ner: the NER chunk
        - assertion: the assertion label

        Internally uses the `NerConverterInternal` to identify the NER chunks.

        Args:
            df (DataFrame): Data Frame containing the sentences and assertion labels.
            taskColumn (str, optional): Column name containing the task id annotation.
            Defaults to "task_id".
            tokenCol (str, optional): Column name containing the token annotation.
            Defaults to "token".
            nerLabel (str, optional): Column name containing the NER label annotation.
            Defaults to "ner_label".
            assertion_label (str, optional): Column name containing the assertion label annotation.
            Defaults to "assertion_label".

        Returns:
            DataFrame: data frame with the training data.
        """
        jdf = self._java_obj.generatePlainAssertionTrainSet(df._jdf, taskColumn, tokenCol, nerLabel, assertion_label)
        session = df.sparkSession if hasattr(df, "sparkSession") else df.sql_ctx
        return DataFrame(jdf, session)

    def generateConll(self, df, path: str, taskColumn: str = "task_id", tokenCol: str = "token",
                        nerLabel: str = "ner_label"):
        """Saves a CoNLL file from the exported annotations.

        Args:
            df (DataFrame): DataFrame containing the annotations read using `readDataset`.
            path (str): Path to where the CoNLL file will be saved.
            taskColumn (str, optional): Column containing the task id information.
            Defaults to "task_id".
            tokenCol (str, optional): Column containing the token information.
            Defaults to "token".
            nerLabel (str, optional): Column containing the NER label.
            Defaults to "ner_label".
        """
        jdf = self._java_obj.generateConll(df._jdf, path, taskColumn, tokenCol, nerLabel)


class CodiEspReader(ExtendedJavaWrapper):
    """Parser for the CodiEsp dataset.
    """
    def __init__(self, scheme="IOB"):
        super(CodiEspReader, self).__init__("com.johnsnowlabs.nlp.training.CodiEspReader", scheme)

    def readDatasetTaskX(self, spark, path, textFolder, sep="\t"):
        """Reads the CodiEsp dataset into a spark data frame.
        """
        # TODO: Replace with std pyspark
        jSession = spark._jsparkSession

        jdf = self._java_obj.readDatasetTaskX(jSession, path, textFolder, sep)
        dataframe = self.getDataFrame(spark, jdf)
        return dataframe


class CantemistReader(ExtendedJavaWrapper):
    """Parser for the Cantemist dataset.
    """
    def __init__(self, scheme="IOB"):
        super(CantemistReader, self).__init__("com.johnsnowlabs.nlp.training.CantemistReader", scheme)

    def readDatasetTaskNer(self, spark, textFolder):
        """Reads the Cantemist dataset into a spark data frame.
        """
        # TODO: Replace with std pyspark
        jSession = spark._jsparkSession

        jdf = self._java_obj.readDatasetTaskNer(jSession, textFolder)
        dataframe = self.getDataFrame(spark, jdf)
        return dataframe


tf_graph = TFGraphBuilderFactory()
tf_graph_1x = TFGraphBuilderFactory1x()


class SynonymAugmentationUMLS(ExtendedJavaWrapper):
    """Augments datasets with synonyms from UMLS.
    """
    def __init__(self, spark, umls_path="", code_col="code", description_col="description", case_sensitive=False):
        self._spark = spark
        super(SynonymAugmentationUMLS, self).__init__("com.johnsnowlabs.nlp.training.SynonymAugmentationUMLS",
                                                      spark._jsparkSession, umls_path, code_col, description_col, case_sensitive)

    def augmentCsv(self, corpus_csv_path, ner_pipeline, language="ENG", do_product=False,
                   augmentation_mode="plain_text", synonym_source="umls", regex_parsers=None,
                   euclidean_distance_threshold=10.0, cosine_distance_threshold=0.25, synonym_limit=5, casing_functions=None):
        """Augment based on CSV file.
        """
        regex_parsers, casing_functions = regex_parsers or [], casing_functions or ['infer']
        jdf = self._java_obj.augmentCsv(corpus_csv_path, ner_pipeline._to_java(), language, do_product,
                                        augmentation_mode, synonym_source, regex_parsers,
                                        euclidean_distance_threshold, cosine_distance_threshold, synonym_limit, casing_functions)
        return self.getDataFrame(self._spark, jdf)

    def augmentDataFrame(self, corpus_df, ner_pipeline, language="ENG", do_product=False,
                         augmentation_mode="plain_text", synonym_source="umls", regex_parsers=None,
                         euclidean_distance_threshold=10.0, cosine_distance_threshold=0.25, synonym_limit=5, casing_functions=None):
        """Augment based on Spark Dataframe.
        """
        regex_parsers, casing_functions = regex_parsers or [], casing_functions or ['infer']
        jdf = self._java_obj.augmentDataFrame(corpus_df._jdf, ner_pipeline._to_java(), language, do_product,
                                              augmentation_mode, synonym_source, regex_parsers,
                                              euclidean_distance_threshold, cosine_distance_threshold, synonym_limit, casing_functions)
        return self.getDataFrame(self._spark, jdf)


class REDatasetHelper:
    """
    Class to preprocess RE dataset loaded into Spark Dataframe.

    Examples:
        >>> from sparknlp_jsl.training import REDatasetHelper
        >>> PATH = "/content/i2b2_clinical_rel_dataset.csv"
        >>> data = spark.read.option("header","true").format("csv").load(PATH)
        >>>
        >>> column_map = {
                "begin1": "firstCharEnt1",
                "end1": "lastCharEnt1",
                "begin2": "firstCharEnt2",
                "end2": "lastCharEnt2",
                "chunk1": "chunk1",
                "chunk2": "chunk2",
                "label1": "label1",
                "label2": "label2"
            }
        >>>
        >>> # apply preprocess function to dataframe
        >>> data = REDatasetHelper(data).create_annotation_column(column_map)
        >>> data.show(5)
        +-------+-------------+--------------------+--------------------+-------+--------------------+------+----+-----+--------------------+------+----+---------+---------+------------+-------------+------------+-------------+-------------+-------------+-------------+
        |dataset|       source|            txt_file|            sentence|sent_id|              chunk1|begin1|end1|  rel|              chunk2|begin2|end2|   label1|   label2|lastCharEnt1|firstCharEnt1|lastCharEnt2|firstCharEnt2|words_in_ent1|words_in_ent2|words_between|
        +-------+-------------+--------------------+--------------------+-------+--------------------+------+----+-----+--------------------+------+----+---------+---------+------------+-------------+------------+-------------+-------------+-------------+-------------+
        |   test|beth+partners|i2b2 2010 VA/test...|VITAL SIGNS - Tem...|     44|    respiratory rate|    12|  13|    O|          saturation|    17|  17|     test|     test|          64|           49|          84|           75|            2|            1|            3|
        |   test|beth+partners|i2b2 2010 VA/test...|No lotions , crea...|    146|             lotions|     1|   1|TrNAP|           incisions|     7|   7|treatment|  problem|           9|            3|          42|           34|            1|            1|            5|
        |  train|     partners|i2b2 2010 VA/conc...|Because of expect...|     43|expected long ter...|     2|   6|    O|         a picc line|     8|  10|treatment|treatment|          54|           11|          68|           58|            5|            3|            1|
        |  train|     partners|i2b2 2010 VA/conc...|She states this l...|     21|    light-headedness|     3|   3|  PIP|         diaphoresis|    12|  12|  problem|  problem|          31|           16|          92|           82|            1|            1|            8|
        |   test|beth+partners|i2b2 2010 VA/test...|Initial electroca...|     61|an inferior and r...|    38|  43|  PIP|1-mm st depressio...|    28|  34|  problem|  problem|         239|          196|         176|          145|            6|            7|            3|
        +-------+-------------+--------------------+--------------------+-------+--------------------+------+----+-----+--------------------+------+----+---------+---------+------------+-------------+------------+-------------+-------------+-------------+-------------+
        only showing top 5 rows
        >>> #   if data contains different splits, you can first preprocess then filter by dataset column.
        >>> train_data = data.where("dataset='train'")
        >>> test_data = data.where("dataset='test'")
    """

    def __init__(self, spark_df: DataFrame):
        self.annotation_schema = T.ArrayType(T.StructType([
            T.StructField('annotatorType', T.StringType(), False),
            T.StructField('begin', T.IntegerType(), False),
            T.StructField('end', T.IntegerType(), False),
            T.StructField('result', T.StringType(), False),
            T.StructField('metadata', T.MapType(T.StringType(), T.StringType()), False),
            T.StructField('embeddings', T.ArrayType(T.FloatType()), False)
        ]))

        self.data = spark_df

    def create_annotation_column(self, column_map,  ner_column_name="train_ner_chunks"):
        """Creates label column for RelationExtractionApproach.

        
        Args
            column_map: Required mapping between entity columns and dataset columns.
                Required columns are: begin1, end1, chunk1, label1, begin2, end2, chunk2, label2.
            ner_column_name: a label column name for RelationExtractionApproach.
                
        Examples:
            # for dataset with following schema:
            >>> data.printSchema()
            root
            |-- sentence: string (nullable = true)
            |-- chunk1: string (nullable = true)
            |-- begin1: string (nullable = true)
            |-- end1: string (nullable = true)
            |-- rel: string (nullable = true)
            |-- chunk2: string (nullable = true)
            |-- begin2: string (nullable = true)
            |-- end2: string (nullable = true)
            |-- label1: string (nullable = true)
            |-- label2: string (nullable = true)
            |-- lastCharEnt1: string (nullable = true)
            |-- firstCharEnt1: string (nullable = true)
            |-- lastCharEnt2: string (nullable = true)
            |-- firstCharEnt2: string (nullable = true)

            #  map should be as follows:
            >>> column_map = {
                    "begin1": "firstCharEnt1",
                    "end1": "lastCharEnt1",
                    "begin2": "firstCharEnt2",
                    "end2": "lastCharEnt2",
                    "chunk1": "chunk1",
                    "chunk2": "chunk2",
                    "label1": "label1",
                    "label2": "label2"
                }
        
        Returns:
            DataFrame: A new dataframe extended with ner chunk column.
        """

        required_maps = ["begin1", "end1", "chunk1", "label1", "begin2", "end2", "chunk2", "label2"]
        for col in required_maps:
            if column_map.get(col) is None:
                raise ValueError(f'{col} value is None. Following columns should be mapped: {required_maps}')

        num_rows = self.data.count()
        cast_columns = ["begin1", "end1", "begin2", "end2"]
        for col in cast_columns:

            # Â cast integer columns before creating ner chunks
            data_casted = self.data.withColumn(column_map[col], self.data[column_map[col]].cast(T.IntegerType()))
            if num_rows == data_casted.where(F.col(column_map[col]).isNull()).count():
                raise ValueError(f'{column_map[col]} could not be casted as integer!')
            else:
                self.data = data_casted

        @F.udf(returnType=self.annotation_schema)
        def create_train_annotations(begin1, end1, chunk1, label1, begin2, end2, chunk2, label2):

            entity1 = sparknlp.annotation.Annotation(
                "chunk", begin1, end1, chunk1, {'entity': label1.upper(), 'sentence': '0'}, [])
            entity2 = sparknlp.annotation.Annotation(
                "chunk", begin2, end2, chunk2, {'entity': label2.upper(), 'sentence': '0'}, [])

            entity1.annotatorType = "chunk"
            entity2.annotatorType = "chunk"

            return [entity1, entity2]

        self.data = self.data.withColumn(
            ner_column_name,
            create_train_annotations(
                column_map["begin1"], column_map["end1"], column_map["chunk1"], column_map["label1"],
                column_map["begin2"], column_map["end2"], column_map["chunk2"], column_map["label2"]
            ).alias(ner_column_name, metadata={'annotatorType': "chunk"})
        )

        drop_columns = []
        for col in required_maps:
            if column_map[col] != col:
                drop_columns.append(col)

        return_columns = set(self.data.columns) - set(drop_columns)
        self.data = self.data.select(list(return_columns))
        return self.data
