from pyspark.ml.wrapper import JavaModel
import json

class LLMLoader(JavaModel):
    def setInputPrefix(self, inputPrefix):
        """Set a prefix for infilling (default: empty)
        """
        self._call_java("setInputPrefix", inputPrefix)

        return self

    def setInputSuffix(self, inputSuffix):
        """Set a suffix for infilling (default: empty)
        """
        self._call_java("setInputSuffix", inputPrefix)

        return self

    def setCachePrompt(self, cachePrompt):
        """Whether to remember the prompt to avoid reprocessing it
        """
        self._call_java("setCachePrompt", cachePrompt)

        return self

    def setNPredict(self, nPredict):
        """Set the number of tokens to predict (default: -1, -1 = infinity, -2 = until context filled)
        """
        self._call_java("setNPredict", nPredict)

        return self

    def setTopK(self, topK):
        """Set top-k sampling (default: 40, 0 = disabled)
        """
        self._call_java("setTopK", topK)

        return self

    def setTopP(self, topP):
        """Set top-p sampling (default: 0.9, 1.0 = disabled)
        """
        self._call_java("setTopP", topP)

        return self

    def setMinP(self, minP):
        """Set min-p sampling (default: 0.1, 0.0 = disabled)
        """
        self._call_java("setMinP", minP)

        return self

    def setTfsZ(self, tfsZ):
        """Set tail free sampling, parameter z (default: 1.0, 1.0 = disabled)
        """
        self._call_java("setTfsZ", tfsZ)

        return self

    def setTypicalP(self, typicalP):
        """Set locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)
        """
        self._call_java("setTypicalP", typicalP)

        return self

    def setTemperature(self, temperature):
        """Set the temperature (default: 0.8)
        """
        self._call_java("setTemperature", temperature)

        return self

    def setDynamicTemperatureRange(self, dynatempRange):
        """Set the dynamic temperature range (default: 0.0, 0.0 = disabled)
        """
        self._call_java("setDynamicTemperatureRange", dynatempRange)

        return self

    def setDynamicTemperatureExponent(self, dynatempExponent):
        """Set the dynamic temperature exponent (default: 1.0)
        """
        self._call_java("setDynamicTemperatureExponent", dynatempExponent)

        return self

    def setRepeatLastN(self, repeatLastN):
        """Set the last n tokens to consider for penalties (default: 64, 0 = disabled, -1 = ctx_size)
        """
        self._call_java("setRepeatLastN", repeatLastN)

        return self

    def setRepeatPenalty(self, repeatPenalty):
        """Set the penalty of repeated sequences of tokens (default: 1.0, 1.0 = disabled)
        """
        self._call_java("setRepeatPenalty", repeatPenalty)

        return self

    def setFrequencyPenalty(self, frequencyPenalty):
        """Set the repetition alpha frequency penalty (default: 0.0, 0.0 = disabled)
        """
        self._call_java("setFrequencyPenalty", frequencyPenalty)

        return self

    def setPresencePenalty(self, presencePenalty):
        """Set the repetition alpha presence penalty (default: 0.0, 0.0 = disabled)
        """
        self._call_java("setPresencePenalty", presencePenalty)

        return self

    def setMiroStatTau(self, mirostatTau):
        """Set the MiroStat target entropy, parameter tau (default: 5.0)
        """
        self._call_java("setMiroStatTau", mirostatTau)

        return self

    def setMiroStatEta(self, mirostatEta):
        """Set the MiroStat learning rate, parameter eta (default: 0.1)
        """
        self._call_java("setMiroStatEta", mirostatEta)

        return self

    def setPenalizeNl(self, penalizeNl):
        """Whether to penalize newline tokens
        """
        self._call_java("setPenalizeNl", penalizeNl)

        return self

    def setNKeep(self, nKeep):
        """Set the number of tokens to keep from the initial prompt (default: 0, -1 = all)
        """
        self._call_java("setNKeep", nKeep)

        return self

    def setSeed(self, seed):
        """Set the RNG seed (default: -1, use random seed for &lt; 0)
        """
        self._call_java("setSeed", seed)

        return self

    def setNProbs(self, nProbs):
        """Set the amount top tokens probabilities to output if greater than 0.
        """
        self._call_java("setNProbs", nProbs)

        return self

    def setMinKeep(self, minKeep):
        """Set the amount of tokens the samplers should return at least (0 = disabled)
        """
        self._call_java("setMinKeep", minKeep)

        return self

    def setGrammar(self, grammar):
        """ Set BNF-like grammar to constrain generations (see samples in grammars/ dir)
        """
        self._call_java("setGrammar", grammar)

        return self

    def setPenaltyPrompt(self, penaltyPrompt):
        """Override which part of the prompt is penalized for repetition.
        E.g. if original prompt is "Alice: Hello!" and penaltyPrompt is "Hello!", only the latter will be penalized if
        repeated. See <a href="https://github.com/ggerganov/llama.cpp/pull/3727">pull request 3727</a> for more details.
        """
        if type(penaltyPrompt) is int:
            penaltyPrompt = [penaltyPrompt]
        self._call_java("setPenaltyPrompt", penaltyPrompt)

        return self

    def setIgnoreEos(self, ignoreEos):
        """Set whether to ignore end of stream token and continue generating (implies --logit-bias 2-inf)
        """
        self._call_java("setIgnoreEos", ignoreEos)

        return self

    def setStopStrings(self, stopStrings):
        """Set strings upon seeing which token generation is stopped
        """
        if type(stopStrings) is str:
            stopStrings = [stopStrings]

        self._call_java("setStopStrings", stopStrings)

        return self

    def setUseChatTemplate(self, useChatTemplate):
        """Set whether or not generate should apply a chat template (default: false)
        """
        self._call_java("setUseChatTemplate", useChatTemplate)

        return self

    def encodeModel(self, model_path, output_model_path, metadata):
        self._call_java("encodeModel", model_path, output_model_path, metadata)

    def getMetadata(self, param):
        metadata_json = self._call_java("getMetadata", param)
        return json.loads(metadata_json)

    def getMetadataEntry(self, param):
        return self._call_java("getMetadataEntry", param)

    def load(self, model_path):
        self._call_java("load", model_path)

        return self

    def generate(self, prompt):
        return self._call_java("generate", prompt)

    def pretrained(self, name, lang="en", remote_loc="clinical/models"):
        return self._call_java("pretrained", name, lang, remote_loc)

    def __init__(self, spark):
        classname="com.johnsnowlabs.ml.gguf.LLMLoader"
        super(LLMLoader, self).__init__(java_model=None)
        self.__class__._java_class_name = classname
        self._java_obj = self._new_java_obj(classname, self.uid)
        self._call_java("setSparkSession", spark._jsparkSession)