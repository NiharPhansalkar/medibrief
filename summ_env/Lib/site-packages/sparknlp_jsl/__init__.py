import os
import re
import sys
from pyspark.sql import SparkSession
from sparknlp_jsl import finance
from sparknlp_jsl import legal
from sparknlp_jsl import annotator
from util import read_version
from .utils import Deidentifier as Deid
import com.johnsnowlabs



from pyspark.ml.wrapper import JavaModel
from pyspark.ml.wrapper import JavaParams

@staticmethod
def _from_java(java_stage):
    def _from_java_spk34(java_stage: "JavaObject"):
        """
        Given a Java object, create and return a Python wrapper of it.
        Used for ML persistence.

        Meta-algorithms such as Pipeline should override this method as a classmethod.
        """

        def __get_class(clazz: str):
            """
            Loads Python class from its name.
            """
            parts = clazz.split(".")
            module = ".".join(parts[:-1])
            m = __import__(module, fromlist=[parts[-1]])
            return getattr(m, parts[-1])

        stage_name = java_stage.getClass().getName().replace("org.apache.spark", "pyspark")
        # Generate a default new instance from the stage_name class.
        py_type = __get_class(stage_name)
        if issubclass(py_type, JavaParams):
            # Load information from java_stage to the instance.
            py_stage = py_type()
            py_stage._java_obj = java_stage

            # SPARK-10931: Temporary fix so that persisted models would own params from Estimator
            if issubclass(py_type, JavaModel):
                py_stage._create_params_from_java()

            py_stage._resetUid(java_stage.uid())
            py_stage._transfer_params_from_java()
        elif hasattr(py_type, "_from_java"):
            py_stage = py_type._from_java(java_stage)
        else:
            raise NotImplementedError(
                "This Java stage cannot be loaded into Python currently: %r" % stage_name
            )
        return py_stage

    def _from_java_pre_spk34(java_stage):
        """
        Given a Java object, create and return a Python wrapper of it.
        Used for ML persistence.

        Meta-algorithms such as Pipeline should override this method as a classmethod.
        """

        def __get_class(clazz):
            """
            Loads Python class from its name.
            """
            parts = clazz.split('.')
            module = ".".join(parts[:-1])
            m = __import__(module)
            for comp in parts[1:]:
                m = getattr(m, comp)
            return m

        stage_name = java_stage.getClass().getName().replace("org.apache.spark", "pyspark")
        # Generate a default new instance from the stage_name class.
        py_type = __get_class(stage_name)
        if issubclass(py_type, JavaParams):
            # Load information from java_stage to the instance.
            py_stage = py_type()
            py_stage._java_obj = java_stage

            # SPARK-10931: Temporary fix so that persisted models would own params from Estimator
            if issubclass(py_type, JavaModel):
                py_stage._create_params_from_java()

            py_stage._resetUid(java_stage.uid())
            py_stage._transfer_params_from_java()
        elif hasattr(py_type, "_from_java"):
            py_stage = py_type._from_java(java_stage)
        else:
            raise NotImplementedError("This Java stage cannot be loaded into Python currently: %r"
                                      % stage_name)
        return py_stage


    try :
        return _from_java_spk34(java_stage)
    except:
        return _from_java_pre_spk34(java_stage)

import pyspark.ml.wrapper
# Monkey Patch which enables pre Spark 3.4.0 Class Resolution
pyspark.ml.wrapper.JavaParams._from_java = _from_java



com.johnsnowlabs.legal = legal
com.johnsnowlabs.finance = finance
com.johnsnowlabs.annotator = annotator
sys.modules['com.johnsnowlabs.annotator'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.assertion'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.assertion.context'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.assertion.logreg'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.assertion.dl'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.assertion.merger'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.resolution'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.deid'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.classification'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.generic_classifier'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.context'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.merge'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.keyword'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.re'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.qa'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.chunker'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.seq2seq'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.matcher'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.regex'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.embeddings'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.splitter'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.rag'] = annotator
sys.modules['com.johnsnowlabs.nlp.annotators.windowed'] = annotator

sys.modules['com.johnsnowlabs.finance'] = finance
sys.modules['com.johnsnowlabs.finance.token_classification'] = finance
sys.modules['com.johnsnowlabs.finance.token_classification.ner'] = finance
sys.modules['com.johnsnowlabs.finance.chunk_classification'] = finance
sys.modules['com.johnsnowlabs.finance.chunk_classification.resolution'] = finance
sys.modules['com.johnsnowlabs.finance.chunk_classification.deid'] = finance
sys.modules['com.johnsnowlabs.finance.chunk_classification.assert'] = finance
sys.modules['com.johnsnowlabs.finance.chunk_classification.assertion'] = finance
sys.modules['com.johnsnowlabs.finance.graph'] = finance
sys.modules['com.johnsnowlabs.finance.graph.relation_extraction'] = finance
sys.modules['com.johnsnowlabs.finance.sequence_classification'] = finance
sys.modules['com.johnsnowlabs.finance.sequence_generation'] = finance

sys.modules['com.johnsnowlabs.legal'] = legal
sys.modules['com.johnsnowlabs.legal.token_classification'] = legal
sys.modules['com.johnsnowlabs.legal.token_classification.ner'] = legal
sys.modules['com.johnsnowlabs.legal.chunk_classification'] = legal
sys.modules['com.johnsnowlabs.legal.chunk_classification.resolution'] = legal
sys.modules['com.johnsnowlabs.legal.chunk_classification.deid'] = legal
sys.modules['com.johnsnowlabs.legal.chunk_classification.assert'] = legal
sys.modules['com.johnsnowlabs.legal.chunk_classification.assertion'] = legal
sys.modules['com.johnsnowlabs.legal.graph'] = legal
sys.modules['com.johnsnowlabs.legal.graph.relation_extraction'] = legal
sys.modules['com.johnsnowlabs.legal.sequence_classification'] = legal
sys.modules['com.johnsnowlabs.legal.sequence_generation'] = legal

annotators = annotator
transformer_seq_classification = annotator

version_regex = re.compile("^(\\d+\\.)(\\d+\\.)(\\*|\\d+)(-?rc\\d*)?$")
size_regex = re.compile("([0-9])+[GMK]")


def start(secret:str, gpu:bool=False, apple_silicon:bool=False, aarch64=False, public:str= "", params:dict=None):
    """Starts a SparkSession with default parameters for Spark NLP Licensed

    The default parameters would result in the equivalent of:

    .. code-block:: python

        SparkSession.builder \\
            .appName("Spark NLP Licensed") \\
            .master("local[*]") \\
            .config("spark.driver.memory", "32G") \\
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \\
            .config("spark.kryoserializer.buffer.max", "2000M") \\
            .config("spark.driver.maxResultSize", "0") \\
            .config("spark.files.overwrite", "true") \\
            .config("spark.extraListeners", "com.johnsnowlabs.license.LicenseLifeCycleManager") \\
            .config("spark.jars", "https://pypi.johnsnowlabs.com/|secret|/spark-nlp-jsl-|release|.jar") \\
            .config("spark.jars.packages", "com.johnsnowlabs.nlp:spark-nlp_2.12:|release|") \\
            .getOrCreate()

    Args:
        secret (str): Your secret key
        gpu (bool): Whether to use GPU or not
        apple_silicon (bool): Whether to use M1 or not
        aarch64 (bool): Whether to use aarch64 or not
        public (str): Spark NLP version
        params (dict): SparkSession params

    Returns:
        SparkSession: SparkSession with Spark NLP Licensed
    """
    if params is None:
        params = {}
    else:
        if not isinstance(params, dict):
            raise TypeError('params must be a dictionary like {"spark.executor.memory": "8G"}')

    if '_instantiatedSession' in dir(SparkSession) and SparkSession._instantiatedSession is not None:
        print('Warning::Spark Session already created, some configs may not take.')

    try:
        matched = re.match(version_regex, public)
        if matched:
            public = matched.string
        else:
            public = pub_version()
    except:
        public = pub_version()

    # Spark NLP on Apache Spark 3.0.x
    maven_spark = "com.johnsnowlabs.nlp:spark-nlp_2.12:{}".format(public)
    maven_gpu_spark = "com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:{}".format(public)
    maven_silicon = "com.johnsnowlabs.nlp:spark-nlp-silicon_2.12:{}".format(public)
    maven_aarch64 = "com.johnsnowlabs.nlp:spark-nlp-aarch64_2.12:{}".format(public)

    __check_size_and_overwrite("spark.driver.memory", "32G", params)
    __check_size_and_overwrite("spark.driver.maxResultSize", "0", params)
    __check_size_and_overwrite("spark.kryoserializer.buffer.max", "2000M", params)
    params.update({"spark.serializer": "org.apache.spark.serializer.KryoSerializer"})


    builder = SparkSession.builder \
        .appName("Spark NLP Licensed") \
        .master("local[*]")

    spark_jars_packages = ""
    spark_jars = "https://pypi.johnsnowlabs.com/" + secret + f"/spark-nlp-jsl-{version()}.jar"

    if gpu:
        spark_jars_packages = maven_gpu_spark
    elif apple_silicon:
        spark_jars_packages = maven_silicon
    elif aarch64:
        spark_jars_packages = maven_aarch64
    else:
        spark_jars_packages = maven_spark

    if params.get("spark.files.overwrite") is None:
        builder.config("spark.files.overwrite", "true")

    if params.get("spark.jars.packages") is None:
        builder.config("spark.jars.packages", spark_jars_packages)

    if params.get("spark.jars") is None:
        builder.config("spark.jars", spark_jars)

    if params.get("spark.extraListeners") is None:
        params["spark.extraListeners"] = "com.johnsnowlabs.license.LicenseLifeCycleManager"
    else:
        params["spark.extraListeners"] += ",com.johnsnowlabs.license.LicenseLifeCycleManager"

    for key, value in params.items():
        if key == "spark.jars.packages":
            packages = spark_jars_packages + "," + value
            builder.config(key, packages)
        elif key == "spark.jars":
            jars = spark_jars + "," + value
            builder.config(key, jars)
        else:
            builder.config(key, value)

    # Force the check of the license and load of S3 credentials
    spark = builder.getOrCreate()

    spark._jvm.com.johnsnowlabs.util.start.registerListenerAndStartRefresh()
    spark._jvm.com.johnsnowlabs.util.start.setLocaleLanguageEN()
    spark._jvm.com.johnsnowlabs.license.LicenseValidator.load()
    """
    Ä±t is assigned to English by default to avoid problems on computers with different local languages.
    """


    return spark


def get_credentials(spark):
    """Gets John Snow Labs credentials

    Args:
        spark (SparkSession): SparkSession

    Returns:
        tuple: (secretKey, keyId, token)
    """
    creds = spark._jvm.com.johnsnowlabs.util.start.getAwsCredentials()
    return (creds.secretKey(), creds.keyId(), creds.token())


def __check_size_and_overwrite(key:str, defaultValue, params:dict):
    """Checks if the size is valid and overwrites it.

    If the value contained in params[key] is not valid, use the default value.

    Args:
        key (str): which key on the params dict to check
        defaultValue (any): The default value for the key
        params (_type_): Dictionary containing the parameters
    """
    if params.get(key):
        value = params[key]
        matched = re.match(size_regex, value)
        if not matched:
            params[key] = defaultValue
    else:
        params[key] = defaultValue


def pub_version():
    """Gets the public version of Spark NLP

    Returns:
        str: Public version of Spark NLP
    """
    return read_version.get_version_from_file('PUBLIC_VERSION')


def version():
    """Gets the version of Spark NLP

    Returns:
        str: Version of Spark NLP
    """
    return read_version.get_version_from_file('VERSION')


def library_settings(spark):
    """Gets the library settings

    Args:
        spark (SparkSession): SparkSession

    Returns:
        str: Library settings
    """
    configs = spark._jvm.com.johnsnowlabs.util.LibrarySettings.getAllConfigsAsString()
    return configs


def __set_s3_credentials_as_spark_properties(spark):
    """Sets S3 credentials as Spark properties

    Args:
        spark (SparkSession): SparkSession
    """
    credentials = spark._jvm.com.johnsnowlabs.license.LicenseValidator.getS3Credentials()
    if credentials.isDefined():
        awsid = credentials.get()._2()
        secret = credentials.get()._1()
        spark.conf.set("spark.jsl.settings.pretrained.credentials.secret_access_key", secret)
        spark.conf.set("spark.jsl.settings.pretrained.credentials.access_key_id", awsid)

# auto register and start refresh for databricks environments
if "DATABRICKS_RUNTIME_VERSION" in os.environ and "AWS_ACCESS_KEY_ID" not in os.environ:
    try:
        from pyspark.sql import SparkSession
        SparkSession.getActiveSession()._jvm.com.johnsnowlabs.util.start.registerListenerAndStartRefresh()
    except:
        print("Warning::Can not fetch aws credentials. you need to start the library manually using sparknlp_jsl.start().")

# auto load license validator on existing spark session env like databricks
def load_license_validator():
    try:
        from pyspark.sql import SparkSession
        spark = SparkSession.getActiveSession()
        if spark:
            spark._jvm.com.johnsnowlabs.license.LicenseValidator.load()
    except:
        print("Warning::Failed to load license validator.")

load_license_validator()

# set ipython kernel name as env variable
try:
    from IPython import get_ipython
    os.environ["IPYTHON_NAME"] =  str(get_ipython())
except:
    pass
