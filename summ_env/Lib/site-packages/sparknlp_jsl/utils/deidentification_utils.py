from sparknlp import DocumentAssembler
from sparknlp.annotation import Annotation
from sparknlp.common import AnnotatorType
from pyspark.sql.types import Row
from typing import List, Dict

from sparknlp_jsl.structured_deidentification import StructuredDeidentification
from pyspark.sql import SparkSession, DataFrame


#### ------------------------------------- StructuredDeidentification ------------------------------------- ####

def structured_deidentifier(
            spark,
            input_file_path=None,
            output_file_path="deidentified.csv",
            separator=",",
            columns_dict=None,
            ref_source="faker",
            obfuscateRefFile=None,
            columns_seed=None,
            shift_days=None,
            date_formats=None,
            language="en",
            id_column="",
        ):
        """This method is used to deidentify structured data. It takes the input as a file path and returns a deidentified dataframe and a file in csv/json format.

        Parameters
        ----------

        input_file_path (str): The path of the input file.
        output_file_path (str): The path of the output file.
        separator (str): The seperator of the input csv file.
        columns_dict (dict): A dictionary that contains the column names and the tags that should be used for deidentification.
        ref_source (str): The source of the reference file. It can be "faker" or "file".
        obfuscateRefFile (str): The path of the reference file for obfuscation.
        columns_seed (int): The seed value for the random number generator.
        shift_days (int): The number of days to be shifted.
        date_formats (list): A list of date formats.
        language (str): The language used to select faker entities. Options: 'en'(English), 'de'(German), 'es'(Spanish), 'fr'(French), 'ar'(Arabic) or 'ro'(Romanian). Default:'en'.
        idColumn (str): The column that contains the id of the row. If provided, data will obfuscate consistently by idColumn, especially date entities.
     
        Returns:
        -------
        Spark DataFrame: A deidentified dataframe.
        csv/json file: A deidentified file.

        """


        if not columns_dict:
            columns_dict = {"NAME": "NAME", "AGE": "AGE"}

        if not date_formats:
            date_formats = ["dd/MM/yyyy", "dd-MM-yyyy", "d/M/yyyy", "dd-MM-yyyy", "d-M-yyyy"]

        obfuscator_args = {
            "spark": spark,
            "columns": columns_dict,
            "obfuscateRefSource": ref_source,
            "columnsSeed": columns_seed,
            "dateFormats": date_formats,
            "language": language,
            "idColumn": id_column
        }

        if ref_source == "file" or ref_source == "both":
            obfuscator_args["obfuscateRefFile"] = obfuscateRefFile

        if ref_source == "faker" and shift_days is not None:
            obfuscator_args["days"] = shift_days

        obfuscator = StructuredDeidentification(**obfuscator_args)

        try:
            input_file_type = input_file_path.split(".")[-1]

            data = spark.read.csv(
                input_file_path, header=True, sep=separator
            )

            if input_file_type == "csv":
                data = data

            elif input_file_type == "json":
                data = spark.read.format(input_file_type).load(
                    input_file_path
                )

        except ValueError:
            raise ValueError("You entered an invalid file path or file format...")

        results_df = obfuscator.obfuscateColumns(data)
        results_df_pd = results_df.toPandas()

        if input_file_type == "csv":
            results_df_pd.to_csv(f"{output_file_path}", index=False)
            print(
                f"Deidentifcation successfully completed and the results saved as '{output_file_path}' !"
            )

        elif input_file_type == "json":
            results_df_pd.to_json(f"{output_file_path}", orient="records")
            print(
                f"Deidentifcation successfully completed and the results saved as '{output_file_path}' !"
            )

        return results_df


#### ------------------------------------- Dict To DeIdentification DataFrame ------------------------------------- ####
def dict_to_annotation_converter(spark: SparkSession, data: list, document_col_name: str = "document", chunk_col_name: str = "chunk", adjust_end: bool = False):
    """This method is used to convert a list of dictionaries to a Spark DataFrame. The returned DataFrame will contain chunk and document columns that are compatible with Spark NLP and can be used for deidentification.

    The input data should be a list of dictionaries.
    Each dictionary should have a "text" key and a "chunks" key.
    The "text" key should contain the text of the document and the "chunks" key should contain a list of dictionaries.

    Each dictionary in the "chunks" list should have "begin", "end", "result", "entity" and "metadata" keys.

    The "begin" key should contain the start index of the chunk,
    the "end" key should contain the end index of the chunk,
    the "result" key should contain the text of the chunk,
    the "entity" key should contain the entity type of the chunk
    and the "metadata" key should contain the metadata of the chunk.

    Parameters
    ----------
        spark (SparkSession): The Spark session object.
        data (list): A list of dictionaries.
        document_col_name (str): The name of the document column. Default: "document".
        chunk_col_name (str): The name of the chunk column. Default: "chunk".
        adjust_end (bool): If True, the end values of the chunks will minus 1. Default: False.

    Notes
    -----
    - The provided begin, end, and result values are not validated for accuracy. It is the user's responsibility to ensure these values are correct.

    - Spark NLP annotators calculate the end index of the annotations by minus 1 from the actual end index. If the end index is not minus 1, the annotators will not be able to calculate the correct end index. Therefore, the end index of the chunks should be minus 1. If the end index is not minus 1 in chunks, the adjust_end parameter should be set to True.

    - document_col_name and chunk_col_name should not be the same as the column names in the input data.

    - The name of 'document_metadata' is reserved for the metadata column of the document. 'document_metadata' values should be a dictionary. All dictionaries must have a 'document_metadata' key if the 'document_metadata' is provided in one of the dictionaries.

    - 'text' and 'chunks' keys are reserved for the text and chunks of the document. 'text' values should be a string and 'chunks' values should be a list of dictionaries.

    - 'begin', 'end', 'result', 'entity' and 'metadata' keys are reserved for the chunk annotations. 'begin' and 'end' values should be integers, 'result' values should be a string, 'entity' values should be a string and 'metadata' values should be a dictionary.

    Example:
    --------

    >>> list_of_dict = [
    ...        {
    ...            "text": "My name is George, and I was born on 12/11/1995. I have the pleasure of working at John Snow Labs.",
    ...            "chunks": [
    ...                {
    ...                    "begin": 11,
    ...                    "end": 16,
    ...                    "result": "George",
    ...                    "entity": "PERSON",
    ...                    "metadata": {"confidence": "1", "ner_source": "llm_output"}
    ...                },
    ...                {
    ...                    "begin": 37,
    ...                    "end": 46,
    ...                    "result": "12/11/1995",
    ...                    "entity": "DATE",
    ...                    "metadata": {"confidence": "0.9", "ner_source": "llm_output"}
    ...                },
    ...                {
    ...                    "begin": 83,
    ...                    "end": 96,
    ...                    "result": "John Snow labs",
    ...                    "entity": "ORG",
    ...                    "metadata": {"confidence": "0.87", "ner_source": "llm_output"}
    ...                }
    ...                ],
    ...            "doc_id": "1",
    ...            "document_metadata": {"dateshift": "10"},
    ...            "file_path": "/path/to/file1"
    ...        },
    ...        {
    ...            "text": "I am Bush, and English is my native language. You can reach me at my email: bush@example.com.",
    ...
    ...            "chunks": [
    ...                {
    ...                    "begin": 5,
    ...                    "end": 8,
    ...                    "result": "Bush",
    ...                    "entity": "PERSON",
    ...                    "metadata": {"confidence": "1", "ner_source": "ner_dl"}
    ...                },
    ...                {
    ...                    "begin": 15,
    ...                    "end": 21,
    ...                    "result": "English",
    ...                    "entity": "LANGUAGE",
    ...                    "metadata": {"confidence": "0.98", "ner_source": "ner_dl"}
    ...                },
    ...                {
    ...                    "begin": 76,
    ...                    "end": 91,
    ...                    "result": "bush@example.com",
    ...                    "entity": "EMAIL",
    ...                    "metadata": {"confidence": "0.87", "ner_source": "ner_dl"}
    ...                }
    ...            ],
    ...            "doc_id": "2",
    ...            "document_metadata": {"dateshift": "5"},
    ...            "file_path": "/path/to/file2"
    ...        }
    ...    ]
    >>> from sparknlp.annotator import *
    >>> from sparknlp_jsl.utils import *
    >>> result_df = dict_to_annotation_converter(spark, list_of_dict, document_col_name="document", chunk_col_name="chunk", adjust_end=False)
    >>> light_deIdentification = (
    ...        LightDeIdentification()
    ...        .setInputCols(["document", "chunk"]) # The document and chunk columns created by the dict_to_annotation_converter function.
    ...        .setOutputCol("deidentified")
    ...        .setMode("mask")
    ...    )
    >>> light_deIdentification.transform(result_df).selectExpr("deidentified.result").show(truncate=False)

        +---------------------------------------------------------------------------------------------+
        |result                                                                                       |
        +---------------------------------------------------------------------------------------------+
        |[My name is <PERSON>, and I was born on <DATE>. I have the pleasure of working at <ORG>.]    |
        |[I am <PERSON>, and <LANGUAGE> is my native language. You can reach me at my email: <EMAIL>.]|
        +---------------------------------------------------------------------------------------------+

    Returns:
    -------
    Spark DataFrame: A Spark DataFrame with the converted columns which are compatible with Spark NLP and is ready to deidentify.
    The returned DataFrame will contain chunk and document columns that are compatible with Spark NLP and can be used for deidentification.
    Additionally, the returned DataFrame will contain all the columns from the input data except the "chunks" column.
    """

    chunks = __dict_to_chunks(data=data, adjust_end=adjust_end)
    chunk_df = __chunks_to_chunk_df(spark=spark, chunks=chunks, chunk_col_name=chunk_col_name)
    keys_to_remove = ['chunks']
    filtered_data = [{k: v for k, v in d.items() if k not in keys_to_remove} for d in data]
    base_df = spark.createDataFrame(filtered_data)
    document_assembler = (
        DocumentAssembler()
        .setInputCol("text")
        .setOutputCol(document_col_name)
    )
    if "document_metadata" in base_df.columns:
        document_assembler = document_assembler.setMetadataCol("document_metadata")

    document_df = document_assembler.transform(base_df)
    from pyspark.sql.functions import monotonically_increasing_id
    return document_df.withColumn("tmp_id", monotonically_increasing_id())\
        .join(chunk_df.withColumn("tmp_id", monotonically_increasing_id()), "tmp_id")\
        .drop("tmp_id")


def __convert_to_compatible_column(spark: SparkSession, data_frame: DataFrame, column_name: str, annotatorType = "chunk"):
    """
    This method is used to convert a column to a compatible column for Spark NLP.
    :param spark: SparkSession
    :param data_frame: The DataFrame that contains the column.
    :param column_name: The name of the column to be converted.
    :param annotatorType: The annotator type of the column to be converted.
    :return: data_frame: A DataFrame with the converted column.
    """
    import json
    from pyspark.sql import Column
    from pyspark.sql.functions import col
    meta = {"annotatorType": annotatorType}
    sc = spark.sparkContext
    jmeta = sc._gateway.jvm.org.apache.spark.sql.types.Metadata
    new_metadata_column = Column(getattr(col(column_name)._jc, "as")("", jmeta.fromJson(json.dumps(meta))))
    return data_frame.withColumn(column_name, new_metadata_column)

def __dict_to_chunks(data: list, adjust_end: bool):
    """
    This method is used to convert a list of dictionaries to chunk annotations.
    :param data: A list of dictionaries.
    :param adjust_end: If True, the end values of the chunks will minus 1.
    :return: chunks: A list of chunk annotations in Row format.
    """
    CHUNK_TYPE = AnnotatorType.CHUNK
    chunks = []
    for raw_dict in data:
        keys = raw_dict.keys()
        if "text" not in keys or "chunks" not in keys:
            raise ValueError("All dictionary should have 'text' and 'chunks' keys")

        raw_chunks = raw_dict["chunks"]
        chunks_a_row = []
        for index, raw_chunk in enumerate(raw_chunks):
            raw_chunk_keys = raw_chunk.keys()
            if "begin" not in raw_chunk_keys or "end" not in raw_chunk_keys or "result" not in raw_chunk_keys or "entity" not in raw_chunk_keys or "metadata" not in raw_chunk_keys:
                raise ValueError("All chunks dictionary should have 'begin', 'end', 'result', 'entity' and 'metadata' keys")
            chunk = Annotation(CHUNK_TYPE, raw_chunk["begin"], raw_chunk["end"] - 1 if adjust_end else raw_chunk["end"], raw_chunk["result"], dict({"sentence": "0", "chunk": str(index), "entity": raw_chunk["entity"]}, **raw_chunk["metadata"]), [])
            chunks_a_row.append(chunk)
        chunks.append(Row(chunks_a_row))

    return chunks

def __chunks_to_chunk_df(spark: SparkSession, chunks: list, chunk_col_name: str):
    """
    This method is used to convert a list of chunk annotations to a DataFrame with a chunk column.
    :param spark: SparkSession
    :param chunks: list[Row]
    :param chunk_col_name: column name for the chunk column
    :return: A DataFrame with the chunk column.
    """
    from pyspark.sql.types import StructField, StructType
    struct_field_chunk = StructField(chunk_col_name, Annotation.arrayType(), nullable=False)
    struct_chunk = StructType([struct_field_chunk])
    chunk_df = spark.createDataFrame(chunks, struct_chunk)
    return __convert_to_compatible_column(spark=spark, data_frame=chunk_df, column_name=chunk_col_name, annotatorType=AnnotatorType.CHUNK)

