class PipelineOutputParser:
    """
    This class is used to parse the output of a Spark NLP pipeline.

    It provides methods for extracting named entities, assertions, code mapping, relations, summaries, and deidentification from the output.

    Examples
    --------
    >>> import sparknlp
    >>> import sparknlp_jsl
    >>> from sparknlp.pretrained import PretrainedPipeline
    >>> from sparknlp_jsl.pipeline_output_parser import PipelineOutputParser

    Load the pipeline and the text

    >>> text = [
    ...     "Immunohistochemistry was negative for thyroid transcription factor-1 and napsin A. The test was positive for ER and PR, and negative for HER2.",
    ...     "The had previously undergone a left mastectomy and an axillary lymph node dissection for a left breast cancer twenty years ago." +
    ...     "The tumor was positive for ER and PR. Postoperatively, radiotherapy was administered to the residual breast. The cancer recurred as a right lung metastasis 13 years later." +
    ...     "He underwent a regimen consisting of adriamycin (60 mg/m2) and cyclophosphamide (600 mg/m2) over six courses, as first line therapy."
    ... ]

    >>> oncology_pipeline = PretrainedPipeline("oncology_biomarker_pipeline", "en", "clinical/models")
    >>> annotations = oncology_pipeline.fullAnnotate(text)

    Define the column_maps dictionary

    >>> column_maps = {
    ...     'document_identifier': 'XYZ_123',
    ...     'document_text': 'document',
    ...     'entities': ["merged_chunk"],
    ...     'assertions': ["assertion_chunk"],
    ...     'resolver': ["icd10_code"]
    ...     'relations': [],
    ...     'summaries': [],
    ...     "deidentifications" : [],
    ...     "classifications":[]
    ... }

    Initialized parser from the dictionary

    >>> pipeline_parser = PipelineOutputParser(column_maps)

    Run the parser on the output of a Spark NLP pipeline

    >>> parsed_result = pipeline_parser.run(annotations)
    >>> print(parsed_result)

    """

    def __init__(self, columns_maps=None):
        """
        Initialize the PipelineOutputParser class.

        Args:
            columns_maps (dict, optional): A dictionary containing the column mappings. Defaults to None.

        Attributes:
            document_identifier (str): The identifier for the document.
            document_text (str): The text of the document.
            entities (list): A list of entity column names.
            assertions (list): A list of assertion column names.
            resolutions (list): A list of resolution column names.
            relations (list): A list of relation column names.
            summaries (list): A list of summary column names.
            deidentifications (list): A list of deidentification column names.
            classifications (list): A list of classification column names.
        """
        # for map_key, map_value in columns_maps.items():
        #     try :
        #         self[map_key] =  map_value
        #     except KeyError:
        #         self[map_key] =  None
        #         print("""Key does not exist in the dictionary.
        #         Please provide a dictionary containing the 'document_identifier', 'document_text', 'entities', 'relations', 'summary', and 'deidentification' keys.""")

        if columns_maps is None:
            columns_maps = {
                'document_identifier': None,
                'document_text': [],
                'entities': [],
                'assertions': [],
                'resolutions': [],
                'relations': [],
                'summaries': [],
                'deidentifications': [],
                'classifications': []
            }
        try :
            self.document_identifier = columns_maps["document_identifier"] if _is_not_none_or_empty(columns_maps.get("document_identifier")) else None
            self.document_text = columns_maps["document_text"] if _is_not_none_or_empty(columns_maps.get("document_text")) else []
            self.entities = columns_maps["entities"] if _is_not_none_or_empty(columns_maps.get("entities")) else []
            self.assertions = columns_maps["assertions"] if _is_not_none_or_empty(columns_maps.get("assertions")) else []
            self.resolutions = columns_maps["resolutions"] if _is_not_none_or_empty(columns_maps.get("resolutions")) else []
            self.relations = columns_maps["relations"] if _is_not_none_or_empty(columns_maps.get("relations")) else []
            self.summaries = columns_maps["summaries"] if _is_not_none_or_empty(columns_maps.get("summaries")) else []
            self.deidentifications = columns_maps["deidentifications"] if _is_not_none_or_empty(columns_maps.get("deidentifications")) else []
            self.classifications = columns_maps["classifications"] if _is_not_none_or_empty(columns_maps.get("classifications")) else []
        except KeyError:
            print("""Key does not exist in the dictionary.
            Please provide a dictionary containing the 'document_identifier', 'document_text', 'entities', 'assertions', 'resolutions', 'relations', 'summaries', 'deidentifications' and 'classifications' keys.""")


    def check_column_mapping_dict(self, result: dict):
        """
        This function is used to check if the column mapping dictionary is valid.
        It checks if the following keys are present in the dictionary: document_text, entities, relations, and summary.
        It also checks if the values of the entities, relations, and summary keys are also dictionaries, and if the keys ner_chunk_column_name, resolver_column_name, and assertion_column_name are present in these dictionaries.
        Args:
            result (list): The output of the pipeline.
        Raises:
            KeyError: If a key is missing from the column mapping dictionary.
        """

        result_column_name_list = list(result.keys())
        result_column_name_list.append(None)

        if self.entities and self.relations and self.summaries and self.document_text and self.deidentifications and self.classifications:

            if self.document_text not in result_column_name_list:
                raise KeyError(f"The column name '{self.document_text}' is not present in the results.")

            for entity_col in self.entities:
                if entity_col not in result_column_name_list:
                    raise KeyError(f"The column name '{entity_col}' is not present in the results.")

            for assertion_col in self.assertions:
                if assertion_col not in result_column_name_list:
                    raise KeyError(f"The column name '{assertion_col}' is not present in the results.")

            for resolution_col_list in self.resolutions:
                if resolution_col_list["resolver_column_name"] not in result_column_name_list:
                    raise KeyError(f"""The column name '{resolution_col_list["resolver_column_name"]}' is not present in the results.""")
                # elif resolution_col_list["vocab"] not in result_column_name_list:
                #     raise KeyError(f"""The column name '{resolution_col_list["vocab"]}' is not present in the results.""")

            for relation_col in self.relations:
                if relation_col not in result_column_name_list:
                    raise KeyError(f"The column name '{relation_col}' is not present in the results.")

            for summary_col in self.summaries:
                if summary_col not in result_column_name_list:
                    raise KeyError(f"The column name '{summary_col}' is not present in the results.")

            for deidentification_col_list in self.deidentifications:
                if deidentification_col_list["original"] not in result_column_name_list:
                    raise KeyError(f"""The column name '{deidentification_col_list["original"]}' is not present in the results.""")
                elif deidentification_col_list["obfuscated"] not in result_column_name_list:
                    raise KeyError(f"""The column name '{deidentification_col_list["obfuscated"]}' is not present in the results.""")
                elif deidentification_col_list["masked"] not in result_column_name_list:
                    raise KeyError(f"""The column name '{deidentification_col_list["masked"]}' is not present in the results.""")

            for classification_col_list in self.classifications:
                if classification_col_list["classification_column_name"] not in result_column_name_list:
                    raise KeyError(f"""The column name '{classification_col_list["classification_column_name"]}' is not present in the results.""")
                elif classification_col_list["sentence_column_name"] not in result_column_name_list:
                    raise KeyError(f"""The column name '{classification_col_list["sentence_column_name"]}' is not present in the results.""")


    def get_unique_id(self, ner_chunk_result, ner_chunk_begin, ner_chunk_end, ner_label):
        """
        Generate a unique identifier for a named entity chunk.

        Returns:
        str: A unique identifier for the named entity chunk, encoded in base64. The identifier is truncated to the first 8 characters.

        Note:
        This method uses the uuid3 function from the uuid module to generate a unique identifier based on the provided parameters.
        The identifier is then encoded in base64 and truncated to the first 8 characters.
        """
        import uuid

        uid = uuid.uuid3(uuid.NAMESPACE_DNS, f"{ner_chunk_result}_{ner_chunk_begin}_{ner_chunk_end}_{ner_label}")

        return str(uid)[:8]


    def check_unique_id(self, entities, ner_chunk_result, ner_chunk_begin, ner_chunk_end):

        for entity_record in entities:
            if ((entity_record['chunk'] == ner_chunk_result) and
                (entity_record['begin'] == ner_chunk_begin ) and
                (entity_record['end'] == ner_chunk_end) ) :

                return entity_record['chunk_id']

        return 'None'


    def get_documents(self, result):
        """
        Retrieves the document text from the pipeline output.

        Arg:
        result (list): The output of the pipeline from fullAnnotate. This list contains annotations for each document.

        Returns:
        list: A list of strings, where each string represents the text of a document.
        """
        document_result = []
        for annotation in result[self.document_text]:
            document_result.append(annotation.result)
        return document_result


    def get_entities(self, result):
        """
        Extract named entities from the pipeline output.

        Args:
            result (list): The output of the pipeline.

        Returns:
            entity_results (list): A list of dictionaries, where each dictionary represents a named entity.
            Each dictionary contains the following keys:
                - chunk_id (str): The unique identifier for the entity.
                - chunk (str): The text of the entity.
                - begin (int): The index of the beginning of the entity in the text.
                - end (int): The index of the end of the entity in the text.
                - ner_label (str): The label of the entity, as determined by the NER model.
                - ner_source (str): The source of the NER model that generated this entity.
                - ner_confidence (float): The confidence score of the NER model in identifying this entity.
        """

        entity_results = []
        for ner_chunk_col in self.entities:
            for entity in result[ner_chunk_col]:
                entity_record = {
                    'chunk_id': self.get_unique_id(entity.result, entity.begin, entity.end, entity.metadata['entity']),
                    'chunk': entity.result,
                    'begin': entity.begin,
                    'end': entity.end,
                    'ner_label': entity.metadata['entity'],
                    'ner_source': entity.metadata['ner_source'],
                    'ner_confidence': entity.metadata['confidence'],
                    }

                if entity_record not in entity_results:
                    entity_results.append(entity_record)

        return sorted(entity_results, key=lambda d: d['begin'])


    def get_assertions(self, result):
        """
        Extracts assertions from the pipeline output.

        Args:
            assertion_result (list): The output of the assertion column.

        Returns:
            A list of dictionaries, where each dictionary represents an assertion, containing the following keys:
                - chunk_id (str): The unique identifier for the chunk of text that the assertion applies to.
                - chunk (str): The text of the chunk.
                - assertion (str): The assertion status.
                - assertion_confidence (str): The confidence score of the assertion model in identifying assertion status.
                - assertion_source (str): The source of the assertion information
        """

        assertion_results = []
        for assertions_col in self.assertions:

            for assertion in result[assertions_col]:
                assertion_record = {
                    'chunk_id': self.get_unique_id(assertion.metadata['ner_chunk'], assertion.begin, assertion.end, assertion.metadata['ner_label']),
                    'chunk':assertion.metadata['ner_chunk'],
                    'assertion': assertion.result,
                    'assertion_confidence': assertion.metadata.get("confidence") if _is_not_none_or_empty(assertion.metadata.get("confidence")) else "null",
                    'assertion_source': assertion.metadata.get("assertion_source") if _is_not_none_or_empty(assertion.metadata.get("assertion_source")) else "null",
                    }

                if assertion_record not in assertion_results:
                    assertion_results.append(assertion_record)

        return assertion_results


    def get_resolutions(self, result, entities):
        """
        Extracts terminology codes from the pipeline output.

        Returns:
            A list of dictionaries, where each dictionary represents a named entity, containing the following keys:
                - vocab (str): The vocablary information about teminology codes.
                - chunk_id (str): The unique identifier for entity.
                - chunk (str): The text of the chunk.
                - term_code (str): The code of the term.
                - resolutions (str): The resolved text of the term.
                - all_k_codes (list): A list of all k results.
                - all_k_resolutions (list): A list of all k resolutions.
                - all_k_aux_labels (list): A list of all k aux labels.
                - all_k_distances (list): A list of all k distances.
                - confidence (float): The confidence score of the resolutions.
        """
        resolution_result = []
        for resolution_col in self.resolutions:
            for term_codes in result[resolution_col['resolver_column_name']]:

                try:
                    if str( term_codes.metadata['entity']) != "<class 'NoneType'>":
                        chunk_id = self.get_unique_id(term_codes.metadata['target_text'], term_codes.begin, term_codes.end, term_codes.metadata['entity'])
                except:
                    chunk_id = self.check_unique_id(self, entities, term_codes.metadata['target_text'], term_codes.begin, term_codes.end)

                resolution_record = {
                    'vocab': resolution_col['vocab'],
                    'chunk_id': chunk_id,
                    'chunk': term_codes.metadata['target_text'],
                    'code': term_codes.result,
                    'resolutions': term_codes.metadata['resolved_text'],
                    'all_k_codes': term_codes.metadata['all_k_results'],
                    'all_k_resolutions': term_codes.metadata['all_k_resolutions'],
                    'all_k_aux_labels': term_codes.metadata['all_k_aux_labels'],
                    'all_k_distances': term_codes.metadata['all_k_distances'],
                    'confidence': term_codes.metadata['confidence'],
                }
                if resolution_record not in resolution_result:
                    resolution_result.append(resolution_record)

        return resolution_result


    def get_relations(self, result, return_relation_entities = False):
        """
        Extract relations from the pipeline output.

        Args:
            result (list): The output of the pipeline.

        Returns:
            A list of dictionaries, where each dictionary represents a relation, containing the following keys:
                - relation (str): The type of relation.
                - entity1 (str): The first entity envolved in the relation.
                - chunk1_id (str): The unique identifier for the first chunk.
                - entity1_begin (int): The index of the beginning of the first chunk.
                - entity1_end (int): The index of the end of the first chunk.
                - chunk1 (str): The text of the first chunk.
                - entity2 (str): The second entity envolved in the relation.
                - chunk2_id (str): The unique identifier for the second chunk.
                - entity2_begin (int): The index of the beginning of the second chunk.
                - entity2_end (int): The index of the end of the second chunk.
                - chunk2 (str): The text of the second chunk.
                - confidence (float): The confidence score of the relation.
                - direction (str): The direction of the relation.
        """
        relation_results = []
        for rel_col in self.relations:
            for rel in result[rel_col]:

                if return_relation_entities:
                    relation_record = {
                        'relation': rel.result,
                        'chunk1_id': self.get_unique_id(rel.metadata['chunk1'], rel.metadata['entity1_begin'], rel.metadata['entity1_end'], rel.metadata['entity1']),
                        'chunk1': rel.metadata['chunk1'],
                        'entity1': rel.metadata['entity1'],
                        'entity1_begin': rel.metadata['entity1_begin'],
                        'entity1_end': rel.metadata['entity1_end'],
                        'chunk2_id': self.get_unique_id(rel.metadata['chunk2'], rel.metadata['entity2_begin'], rel.metadata['entity2_end'], rel.metadata['entity2']),
                        'chunk2': rel.metadata['chunk2'],
                        'entity2': rel.metadata['entity2'],
                        'entity2_begin': rel.metadata['entity2_begin'],
                        'entity2_end': rel.metadata['entity2_end'],
                        'confidence': rel.metadata['confidence'],
                        'direction': rel.metadata['direction']
                    }
                    if relation_record not in relation_results:
                        relation_results.append(relation_record)

                else:
                    relation_record = {
                        'relation': rel.result,
                        'chunk1_id': self.get_unique_id(rel.metadata['chunk1'], rel.metadata['entity1_begin'], rel.metadata['entity1_end'], rel.metadata['entity1']),
                        'chunk1': rel.metadata['chunk1'],
                        'chunk2_id': self.get_unique_id(rel.metadata['chunk2'], rel.metadata['entity2_begin'], rel.metadata['entity2_end'], rel.metadata['entity2']),
                        'chunk2': rel.metadata['chunk2'],
                        'confidence': rel.metadata['confidence'],
                        'direction': rel.metadata['direction']
                    }

                    if relation_record not in relation_results:
                        relation_results.append(relation_record)

        if return_relation_entities:
            return sorted(relation_results, key=lambda d: d['entity1_begin'])


        return relation_results


    def get_summary(self, result):
        """
        Generates summary from the provided text.

        Returns:
            Summarized text
        """
        summary_result = []
        for summary_col in self.summaries:
            for annotation in result[summary_col]:
                summary_result.append(annotation.result)

        return summary_result


    def get_deidentification(self, result ):
        """
        Extract deidentification text from the pipeline output.

        Args:
            result (list): The output of the pipeline.

        Returns:
            A list of dictionaries, where each dictionary represents a deidentification containing the following keys:
                - original (list): A list of original text.
                - obfuscated (list): A list of deidentified text.
                - masked (list):  A list of masked text.
        """
        deidentification_list = []
        for deid_pairs in self.deidentifications:
            original_text_list = []
            obfuscated_text_list = []
            masked_text_list = []

            if _is_not_none_or_empty(deid_pairs["original"]):
                original_text_list = [original_text.result for original_text in result[deid_pairs["original"]]]

            if _is_not_none_or_empty(deid_pairs["obfuscated"]):
                obfuscated_text_list = [obfuscated_text.result for obfuscated_text in result[deid_pairs["obfuscated"]]]

            if _is_not_none_or_empty(deid_pairs["masked"]):
                masked_text_list = [masked_text.result for masked_text in result[deid_pairs["masked"]]]
            else:
                try:
                    masked_text_list = [masked_text.metadata["masked"] for masked_text in result[deid_pairs["obfuscated"]]]
                except:
                    raise KeyError(f"If you have set the setMetadataMaskingPolicy('entity_labels') parameter, you must set the 'masked' field to None, and provide obfuscated column name")

            deidentification_list.append({
                "original": original_text_list,
                "obfuscated": obfuscated_text_list,
                "masked": masked_text_list
            })

        return deidentification_list


    def get_classification(self, result):
        """
        This function retrieves classification results from the pipeline output.

        Parameters:
        result (list): The output of the pipeline from fullAnnotate. This list contains annotations for each document.

        Returns:
        list: A list of dictionaries, where each dictionary represents a classification result.
            Each dictionary contains the following keys:
            - category (str): The category of the classification.
            - sentence (str): The sentence from which the classification was made.
            - sentence_id (int): The ID of the sentence.
        """

        classification_result = []
        for classification_col_list in self.classifications:
            if (_is_not_none_or_empty(classification_col_list["classification_column_name"]) and _is_not_none_or_empty(classification_col_list["sentence_column_name"])):
                for classification_annotation, sentence_annotation in zip(result[classification_col_list["classification_column_name"]],
                                                                          result[classification_col_list["sentence_column_name"]]):
                    classification_result.append({
                        "category":classification_annotation.result,
                        "sentence":sentence_annotation.result,
                        "sentence_id":sentence_annotation.metadata["sentence"]
                    })

        return classification_result


    def update_columns_maps(self, columns_maps=None):
        """
        Update the column mapping dictionary.

        Args:
            columns_maps (dict): A dictionary containing the column mappings.
        """
        return PipelineOutputParser(columns_maps)


    def run(self, results, return_relation_entities = False):
        """
        Parse the pipeline output.

        Args:
            results (list): The output of the pipeline from fullAnnotate.

        Returns:
            A dictionary containing the parsed results, where the key is "result" and the value is a list of dictionaries, where each dictionary represents a document, containing the following keys:
                - document_identifier (str): The informations of the document.
                - document_id (str): The ID of the document.
                - document_text (str): The text of the document.
                - entities (list): A list of dictionaries, where each dictionary represents a named entity, containing the following keys:
                    - chunk_id (str): The unique identifier for the chunk.
                    - chunk (str): The text of the entity.
                    - begin (int): The index of the beginning of the entity in the text.
                    - end (int): The index of the end of the entity in the text.
                    - ner_label (str): The label of the entity, as determined by the NER model.
                    - ner_source (str): The source of the NER model that generated this entity.
                    - ner_confidence (float): The confidence score of the NER model in identifying this entity.
                - assertions (list): A list of dictionaries, where each dictionary represents a assertion, containing the following keys:
                    - chunk_id (str): The unique identifier for the chunk.
                    - chunk (str): The text of the chunk.
                    - assertion (str): The assertion status.
                    - assertion_confidence (str): The confidence score of the assertion model in identifying assertion status.
                    - assertion_source (str): The source of the assertion information
                - resolutions (list): A list of dictionaries, where each dictionary represents a resolutions, containing the following keys:
                    - vocab (str): The vocablary information about teminology codes.
                    - chunk_id (str): The unique identifier for the chunk.
                    - chunk (str): The text of the chunk.
                    - term_code (str): The code of the term.
                    - resolutions (str): The resolved text of the term.
                    - all_k_codes (list): A list of all k results.
                    - all_k_resolutions (list): A list of all k resolutions.
                    - all_k_aux_labels (list): A list of all k aux labels.
                    - all_k_distances (list): A list of all k distances.
                    - confidence (float): The confidence score of the resolutions.
                - relations (list): A list of dictionaries, where each dictionary represents a relation, containing the following keys:
                    - relation (str): The type of relation.
                    - entity1 (str): The first entity envolved in the relation.
                    - chunk1_id (str): The unique identifier for the first chunk.
                    - entity1_begin (int): The index of the beginning of the first chunk.
                    - entity1_end (int): The index of the end of the first chunk.
                    - chunk1 (str): The text of the first chunk.
                    - entity2 (str): The second entity envolved in the relation.
                    - chunk2_id (str): The unique identifier for the second chunk.
                    - entity2_begin (int): The index of the beginning of the second chunk.
                    - entity2_end (int): The index of the end of the second chunk.
                    - chunk2 (str): The text of the second chunk.
                    - confidence (float): The confidence score of the relation.
                    - direction (str): The direction of the relation.
                - deidentifications: (list) A list of dictionaries, where each dictionary represents a deidentification, containing the following keys:
                    - original (list): A list of original text.
                    - obfuscated (list): A list of deidentified text.
                    - masked (list):  A list of masked text.
                - classifications (list):  A list of dictionaries, where each dictionary represents a classification result.
                    - category (str): The category of the classification.
                    - sentence (str): The sentence from which the classification was made.
                    - sentence_id (int): The ID of the sentence.

        """

        self.check_column_mapping_dict(results[0])

        result_list = []

        for idx, result in enumerate(results):
            document_identifier = self.document_identifier if self.document_identifier else 'XXXX'
            document_id = idx
            document_text = self.get_documents(result) if self.document_text else []
            entities = self.get_entities(result) if self.entities else []
            assertions = self.get_assertions(result) if self.assertions else []
            resolutions = self.get_resolutions(result, entities) if self.resolutions else []
            relations = self.get_relations(result, return_relation_entities) if self.relations else []
            summaries = self.get_summary(result) if self.summaries else []
            deidentifications = self.get_deidentification(result) if self.deidentifications else []
            classifications = self.get_classification(result) if self.classifications else []

            result_list.append({
                'document_identifier': document_identifier,
                'document_id': document_id,
                'document_text': document_text,
                'entities': entities,
                'assertions': assertions,
                'resolutions': resolutions,
                'relations': relations,
                'summaries': summaries,
                'deidentifications': deidentifications,
                'classifications': classifications
            }
        )

        return {f"result": result_list}


def _is_not_none_or_empty(value):
    if value is not None:
        if isinstance(value, str) and value.strip() != "":
            return True
        elif isinstance(value, list) and len(value) > 0:
            return True
    return False