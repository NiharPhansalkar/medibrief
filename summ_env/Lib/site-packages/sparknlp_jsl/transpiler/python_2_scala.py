"""
 The following script provides a set of functions for transpiling Python code to Scala, focusing on
 specific adaptations required for Spark and SparkNLP libraries. It includes functionalities such as
 converting Python dictionaries to Scala Maps, converting class names,generating import sections for
 Spark-related libraries, and executing various code transformations.
"""

import re
from textwrap import dedent


from sparknlp_jsl.transpiler.class_names import ClassNames

from sparknlp_jsl.transpiler.source_loader import load_file_as_dict


# Convert Python Dictionary to Scala Map
def convert_python_dict_to_scala_script(py_code):
    """
    Converts a Python dictionary in the provided code to Scala Map.

    Args:
        py_code (str): Python code containing a dictionary.

    Returns:
        str: Scala code with the dictionary converted to a Map.
    """
    match = re.search(r'\{[^{}]+\}', py_code)
    if match:
        python_dict_str = match.group(0)

        try:
            python_dict = eval(python_dict_str)
        except Exception as e:
            return f"Error: {str(e)}"

        scala_map_str = "Map("
        for key, value in python_dict.items():
            scala_map_str += f'"{key}" -> "{value}", '
        scala_map_str = scala_map_str.rstrip(', ')
        scala_map_str += ")"

        scala_script = py_code.replace(python_dict_str, scala_map_str)

        return scala_script
    else:
        return py_code


def convert_class_name(py_code, classes):
    """
    Converts Python class names to Scala class names based on the provided mapping.

    Args:
        py_code (str): Python code containing class names.
        classes (dict): Mapping of Python class names to Scala class names.

    Returns:
        str: Scala code with class names converted.
    """
    for pythonClassName, scalaClassName in classes.items():
        py_code = py_code.replace(pythonClassName, scalaClassName)

    return py_code


def find_key_values(text, dictionary):
    """
    Finds key-value pairs from the given text that match a provided dictionary.

    Args:
        text (str): Text to search for key-value pairs.
        dictionary (dict): Dictionary to match key-value pairs.

    Returns:
        dict: Key-value pairs found in the text.
    """
    # TODO to find exact match EX: NerConverter and NerConverterInternal
    result = {}
    for key in dictionary:
        if key in text:
            result[key] = dictionary[key]
    return result


def get_import_section(py_code, data_dict):
    """
    Generates the import section for Spark-related libraries based on the provided Python code.

    Args:
        py_code (str): Python code.
        data_dict (dict): Dictionary containing data for import sections.

    Returns:
        str: Import section for Spark-related libraries.
    """
    import_section = "import org.apache.spark.ml.Pipeline\n" + "import org.apache.spark.sql.SparkSession\n"

    found_values = find_key_values(py_code, data_dict)

    for key, value in found_values.items():
        import_section += f"import {value}\n"

    dedent(import_section)
    return import_section


def convert(py_code):
    """
    Convert Python code to Scala code.

    Args:
        py_code (str): The input Python code.

    Returns:
        str: The converted Scala code.
    """
    # Perform various replacements to adapt Python code to Scala syntax
    py_code = py_code.replace("val\n", "val ")
    py_code = py_code.replace(")", ")  \n")
    py_code = py_code.replace("nlp.", "")
    py_code = py_code.replace("medical.", "")
    py_code = py_code.replace("finance.", "")
    py_code = py_code.replace("legal.", "")
    py_code = py_code.replace("'", '"')
    py_code = py_code.replace(", ", ",")
    py_code = py_code.replace("True", "true")
    py_code = py_code.replace("False", "false")
    py_code = py_code.replace("#", "//")
    py_code = py_code.replace("Pipeline(stages=[", "new Pipeline().setStages(Array(")
    py_code = py_code.replace("[", "Array(")
    py_code = py_code.replace("]", ")")

    # Exceptional Conditions
    Y = re.split(r'\s+|\n', py_code)
    for i in range(0, len(Y) - 1):
        if Y[i] == "=":
            Y[i - 1] = "\nval " + Y[i - 1]

    for i in range(0, len(Y) - 1):
        if "Pipeline(" in Y[i] and "stages" in Y[i + 1]:
            Y[i] = "new Pipeline()"
            Y[i + 1] = ".setStages(Array("

    for i in range(0, len(Y) - 1):
        if Y[i] == "=" and Y[i - 1] != "val pipeline" and Y[i + 1][0].isupper() and not Y[i + 1].startswith("Array"):
            if ".pretrained" not in Y[i + 1] and ".pretrained" not in Y[i + 2] and ".pretrained" not in Y[i + 3]:
                Y[i + 1] = "new " + Y[i + 1] + "\n"
            else:
                Y[i + 1] = Y[i + 1] + "\n"
                if "()" in Y[i + 1] and "()" in Y[i + 1].split(".")[0]:
                    Y[i + 1] = Y[i + 1].split(".")[0].replace("()", "") + ".".join(Y[i + 1].split(".")[1:])

    for i in range(0, len(Y) - 1):
        if "spark.createDataFrame" in Y[i] and "toDF" in Y[i + 1]:
            text = Y[i]

            match = re.search(r'Array\(Array\((.*?)\)\)', text)
            value_between_arrays = match.group(1)

            index = Y[i].find("spark")
            if index != -1:
                rest = text[:index]
            else:
                print("defined word couldn't find")

            Y[i] = rest + "Seq("
            Y[i] += value_between_arrays
            Y[i] += ")"

    scala_code = ' '.join(Y)
    return scala_code


def anonymize_script(script):
    """
    Anonymize string values in the script.

    Args:
        script (str): The input script.

    Returns:
        Tuple[str, List[str]]: Anonymized script and a list of sensitive values.
    """
    sensitive_values = []

    def replace_sensitive(match):
        value = match.group(1)
        sensitive_values.append(value)
        return "********"

    # Replace string values with placeholders
    anonymized_script = re.sub(r'"([^"]*)"', replace_sensitive, script)

    return anonymized_script, sensitive_values


def restore_sensitive_values(anonymized_script, sensitive_values):
    """
    Restore sensitive values in the anonymized script.

    Args:
        anonymized_script (str): The anonymized script.
        sensitive_values (List[str]): List of sensitive values.

    Returns:
        str: The script with restored sensitive values.
    """
    for value in sensitive_values:
        anonymized_script = anonymized_script.replace("********", f'"{value}"', 1)
    return anonymized_script


def remove_blank_lines(script):
    """
    remove blank lines.

    Args:
        script (str): The input script.

    Returns:
        str: The cleaned script.
    """

    lines = script.split('\n')

    # Remove blank lines
    non_blank_lines = [line.strip() for line in lines if line.strip()]

    # Remove extra spaces and format properly
    cleaned_script = '\n\t'.join(non_blank_lines)

    return cleaned_script


def break_line_after_backslash(script):
    script = script.replace("\\", " \n ")
    return script


def prepare_scala_code(is_spark_initialized, import_section, scala_code):
    """
    Prepare Scala code for build by adding import sections and, if specified, Spark session configuration.

    Args:
        is_spark_initialized (bool): Flag indicating whether Spark session configuration should be added.
        import_section (bool): Flag indicating whether import sections should be added.
        scala_code (str): Scala code to be prepared for build.

    Returns:
        str: Scala code ready for build.

    Notes:
        If 'is_spark_initialized' is True, the function adds Spark session configuration to the Scala code.
        If 'import_section' is True, the function adds import sections to the Scala code.
    """
    final_code = scala_code

    data_dict = get_data_dict()

    spark_section = """  
val spark: SparkSession = SparkSession
    .builder()
    .appName("Traspiler")
    .master("local[*]")
    .config("spark.driver.memory", "8G")
    .config("spark.kryoserializer.buffer.max", "0")
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    .getOrCreate()
com.johnsnowlabs.util.start.registerListenerAndStartRefresh()
"""

    if is_spark_initialized and 'spark.implicits._' not in final_code:
        final_code = spark_section + "import spark.implicits._ \n" + final_code

    if import_section :
        final_code = get_import_section(final_code, data_dict) + final_code

    return final_code


def get_data_dict():
    """
    Load data dictionary from external sources.

    Returns:
    - dict: Data dictionary.
    """

    internal_classes_list = ClassNames.internal_classes
    open_source_class_list = ClassNames.openSource_classes
    data_dict = load_file_as_dict(internal_classes_list, open_source_class_list)
    return data_dict


def run_transpiler(py_code):
    """
    Execute all conversion and processing steps.

    Args:
        py_code (str): The input Python code.

    Returns:
        str: The final processed Scala code.
    """

    anonymized_script, sensitive_values = anonymize_script(py_code)

    py_code = convert_python_dict_to_scala_script(anonymized_script)
    py_code = convert_class_name(py_code, ClassNames.classes)

    scala_code = convert(py_code)

    scala_code_with_imports = prepare_scala_code(is_spark_initialized=True, import_section=True, scala_code=scala_code)

    restored_script = restore_sensitive_values(scala_code_with_imports, sensitive_values)

    restored_script = restored_script.replace("\\", "\n")
    restored_script = remove_blank_lines(restored_script)

    restored_script = restored_script.replace("val", "\nval")
    restored_script = restored_script.replace("\timport", "import")

    # restored_script = break_line_after_backslash(restored_script)

    return restored_script
