"""
This script provides functions for preparing, building, and executing Scala and Python code.
It includes functionalities for managing Spark session configuration, importing necessary libraries, and transpiling
Python code to Scala.

"""
import subprocess
from textwrap import dedent
from pyspark.sql import SparkSession
from sparknlp_jsl.utils.run_transpiled_code import RunTranspiledCode


def build_scala_code(scala_code):
    """
    Build Scala code using the RunTranspiledCode class.

    Parameters:
    - scala_code (str): Scala code to be built.

    Returns:
    - str: Result of building Scala code.
    """
    scala_build_result = RunTranspiledCode.build_scala_code(scala_code)
    return scala_build_result


def compile_scala_code(scala_code):
    """
    Compile Scala code using the RunTranspiledCode class without execution.

    Parameters:
    - scala_code (str): Scala code to be built.

    Returns:
    - str: Result of building Scala code.
    """

    scala_build_result = RunTranspiledCode.compile_scala_code(scala_code)
    return scala_build_result


def prepare_python_code(spark=None, import_section="", py_code=""):
    """
   Merge Python code with necessary imports and, if specified, Spark session configuration.

   Parameters:
   - spark (SparkSession or None): If provided, it should be an instance of SparkSession.
   - import_section (bool): Flag indicating whether import sections should be added.
   - py_code (str): Python code to be merged.

   Returns:
   - str: Merged Python code with required imports and optional Spark session configuration.

   Raises:
   - TypeError: If 'spark' is provided but not an instance of SparkSession.
   """
    if spark is not None and not isinstance(spark, SparkSession):
        raise TypeError("spark must be instance of SparkSession")


    imports = """
import os

from pyspark.ml import Pipeline, PipelineModel
from pyspark.sql import SparkSession

import sparknlp
import sparknlp_jsl

from sparknlp.annotator import *
from sparknlp_jsl.annotator import *
from sparknlp.base import *
from sparknlp.util import *
from sparknlp.pretrained import ResourceDownloader
from pyspark.sql import functions as F
"""

    if spark is not None:
        spark_jars = ""
        spark_jars_packages = ""
        conf = spark.sparkContext.getConf()
        configs = conf.getAll()
        for key in configs:
            if "spark.jars" == key[0]:
                spark_jars = key[1]
            if "spark.jars.packages" == key[0]:
                spark_jars_packages = key[1]
        spark_section = rf"""
spark = SparkSession.builder\
    .appName("Transpiler")\
    .master("local[*]")\
    .config("spark.driver.memory", "8G")\
    .config("spark.driver.maxResultSize", "0")\
    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer")\
    .config("spark.jars", "{spark_jars}")\
    .config("spark.jars.packages", "{spark_jars_packages}")\
    .getOrCreate()
"""

        spark_section = dedent(spark_section)

        py_code = spark_section + "\n" + py_code

    if import_section:
        py_code = imports + "\n" + py_code

    return py_code


def build_python_code(py_code):
    """
    Build Python code using subprocess.

    Parameters:
    - py_code (str): Python code to be built.

    Returns:
    - str: Result of building Python code.
    """
    try:
        # Run the Python script
        result = subprocess.run(["python", "-c", py_code], capture_output=True, text=True, check=True)

        # If successful, return the output
        print(result.stdout.strip())
        return "0"
    except subprocess.CalledProcessError as e:
        # In case of error, return the error message
        print(f"An error occurred: {e.stderr}")
        return "1"
