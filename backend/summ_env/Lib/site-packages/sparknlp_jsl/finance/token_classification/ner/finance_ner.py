import sys

from sparknlp.annotator import NerApproach
from sparknlp_jsl.common import *
import sparknlp
from sparknlp.internal import ExtendedJavaWrapper
from sparknlp_jsl.annotator.ner.medical_ner import MedicalNerApproach as A
from sparknlp_jsl.annotator.ner.medical_ner import MedicalNerModel as M


class FinanceNerApproach(A):
    """Trains generic NER model based on Neural Networks.

    The architecture of the neural network is a Char CNNs - BiLSTM - CRF that
    achieves state-of-the-art in most datasets.

    For instantiated/pretrained models, see :class:`.NerDLModel`.

    The training data should be a labeled Spark Dataset, in the format of
    :class:`.CoNLL` 2003 IOB with `Annotation` type columns. The data should
    have columns of type ``DOCUMENT, TOKEN, WORD_EMBEDDINGS`` and an additional
    label column of annotator type ``NAMED_ENTITY``.

    Excluding the label, this can be done with for example:

    - a SentenceDetector,
    - a Tokenizer and
    - a WordEmbeddingsModel (any embeddings can be chosen, e.g. BertEmbeddings
      for BERT based embeddings).

    For extended examples of usage, see the `Spark NLP Workshop <https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/jupyter/training/english/dl-ner>`__.

    ==================================== ======================
    Input Annotation types               Output Annotation type
    ==================================== ======================
    ``DOCUMENT, TOKEN, WORD_EMBEDDINGS`` ``NAMED_ENTITY``
    ==================================== ======================

    Notes
    -----
        Both DocumentAssembler and SentenceDetector annotators are
        annotators that output the ``DOCUMENT`` annotation type. Thus, any of them
        can be used as the first annotators in a pipeline.

    Parameters
    ----------
    labelColumn
        Column with label per each token
    entities
        Entities to recognize
    minEpochs
        Minimum number of epochs to train, by default 0
    maxEpochs
        Maximum number of epochs to train, by default 50
    verbose
        Level of verbosity during training, by default 2
    randomSeed
        Random seed
    lr
        Learning Rate, by default 0.001
    po
        Learning rate decay coefficient. Real Learning Rage = lr / (1 + po *
        epoch), by default 0.005
    batchSize
        Batch size, by default 8
    dropout
        Dropout coefficient, by default 0.5.

        The coefficient of the dropout layer. The value should be between 0.0 and 1.0.
        Internally, it is used by Tensorflow as: `rate = 1.0 - dropout` when adding
        a dropout layer on top of the recurrent layers.
    graphFolder
        Folder path that contains external graph files. The path can be a local file path,
        a distributed file path (HDFS, DBFS), or a cloud storage (S3).

        When instantiating the Tensorflow model, uses this folder to search for the
        adequate Tensorflow graph. The search is done usaing the name of
        the `.pb` file, which should be `blstn_{ntags}_{embedding_dim}_{lstm_size}_{nchars}.pb`.
        The, the verifications on the obtained file are:
        - Embedding dimension should be exactly the same as the one used to train the model.
        - Number of tags should be greather than or equal to the number of tags in the
        training data.
        - Number of chars should be greather than or equal to the number of chars in
        the training data.

        The returned file will be the first one that satisfies all the conditions.

        If the name of the file is ill-formed, errors will occur during training.
    graphFile
        Path that contains the external graph file.

        When specified, the provided file will be used, and no graph search will happen.
        The path can be a local file path,
        a distributed file path (HDFS, DBFS), or a cloud storage (S3).
    pretrainedModelPath
        Path to an already trained FinanceNerModel, which is used as a starting point
        for training the new model. The path can be a local file path,
        a distributed file path (HDFS, DBFS), or a cloud storage (S3).
    configProtoBytes
        ConfigProto from tensorflow, serialized into byte array.
    useContrib
        whether to use contrib LSTM Cells. Not compatible with Windows. Might
        slightly improve accuracy. By default True.
    validationSplit
        Choose the proportion of training dataset to be validated against the
        model on each Epoch. The value should be between 0.0 and 1.0 and by
        default it is 0.0 and off.

        The validation dataset is randomly extracted from the training dataset
        before training starts. If the value is 0.0, then no validation will be
        performed (hold out data).
    evaluationLogExtended
        Whether logs for validation to be extended, by default False.
    testDataset
        Path to test dataset in parquet format. If set, the dataset will be used
        to calculate statistic on it during training.
    includeConfidence
        Whether to include confidence scores in annotation metadata, by default
        False.

        Setting this parameter to True will add the confidence score to
        the metadata of the NAMED_ENTITY annotation. In addition, if
        `includeAllConfidenceScores` is set to True, then the confidence scores
        of all the tags will be added to the metadata, otherwise only for the
        predicted tag (the one with maximum score).
    includeAllConfidenceScores
        Whether to include confidence scores for all tags in annotation metadata or just
        the score of the predicted tag, by default False.

        Needs the `includeConfidence` parameter to be set to True.
    enableOutputLogs
        Whether to use stdout in addition to Spark logs, by default False.
    outputLogsPath
        Folder path to save training logs. If no path is specified, the logs won't
        be stored in disk. The path can be a local file path,
        a distributed file path (HDFS, DBFS), or a cloud storage (S3).
    enableMemoryOptimizer
        Whether to optimize for large datasets or not. Enabling this option can
        slow down training.

        In practice, if set to True the training will iterate over the spark Data Frame
        and retrieve the batches from the Data Frame iterator. This can be slower
        than the default option as it has to collect the batches on evey bach for every
        epoch, but it can be useful if the dataset is too large to fit in memory.

        If the training data can fit to memory, then it is recommended to set this
        option to False (default value).
    tagsMapping
        A map specifying how old tags are mapped to new ones as a list of comma-separated
        entities, where the first entity is the old tag and the second entity is
        the new tag. For example, if the map is set to ["OLDTAG,NEWTAG", "B-PER,B-VIP",
        "I-PER, I-VIP"], then all occurrences of "OLDTAG" will be mapped to "NEWTAG",
        all occurrences of "B-PER" will be mapped to "B-VIP", and all occurrences of
        "I-PER" will be mapped to "I-VIP". It only works if setOverrideExistingTags
        is set to False.
    earlyStoppingPatience
        Number of epochs to wait before early stopping if no improvement, by default 5.

        Given the earlyStoppingCriterion, if the performance does not improve for the given
        number of epochs, then the training will stop. If the value is 0, then early stopping
        will occurs as soon as the criterion is met (no patience).
    earlyStoppingCriterion
        If set, this param specifies the criterion to stop training if performance is not improving.
        Default value is 0 which is means that early stopping is not used.

        The criterion is set to F1-score if the validationSplit is greater than 0.0
        (F1-socre on validation set) or testDataset is defined (F1-score on test set),
        otherwise it is set to model loss. The priority is as follows:
        - If testDataset is defined, then the criterion is set to F1-score on test set.
        - If validationSplit is greater than 0.0, then the criterion is set to
        F1-score on validation set.
        - Otherwise, the criterion is set to model loss.

        Note that while the F1-score ranges from 0.0 to 1.0, the loss ranges
        from 0.0 to infinity. So, depending on which case you are using,
        the value you use for the criterion can be very different. For example,
        if validationSplit is 0.1, then a criterion of 0.01 means that if the F1-score
        on the validation set difference from last epoch is greater than 0.01, then the
        training should stop.
        However, if there is not validation or test set defined, then a criterion of 2.0
        means that if the loss difference between the last epoch and the current one
        is less than 2.0, then training should stop.

        See also `earlyStoppingPatience`.
    logPrefix
        A prefix that will be appended to every log, default value is empty.
    useBestModel
        Whether to restore and use the model from the epoch that has achieved the
        best performance at the end of the training. By default False (keep the
        model from teh last trained epoch).

        The best model depends on the earlyStoppingCriterion, which can be F1-score
        on test/validation dataset or the value of loss.

    Examples
    --------
    >>> import sparknlp
    >>> from sparknlp.base import *
    >>> from sparknlp_jsl.common import *
    >>> from sparknlp.annotator import *
    >>> from sparknlp.training import *
    >>> import sparknlp_jsl
    >>> from sparknlp_jsl.base import *
    >>> from sparknlp_jsl.annotator import *
    >>> from pyspark.ml import Pipeline

    First extract the prerequisites for the NerDLApproach

    >>> documentAssembler = DocumentAssembler() \\
    ...     .setInputCol("text") \\
    ...     .setOutputCol("document")
    >>> sentence = SentenceDetector() \\
    ...     .setInputCols(["document"]) \\
    ...     .setOutputCol("sentence")
    >>> tokenizer = Tokenizer() \\
    ...     .setInputCols(["sentence"]) \\
    ...     .setOutputCol("token")
    >>> embeddings = BertEmbeddings.pretrained() \\
    ...     .setInputCols(["sentence", "token"]) \\
    ...     .setOutputCol("embeddings")

    Then the training can start

    >>> nerTagger = MedicalNerApproach() \\
    ...     .setInputCols(["sentence", "token", "embeddings"]) \\
    ...     .setLabelColumn("label") \\
    ...     .setOutputCol("ner") \\
    ...     .setMaxEpochs(1) \\
    ...     .setRandomSeed(0) \\
    ...     .setVerbose(0)
    >>> pipeline = Pipeline().setStages([
    ...     documentAssembler,
    ...     sentence,
    ...     tokenizer,
    ...     embeddings,
    ...     nerTagger
    ... ])
    >>> conll = CoNLL()
    >>> trainingData = conll.readDataset(spark, "src/test/resources/conll2003/eng.train")
    >>> pipelineModel = pipeline.fit(trainingData)
    """

    def _create_model(self, java_model:str):
        """Creates the model using the java model.

        Args:
            java_model (str): The name of the java model.
        """
        return FinanceNerModel(java_model=java_model)

    @keyword_only
    def __init__(self):

        super(A, self).__init__(classname="com.johnsnowlabs.finance.token_classification.ner.FinanceNerApproach")
        uc = False if sys.platform == 'win32' else True
        self._setDefault(
            minEpochs=0,
            maxEpochs=50,
            lr=float(0.001),
            po=float(0.005),
            batchSize=8,
            dropout=float(0.5),
            useContrib=uc,
            validationSplit=float(0.0),
            evaluationLogExtended=False,
            includeConfidence=False,
            includeAllConfidenceScores=False,
            enableOutputLogs=False,
            enableMemoryOptimizer=False,
            pretrainedModelPath="",
            overrideExistingTags=True,
            earlyStoppingCriterion=0.0,
            earlyStoppingPatience=0
        )


class _FinanceNerModelLoader(ExtendedJavaWrapper):
    def __init__(self, ner_model_path, path, jspark):
        super(_FinanceNerModelLoader, self).__init__(
            "com.johnsnowlabs.finance.token_classification.ner.FinanceNerModel.loadSavedModel",
            ner_model_path, path, jspark)


class FinanceNerModel(M):
    """This Named Entity recognition annotator is a generic NER model based on
    Neural Networks.

    Neural Network architecture is Char CNNs - BiLSTM - CRF that achieves
    state-of-the-art in most datasets.

    This is the instantiated model of the :class:`.NerDLApproach`. For training
    your own model, please see the documentation of that class.

    Pretrained models can be loaded with :meth:`.pretrained` of the companion
    object:

    >>> nerModel = MedicalNerDLModel.pretrained() \\
    ...     .setInputCols(["sentence", "token", "embeddings"]) \\
    ...     .setOutputCol("ner")


    The default model is ``"ner_dl"``, if no name is provided.

    For available pretrained models please see the `Models Hub
    <https://nlp.johnsnowlabs.com/models?task=Named+Entity+Recognition>`__.
    Additionally, pretrained pipelines are available for this module, see
    `Pipelines <https://nlp.johnsnowlabs.com/docs/en/pipelines>`__.

    Note that some pretrained models require specific types of embeddings,
    depending on which they were trained on. For example, the default model
    ``"ner_dl"`` requires the WordEmbeddings ``"glove_100d"``.

    For extended examples of usage, see the `Spark NLP Workshop
    <https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb>`__.

    ==================================== ======================
    Input Annotation types               Output Annotation type
    ==================================== ======================
    ``DOCUMENT, TOKEN, WORD_EMBEDDINGS`` ``NAMED_ENTITY``
    ==================================== ======================

    Parameters
    ----------
    batchSize
        Size of every batch, by default 8
    configProtoBytes
        ConfigProto from tensorflow, serialized into byte array.
    includeConfidence
        Whether to include confidence scores in annotation metadata, by default
        False
    includeAllConfidenceScores
        Whether to include all confidence scores in annotation metadata or just
        the score of the predicted tag, by default False
    inferenceBatchSize
        Number of sentences to process in a single batch during inference
    classes
        Tags used to trained this NerDLModel
    labelCasing:
        Setting all labels of the NER models upper/lower case. values upper|lower

    Examples
    --------
    >>> import sparknlp
    >>> from sparknlp.base import *
    >>> from sparknlp_jsl.common import *
    >>> from sparknlp.annotator import *
    >>> from sparknlp.training import *
    >>> import sparknlp_jsl
    >>> from sparknlp_jsl.base import *
    >>> from sparknlp_jsl.annotator import *
    >>> from pyspark.ml import Pipeline
    >>> documentAssembler = DocumentAssembler() \\
    ...     .setInputCol("text") \\
    ...     .setOutputCol("document")
    >>> sentence = SentenceDetector() \\
    ...     .setInputCols(["document"]) \\
    ...     .setOutputCol("sentence")
    >>> tokenizer = Tokenizer() \\
    ...     .setInputCols(["sentence"]) \\
    ...     .setOutputCol("token")
    >>> embeddings = WordEmbeddingsModel.pretrained() \\
    ...     .setInputCols(["sentence", "token"]) \\
    ...     .setOutputCol("bert")
    >>> nerTagger = MedicalNerDLModel.pretrained() \\
    ...     .setInputCols(["sentence", "token", "bert"]) \\
    ...     .setOutputCol("ner")
    >>> pipeline = Pipeline().setStages([
    ...     documentAssembler,
    ...     sentence,
    ...     tokenizer,
    ...     embeddings,
    ...     nerTagger
    ... ])
    >>> data = spark.createDataFrame([["U.N. official Ekeus heads for Baghdad."]]).toDF("text")
    >>> result = pipeline.fit(data).transform(data)
    """
    name = "FinanceNerModel"

    def __init__(self, classname="com.johnsnowlabs.finance.token_classification.ner.FinanceNerModel",
                 java_model=None):
        super(FinanceNerModel, self).__init__(
            classname=classname,
            java_model=java_model
        )
        self._setDefault(
            includeConfidence=False,
            includeAllConfidenceScores=False,
            batchSize=8,
            inferenceBatchSize=1
        )
    #
    # @staticmethod
    # def pretrained(name="ner_dl", lang="en", remote_loc=None):
    #     from sparknlp.pretrained import ResourceDownloader
    #     return ResourceDownloader.downloadModel(FinanceNerModel, name, lang, remote_loc)

    @staticmethod
    def loadSavedModel(ner_model_path, folder, spark_session):
        jModel = _FinanceNerModelLoader(ner_model_path, folder, spark_session._jsparkSession)._java_obj
        return FinanceNerModel(java_model=jModel)

    @staticmethod
    def pretrained(name="finner_sec_10k_summary", lang="en", remote_loc="finance/models"):
        """Download a pre-trained FinanceNerModel.

        Args:
            name (str): Name of the pre-trained model, by default "finner_sec_10k_summary"
            lang (str): Language of the pre-trained model, by default "en"
            remote_loc (str): Remote location of the pre-trained model. If None, use the
            open-source location. Other values are "clinical/models",
            "finance/models", or "legal/models".

        Returns:
            FinanceNerModel: A pre-trained FinanceNerModel.
        """
        from sparknlp_jsl.pretrained import InternalResourceDownloader
        return InternalResourceDownloader.downloadModel(FinanceNerModel, name, lang, remote_loc,
                                                        j_dwn='InternalsPythonResourceDownloader')
