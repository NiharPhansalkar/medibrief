from sparknlp_jsl.common import *

class BeamSearchParams(object):
    maxContextLength = Param(Params._dummy(), "maxContextLength", "Maximum length of the context text",
                            typeConverter=TypeConverters.toInt)

    doSample = Param(Params._dummy(), "doSample", "Whether or not to use sampling; use greedy decoding otherwise",
                     typeConverter=TypeConverters.toBoolean)

    topK = Param(Params._dummy(), "topK",
                 "The number of highest probability vocabulary tokens to consider",
                 typeConverter=TypeConverters.toInt)

    ignoreTokenIds = Param(Params._dummy(), "ignoreTokenIds",
                           "A list of token ids which are ignored in the decoder's output",
                           typeConverter=TypeConverters.toListInt)

    maxNewTokens = Param(Params._dummy(), "maxNewTokens",
                         "Maximum number of new tokens to be generated",
                         typeConverter=TypeConverters.toInt)

    noRepeatNgramSize = Param(Params._dummy(), "noRepeatNgramSize",
                              "If set to int > 0, all ngrams of that size can only occur once",
                              typeConverter=TypeConverters.toInt)

    def setIgnoreTokenIds(self, value):
        """A list of token ids which are ignored in the decoder's output.

        Parameters
        ----------
        value : List[int]
            The words to be filtered out
        """
        return self._set(ignoreTokenIds=value)

    def setMaxContextLength(self, value):
        """Sets maximum length of output text.

        Parameters
        ----------
        value : int
            Maximum length of output text
        """
        return self._set(maxContextLength=value)

    def setDoSample(self, value):
        """Sets whether or not to use sampling, use greedy decoding otherwise.

        Parameters
        ----------
        value : bool
            Whether or not to use sampling; use greedy decoding otherwise
        """
        return self._set(doSample=value)

    def setTopK(self, value):
        """Sets the number of highest probability vocabulary tokens to consider

        Parameters
        ----------
        value : int
            Number of highest probability vocabulary tokens to consider
        """
        return self._set(topK=value)

    def setMaxNewTokens(self, value):
        """Sets the maximum number of new tokens to be generated

        Parameters
        ----------
        value : int
            the maximum number of new tokens to be generated
        """
        return self._set(maxNewTokens=value)

    def setNoRepeatNgramSize(self, value):
        """Sets size of n-grams that can only occur once.

        If set to int > 0, all ngrams of that size can only occur once.

        Parameters
        ----------
        value : int
            N-gram size can only occur once
        """
        return self._set(noRepeatNgramSize=value)

    def setRandomSeed(self, seed):
        """Sets random seed.

        Parameters
        ----------
        seed : int
            Random seed
        """
        self._call_java("setRandomSeed", seed)

        return self

    @keyword_only
    def __init__(self):
        super(BeamSearchParams, self).__init__()
        self._setDefault(
            maxContextLength=1024,
            doSample=False,
            topK=1,
            ignoreTokenIds=[],
            maxNewTokens=30,
            noRepeatNgramSize=0
        )