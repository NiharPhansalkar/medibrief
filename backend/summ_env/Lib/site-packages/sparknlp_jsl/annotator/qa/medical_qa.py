from sparknlp_jsl.common import *
from sparknlp_jsl.annotator.qa.beam_search_params import BeamSearchParams

class MedicalQuestionAnswering(AnnotatorModelInternal, BeamSearchParams, HasBatchedAnnotate, HasCaseSensitiveProperties, HasEngine):
    """
    MedicalQuestionAnswering is a GPT based model for answering questions given a context. Unlike span based models, it
    generates the answers to the questions, rather than selecting phrases from the given context. The model is
    capable of answering various types of questions, including yes-no or full text ones.

    ==========================================  ======================
    Input Annotation types                      Output Annotation type
    ==========================================  ======================
    ``DOCUMENT, DOCUMENT``                      ``CHUNK``
    ==========================================  ======================

    Parameters
    ----------
    questionType
        Question type, e.g. "short" or "long". The question types depend on the model, check the model card to get a
        description of question types
    maxNewTokens
        Maximum number of of new tokens to generate, by default 30
    maxContextLength
        Maximum length of context text
    configProtoBytes
        ConfigProto from tensorflow, serialized into byte array.
    doSample
        Whether or not to use sampling; use greedy decoding otherwise, by default False
    topK
        The number of highest probability vocabulary tokens to consider, by default 1
    noRepeatNgramSize
        The number of tokens that can't be repeated in the same order. Useful for preventing loops.
        The default is 0.
    ignoreTokenIds
        A list of token ids which are ignored in the decoder's output, by
        default []
    customPrompt
        Custom prompt template. Available variables {QUESTION} and {CONTEXT}
    useCache
        Cache internal state of the model to improve performance, by default is True.
        Disable it only if you are processing short texts or you want to spare memory usage.
    custom
    Examples
    --------
    >>> context = "This study aims to evaluate local failure patterns in node negative breast cancer patients ..."
    >>> question = "Should chest wall irradiation be included after mastectomy and negative node breast cancer?"
    >>> data = spark.createDataFrame([[question, context]]).toDF("question", "context")
    >>> document_assembler = MultiDocumentAssembler()\
    ...   .setInputCols("question", "context")\
    ...   .setOutputCols("document_question", "document_context")
    ...
    >>> med_qa = sparknlp_jsl.annotators.qa.MedicalQuestionAnswering\
    ...   .load("/models/sparknlp/medical_qa_biogpt")\
    ...   .setQuestionType("short")\
    ...   .setInputCols(["document_question", "document_context"])\
    ...   .setMaxNewTokens(100)\
    ...   .setOutputCol("answer")\
    >>> pipeline = Pipeline(stages=[document_assembler, med_qa])
    >>> pipeline\
    ...   .fit(data)\
    ...   .select("answer.result")\
    ...   .show(truncate=False)
    +-------+
    |result |
    +-------+
    |[yes]  |
    +-------+
    """

    name = "MedicalQuestionAnswering"

    inputAnnotatorTypes = [AnnotatorType.DOCUMENT, AnnotatorType.DOCUMENT]

    outputAnnotatorType = AnnotatorType.DOCUMENT

    configProtoBytes = Param(Params._dummy(),
                             "configProtoBytes",
                             "ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()",
                             TypeConverters.toListInt)

    questionType = Param(Params._dummy(), "questionType",
                         "Question type",
                         typeConverter=TypeConverters.toString)

    customPrompt = Param(Params._dummy(), "customPrompt",
                         "Custom prompt template",
                         typeConverter=TypeConverters.toString)

    useCache = Param(Params._dummy(), "useCache", "Cache internal state of the model to improve performance",
                  typeConverter=TypeConverters.toBoolean)

    modelType = Param(Params._dummy(), "modelType",
                      "Model type",
                      typeConverter=TypeConverters.toString)

    mlFrameworkType = Param(Params._dummy(), "mlFrameworkType",
                            "ML framework type",
                            typeConverter=TypeConverters.toString)
    def setConfigProtoBytes(self, b):
        """Sets configProto from tensorflow, serialized into byte array.

        Parameters
        ----------
        b : List[int]
            ConfigProto from tensorflow, serialized into byte array
        """
        return self._set(configProtoBytes=b)

    def setUseCache(self, value):
        """Cache internal state of the model to improve performance

        Parameters
        ----------
        value : bool
            Whether or not to use cache
        """
        return self._set(useCache=value)

    def setQuestionType(self, value):
        """Sets the question type

        Parameters
        ----------
        value : str
            question type
        """
        return self._set(questionType=value)

    def setCustomPrompt(self, value):
        """Sets the custom prompt template. Available variables {QUESTION} and {CONTEXT}

        Parameters
        ----------
        value : str
            prompt template
        """
        return self._set(customPrompt=value)

    def setQuestionAnswerTerminals(self, questionTerminals):
        """Set terminal symbols for each question type

        Parameters
        ----------
        questionTerminals : dict[str, list[int]]

        """

        self._call_java("setQuestionAnswerTerminals", questionTerminals)
        return self

    def setQuestionPrompts(self, questionPrompts):
        """Set question prompts for each question type

        Parameters
        ----------
        questionPrompts : dict[str, list[str]]

        """

        self._call_java("setQuestionPrompts", questionPrompts)
        return self

    def setQuestionSkipLastToken(self, questionSkipLastToken):
        """Set flag indicating whether to include the last symbol in the answer to each question type

        Parameters
        ----------
        questionSkipLastToken : dict[str, list[bool]]

        """

        self._call_java("setQuestionSkipLastToken", questionSkipLastToken)
        return self

    def getQuestionTypes(self):
        """
        Returns a list of question types
        """
        return self._call_java("getQuestionTypes")

    @keyword_only
    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.qa.MedicalQuestionAnswering", java_model=None):
        super(MedicalQuestionAnswering, self).__init__(
            classname=classname,
            java_model=java_model
        )

        self._setDefault(
            batchSize=4,
            caseSensitive=True,
            questionType="",
            useCache=True,
            customPrompt="",
            modelType="biogpt",
            mlFrameworkType="tensorflow"
        )

    @staticmethod
    def loadSavedModel(folder, spark_session):
        """Loads a locally saved model.

        Parameters
        ----------
        folder : str
            Folder of the saved model
        spark_session : pyspark.sql.SparkSession
            The current SparkSession

        Returns
        -------
        MedicalQuestionAnswering
            The restored model
        """
        from sparknlp_jsl.internal import _MedicalQuestionAnsweringLoader
        jModel = _MedicalQuestionAnsweringLoader(folder, spark_session._jsparkSession)._java_obj
        return MedicalQuestionAnswering(java_model=jModel)

    @staticmethod
    def loadMedicalTextGenerator(med_text_generator_path, spark_session):
        """Turns a MedicalTextGenerator into a MedicalQuestionAnswering model

        Parameters
        ----------
        med_text_generator_path : str
            Path to the MedicalTextGenerator model
        spark_session : pyspark.sql.SparkSession
            The current SparkSession

        Returns
        -------
        MedicalQuestionAnswering
            A MedicalQuestionAnswering model
        """
        from sparknlp_jsl.internal import _MedicalQuestionAnsweringConverter
        jModel = _MedicalQuestionAnsweringConverter(med_text_generator_path, spark_session._jsparkSession)._java_obj
        return MedicalQuestionAnswering(java_model=jModel)

    @staticmethod
    def pretrained(name="flan_t5_base_jsl_qa", lang="en", remote_loc="clinical/models"):
        """Downloads and loads a pretrained model.

        Parameters
        ----------
        name : str, optional
            Name of the pretrained model, by default "medical_qa_biogpt"
        lang : str, optional
            Language of the pretrained model, by default "en"
        remote_loc : str, optional
            Optional remote address of the resource, by default None. Will use
            Spark NLPs repositories otherwise.

        Returns
        -------
        MedicalQuestionAnswering
            The restored model
        """
        from sparknlp_jsl.pretrained import InternalResourceDownloader
        return InternalResourceDownloader.downloadModel(MedicalQuestionAnswering, name, lang, remote_loc,
                                                        j_dwn='InternalsPythonResourceDownloader')