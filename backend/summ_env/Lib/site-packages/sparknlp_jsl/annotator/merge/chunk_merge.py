from sparknlp_jsl.annotator.filtering_params import FilteringParams
from sparknlp_jsl.common import *

class MergeCommonParams:

    mergeOverlapping = Param(Params._dummy(), "mergeOverlapping", "whether to merge overlapping chunks. Defaults to true", typeConverter = TypeConverters.toBoolean)

    def setMergeOverlapping(self, value):
        """Sets whether to merge overlapping chunks. Defaults to true

        Parameters
        ----------
        value : boolean
            whether to merge overlapping chunks. Defaults to true
        """
        return self._set(mergeOverlapping = value)


class MergePrioritizationParams:

    orderingFeatures = Param(Params._dummy(), "orderingFeatures", "Array of strings specifying the ordering features to use for overlapping entities. Possible values are ChunkBegin, ChunkLength, ChunkPrecedence, ChunkConfidence.", typeConverter = TypeConverters.toListString)

    def setOrderingFeatures(self, value):
        """Sets Array of strings specifying the ordering features to use for overlapping entities. Possible values are ChunkBegin, ChunkLength, ChunkPrecedence, ChunkConfidence.

        Parameters
        ----------
        value : List[str]
            Array of strings specifying the ordering features to use for overlapping entities. Possible values are ChunkBegin, ChunkLength, ChunkPrecedence, ChunkConfidence.
        """
        return self._set(orderingFeatures = value)

    selectionStrategy = Param(Params._dummy(), "selectionStrategy", "Whether to select annotations sequentially based on annotation order `Sequential` or using any other available strategy; currently only `Sequential` and `DiverseLonger` are available.", typeConverter = TypeConverters.toString)

    def setSelectionStrategy(self, value):
        """Sets Whether to select annotations sequentially based on annotation order `Sequential` or using any other available strategy; currently only `Sequential` and `DiverseLonger` are available.

        Parameters
        ----------
        value : string
            Whether to select annotations sequentially based on annotation order `Sequential` or using any other available strategy; currently only `Sequential` and `DiverseLonger` are available.
        """
        return self._set(selectionStrategy = value)

    defaultConfidence = Param(Params._dummy(), "defaultConfidence", "When ChunkConfidence ordering feature is included and a given annotation does not have any confidence the value of this param will be used.", typeConverter = TypeConverters.toFloat)

    def setDefaultConfidence(self, value):
        """Sets When ChunkConfidence ordering feature is included and a given annotation does not have any confidence the value of this param will be used.

        Parameters
        ----------
        value : float
            When ChunkConfidence ordering feature is included and a given annotation does not have any confidence the value of this param will be used.
        """
        return self._set(defaultConfidence = value)

    chunkPrecedence = Param(Params._dummy(), "chunkPrecedence", "When ChunkPrecedence ordering feature is used this param contains the comma separated fields in metadata that drive prioritization of overlapping annotations. When used by itself (empty chunkPrecedenceValuePrioritization) annotations will be prioritized based on number of metadata fields present. When used together with chunkPrecedenceValuePrioritization param it will prioritize based on the order of its values.", typeConverter = TypeConverters.toString)

    def setChunkPrecedence(self, value):
        """Sets When ChunkPrecedence ordering feature is used this param contains the comma separated fields in metadata that drive prioritization of overlapping annotations. When used by itself (empty chunkPrecedenceValuePrioritization) annotations will be prioritized based on number of metadata fields present. When used together with chunkPrecedenceValuePrioritization param it will prioritize based on the order of its values.

        Parameters
        ----------
        value : string
            When ChunkPrecedence ordering feature is used this param contains the comma separated fields in metadata that drive prioritization of overlapping annotations. When used by itself (empty chunkPrecedenceValuePrioritization) annotations will be prioritized based on number of metadata fields present. When used together with chunkPrecedenceValuePrioritization param it will prioritize based on the order of its values.
        """
        return self._set(chunkPrecedence = value)

    chunkPrecedenceValuePrioritization = Param(Params._dummy(), "chunkPrecedenceValuePrioritization", "When ChunkPrecedence ordering feature is used this param contains an Array of comma separated values representing the desired order of prioritization for the VALUES in the metadata fields included from chunkPrecedence.", typeConverter = TypeConverters.toListString)

    def setChunkPrecedenceValuePrioritization(self, value):
        """Sets When ChunkPrecedence ordering feature is used this param contains an Array of comma separated values representing the desired order of prioritization for the VALUES in the metadata fields included from chunkPrecedence.

        Parameters
        ----------
        value : List[str]
            When ChunkPrecedence ordering feature is used this param contains an Array of comma separated values representing the desired order of prioritization for the VALUES in the metadata fields included from chunkPrecedence.
        """
        return self._set(chunkPrecedenceValuePrioritization = value)


class MergeResourceParams:

    falsePositivesResource = Param(Params._dummy(), "falsePositivesResource", "Path to csv with false positive text, entity pairs to remove", typeConverter = TypeConverters.identity)

    def setFalsePositivesResource(self, path, read_as=ReadAs.TEXT, options=None):
        """Sets file with false positive pairs

        Parameters
        ----------
        path : str
            Path to the external resource
        read_as : str, optional
            How to read the resource, by default ReadAs.TEXT
        options : dict, optional
            Options for reading the resource, by default {"format": "text"}

        """
        if options is None:
            options = {"delimiter": "\t"}
        return self._set(falsePositivesResource=ExternalResource(path, read_as, options))


    replaceDictResource = Param(Params._dummy(),
                                "replaceDictResource",
                                "replace dictionary pairs",
                                typeConverter=TypeConverters.identity)

    def setReplaceDictResource(self, path, read_as=ReadAs.TEXT, options={"delimiter": ","}):
        """Sets replace dictionary pairs

        Parameters
        ----------
        path : str
            Path to the external resource

        read_as : str, optional
            How to read the resource, by default ReadAs.TEXT
        options : dict, optional
            Options for reading the resource, by default {"format": "text"}
        """

        return self._set(replaceDictResource=ExternalResource(path, read_as, options))

    entitiesConfidenceResource = Param(Params._dummy(),
                                       "entitiesConfidenceResource",
                                       "Path to csv with  entity pairs to remove based on the confidence level",
                                       typeConverter=TypeConverters.identity)

    def setEntitiesConfidenceResource(self, path, read_as=ReadAs.TEXT, options=None):
        """Sets the entitiesConfidenceResource parameter.
        
        Parameters
        ----------
        path : str
              Path to csv with  entity pairs to remove based on the confidence level
        read_as : str
                Read file as 'TEXT', 'SPARK', or 'BINARY'.
        options : dict
                Options for reading the file.
        """
        
        if options is None:
            options = {"delimiter": ","}
        return self._set(entitiesConfidenceResource=ExternalResource(path, read_as, options))




class ChunkMergeApproach(AnnotatorApproachInternal, MergeCommonParams, MergePrioritizationParams, MergeResourceParams, FilteringParams):
    """
    Merges two chunk columns coming from two annotators(NER, ContextualParser or any other annotator producing
    chunks). The merger of the two chunk columns is made by selecting one chunk from one of the columns according
    to certain criteria.
    The decision on which chunk to select is made according to the chunk indices in the source document.
    (chunks with longer lengths and highest information will be kept from each source)
    Labels can be changed by setReplaceDictResource.

    =========================== ======================
    Input Annotation types      Output Annotation type
    =========================== ======================
    ``CHUNK,CHUNK``               ``CHUNK``
    =========================== ======================

    Parameters
    ----------
    mergeOverlapping
        whether to merge overlapping matched chunks. Defaults false
    falsePositivesResource
        file with false positive pairs
    replaceDictResource
        replace dictionary pairs
    chunkPrecedence
        Select what is the precedence when two chunks have the same start and end indices. Possible values are [entity|identifier|field]
    blackList
        If defined, list of entities to ignore. The rest will be proccessed.
    whiteList
        If defined, list of entities to process. The rest will be ignored. Do not include IOB prefix on labels.
    caseSensitive: bool
        Determines whether the definitions of the white listed and black listed entities are case sensitive. Default: True.
        If the filterValue is 'entity', 'caseSensitive' is always False.
    regex: str
       If defined, list of regex to process the chunks (Default: []).
    criteria: str
           Tag representing what is the criteria to filter the chunks. Possibles values are:
           - isIn: Filter by the chunk
           - regex: Filter using a regex
    filterValue: str
        If equal to "entity", use the ner label to filter. If set to "result",
        use the `result` attribute of the annotation to filter.
    entitiesConfidenceResource: str
        Path to a CSV file containing the entity pairs to remove chunks based on
        the confidance level. The CSV file should have two columns: entity and
        confidenceThreshold. The entity column should contain the entity name
        and the confidenceThreshold column should contain the confidence
        threshold to filter the chunks.    
    
    Examples
    --------

    >>> import sparknlp
    >>> from sparknlp.base import *
    >>> from sparknlp_jsl.common import *
    >>> from sparknlp.annotator import *
    >>> from sparknlp.training import *
    >>> import sparknlp_jsl
    >>> from sparknlp_jsl.base import *
    >>> from sparknlp_jsl.annotator import *
    >>> from pyspark.ml import Pipeline
    Define a pipeline with 2 different NER models with a ChunkMergeApproach at the end
    >>> data = spark.createDataFrame([["A 63-year-old man presents to the hospital ..."]]).toDF("text")
    >>> pipeline = Pipeline(stages=[
    ...  DocumentAssembler().setInputCol("text").setOutputCol("document"),
    ...  SentenceDetector().setInputCols(["document"]).setOutputCol("sentence"),
    ...  Tokenizer().setInputCols(["sentence"]).setOutputCol("token"),
    ...   WordEmbeddingsModel.pretrained("embeddings_clinical", "en", "clinical/models").setOutputCol("embs"),
    ...   MedicalNerModel.pretrained("ner_jsl", "en", "clinical/models") \\
    ...     .setInputCols(["sentence", "token", "embs"]).setOutputCol("jsl_ner"),
    ...  NerConverter().setInputCols(["sentence", "token", "jsl_ner"]).setOutputCol("jsl_ner_chunk"),
    ...   MedicalNerModel.pretrained("ner_bionlp", "en", "clinical/models") \\
    ...     .setInputCols(["sentence", "token", "embs"]).setOutputCol("bionlp_ner"),
    ...  NerConverter().setInputCols(["sentence", "token", "bionlp_ner"]) \\
    ...     .setOutputCol("bionlp_ner_chunk"),
    ...  ChunkMergeApproach().setInputCols(["jsl_ner_chunk", "bionlp_ner_chunk"]).setOutputCol("merged_chunk")
    >>> ])
    >>> result = pipeline.fit(data).transform(data).cache()
    >>> result.selectExpr("explode(merged_chunk) as a") \\
    ...   .selectExpr("a.begin","a.end","a.result as chunk","a.metadata.entity as entity") \\
    ...   .show(5, False)
    +-----+---+-----------+---------+
    |begin|end|chunk      |entity   |
    +-----+---+-----------+---------+
    |5    |15 |63-year-old|Age      |
    |17   |19 |man        |Gender   |
    |64   |72 |recurrent  |Modifier |
    |98   |107|cellulitis |Diagnosis|
    |110  |119|pneumonias |Diagnosis|
    +-----+---+-----------+---------+
    """
    inputAnnotatorTypes = [AnnotatorType.CHUNK, AnnotatorType.CHUNK]
    outputAnnotatorType = AnnotatorType.CHUNK

    name = "ChunkMergeApproach"

    @keyword_only
    def __init__(self):
        super(ChunkMergeApproach, self).__init__(classname="com.johnsnowlabs.nlp.annotators.merge.ChunkMergeApproach")

    def _create_model(self, java_model:str):
        """Creates the model using the java model.

        Args:
            java_model (str): The name of the java model.
        """
        return ChunkMergeModel(java_model=java_model)

    def setInputCols(self, *value):
        """Sets column names of input annotations.
        Parameters
        ----------
        *value : str
            Input columns for the annotator
        """
        # Overloaded setInputCols to evade validation until updated on Spark-NLP side
        if type(value[0]) == str or type(value[0]) == list:
            # self.inputColsValidation(value)
            if len(value) == 1 and type(value[0]) == list:
                return self._set(inputCols=value[0])
            else:
                return self._set(inputCols=list(value))
        else:
            raise TypeError("InputCols datatype not supported. It must be either str or list")


class ChunkMergeModel(AnnotatorModelInternal, MergeCommonParams, MergePrioritizationParams, FilteringParams):
    """
    The model produced by ChunkMergeApproach.

    =========================== ======================
    Input Annotation types      Output Annotation type
    =========================== ======================
    ``CHUNK,CHUNK``             ``CHUNK``
    =========================== ======================

    Parameters
    ----------

    mergeOverlapping
        whether to merge overlapping matched chunks. Defaults false
    chunkPrecedence
        Select what is the precedence when two chunks have the same start and end indices. Possible values are [entity|identifier|field]
    blackList
        If defined, list of entities to ignore. The rest will be proccessed.
    whiteList
        If defined, list of entities to process. The rest will be ignored. Do not include IOB prefix on labels.
    caseSensitive: bool
        Determines whether the definitions of the white listed and black listed entities are case sensitive. Default: True.
        If the filterValue is 'entity', 'caseSensitive' is always False.
    regex: str
       If defined, list of regex to process the chunks (Default: []).
    criteria: str
           Tag representing what is the criteria to filter the chunks. Possibles values are:
           - isIn: Filter by the chunk
           - regex: Filter using a regex
    filterValue: str
        If equal to "entity", use the ner label to filter. If set to "result",
        use the `result` attribute of the annotation to filter.
    """
    name = "ChunkMergeModel"

    inputAnnotatorTypes = [AnnotatorType.CHUNK, AnnotatorType.CHUNK]
    outputAnnotatorType = AnnotatorType.CHUNK

    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.merge.ChunkMergeModel",
                 java_model=None):
        super(ChunkMergeModel, self).__init__(
            classname=classname,
            java_model=java_model
        )

    @staticmethod
    def pretrained(name, lang="en", remote_loc=None):
        """Downloads and loads a pretrained model.

        Parameters
        ----------
        name : str, optional
            Name of the pretrained model.
        lang : str, optional
            Language of the pretrained model, by default "en"
        remote_loc : str, optional
            Optional remote address of the resource, by default None. Will use
            Spark NLPs repositories otherwise.

        Returns
        -------
        ChunkMergeModel
            The restored model
        """
        from sparknlp_jsl.pretrained import InternalResourceDownloader
        return InternalResourceDownloader.downloadModel(ChunkMergeModel, name, lang, remote_loc,
                                                        j_dwn='InternalsPythonResourceDownloader')

    def setInputCols(self, *value):
        """Sets column names of input annotations.
        Parameters
        ----------
        *value : str
            Input columns for the annotator
        """
        # Overloaded setInputCols to evade validation until updated on Spark-NLP side
        if type(value[0]) == str or type(value[0]) == list:
            # self.inputColsValidation(value)
            if len(value) == 1 and type(value[0]) == list:
                return self._set(inputCols=value[0])
            else:
                return self._set(inputCols=list(value))
        else:
            raise TypeError("InputCols datatype not supported. It must be either str or list")
