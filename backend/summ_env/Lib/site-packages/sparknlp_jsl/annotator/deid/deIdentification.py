from sparknlp_jsl.annotator.deid.deidentication_params import DeIdentificationParams
from sparknlp_jsl.common import *



class DeIdentification(AnnotatorApproachInternal, DeIdentificationParams):
    """Contains all the methods for training a DeIdentificationModel model.
    This module can obfuscate or mask the entities that contains personal information. These can be set with a file of
    regex patterns with setRegexPatternsDictionary, where each line is a mapping of entity to regex

    .. note::
        If the mode is set to 'obfuscate', the DeIdentificationModel utilizes **java.security.SecureRandom** for generating fake data.
        You can select a generation algorithm by configuring the system environment variable **SPARK_NLP_JSL_SEED_ALGORITHM**.
        The chosen algorithm may impact the generation of fake data, performance, and potential blocking issues.
        For information about standard RNG algorithm names, refer to the SecureRandom section in the `Number Generation Algorithms
        <https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html#SecureRandom>`_.
        The default algorithm is 'SHA1PRNG'.

    ========================================= ======================
    Input Annotation types                    Output Annotation type
    ========================================= ======================
    ``DOCUMENT, CHUNK, TOKEN``                ``DOCUMENT``
    ========================================= ======================

    The configuration params for that module are in class DeidentificationParams.

    Parameters
    ----------
    regexPatternsDictionary
        Dictionary with regular expression patterns that match some protected entity
    obfuscateRefFile
        File with the terms to be used for Obfuscation
    refFileFormat
        Format of the reference file
    refSep
        Sep character in refFile
    selectiveObfuscationModesPath
        Dictionary path where is the json that contains the selective obfuscation modes
            'obfuscate': Replace the values with random values.
            'mask_same_length_chars': Replace the name with the asterisks with same length minus two plus brackets on both end.
            'mask_entity_labels': Replace the values with the entity value.
            'mask_fixed_length_chars': Replace the name with the asterisks with fixed length. You can also invoke "setFixedMaskLength()"
            'skip': Skip the values (intact)
        The entities which have not been given in dictionary will deidentify according to :param:`mode`
    entityCasingModesPath
        Dictionary path where is the json that contains the entity casing modes.
            'lowercase': Converts all characters to lower case using the rules of the default locale.
            'uppercase': Converts all characters to upper case using the rules of the default locale.
            'capitalize': Converts the first character to upper case and converts others to lower case.
            'titlecase': Converts the first character in every token to upper case and converts others to lower case.


    Examples
    --------

    >>> import sparknlp
    >>> from sparknlp.base import *
    >>> from sparknlp_jsl.common import *
    >>> from sparknlp.annotator import *
    >>> from sparknlp.training import *
    >>> import sparknlp_jsl
    >>> from sparknlp_jsl.base import *
    >>> from sparknlp_jsl.annotator import *
    >>> from pyspark.ml import Pipeline
    >>> documentAssembler = DocumentAssembler() \\
    ...     .setInputCol("text") \\
    ...     .setOutputCol("document")
    ...
    >>>  sentenceDetector = SentenceDetector() \\
    ...     .setInputCols(["document"]) \\
    ...     .setOutputCol("sentence") \\
    ...     .setUseAbbreviations(True)
    ...
    >>> tokenizer = Tokenizer() \\
    ...     .setInputCols(["sentence"]) \\
    ...     .setOutputCol("token")
    ...
    >>> embeddings = WordEmbeddingsModel \\
    ...     .pretrained("embeddings_clinical", "en", "clinical/models") \\
    ...     .setInputCols(["sentence", "token"]) \\
    ...     .setOutputCol("embeddings")
    ...
     Ner entities
    >>> clinical_sensitive_entities = MedicalNerModel \\
    ...     .pretrained("ner_deid_enriched", "en", "clinical/models") \\
    ...     .setInputCols(["sentence", "token", "embeddings"]).setOutputCol("ner")
    ...
    >>> nerConverter = NerConverter() \\
    ...     .setInputCols(["sentence", "token", "ner"]) \\
    ...     .setOutputCol("ner_con")
     Deidentification
    >>> deIdentification = DeIdentification() \\
    ...     .setInputCols(["ner_chunk", "token", "sentence"]) \\
    ...     .setOutputCol("dei") \\
    ...     # file with custom regex pattern for custom entities\\
    ...     .setRegexPatternsDictionary("path/to/dic_regex_patterns_main_categories.txt") \\
    ...     # file with custom obfuscator names for the entities\\
    ...     .setObfuscateRefFile("path/to/obfuscate_fixed_entities.txt") \\
    ...     .setRefFileFormat("csv") \\
    ...     .setRefSep("#") \\
    ...     .setMode("obfuscate") \\
    ...     .setDateFormats(Array("MM/dd/yy","yyyy-MM-dd")) \\
    ...     .setObfuscateDate(True) \\
    ...     .setDateTag("DATE") \\
    ...     .setDays(5) \\
    ...     .setObfuscateRefSource("file")
    Pipeline
    >>> data = spark.createDataFrame([
    ...     ["# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09."]
    ...     ]).toDF("text")
    >>> pipeline = Pipeline(stages=[
    ...     documentAssembler,
    ...     sentenceDetector,
    ...     tokenizer,
    ...     embeddings,
    ...     clinical_sensitive_entities,
    ...     nerConverter,
    ...     deIdentification
    ... ])
    >>> result = pipeline.fit(data).transform(data)
    >>> result.select("dei.result").show(truncate = False)
     +--------------------------------------------------------------------------------------------------+
     |result                                                                                            |
     +--------------------------------------------------------------------------------------------------+
     |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , <AGE> years-old , Record date : 2079-11-14.]|
     +--------------------------------------------------------------------------------------------------+

    """
    inputAnnotatorTypes = [AnnotatorType.DOCUMENT, AnnotatorType.CHUNK, AnnotatorType.TOKEN]
    outputAnnotatorType = AnnotatorType.DOCUMENT

    name = "DeIdentification"

    regexPatternsDictionary = Param(Params._dummy(),
                                    "regexPatternsDictionary",
                                    "dictionary with regular expression patterns that match some protected entity",
                                    typeConverter=TypeConverters.identity)
    regexPatternsDictionaryAsJsonString = Param(Params._dummy(),
                                                "regexPatternsDictionaryAsJsonString",
                                                "dictionary with regular expression patterns given as JSON that match some protected entity",
                                                typeConverter=TypeConverters.toString)
    combineRegexPatterns = Param(Params._dummy(), "combineRegexPatterns", "Use loaded regex file and default regex file together.",
                                 TypeConverters.toBoolean)
    obfuscateRefFile = Param(Params._dummy(), "obfuscateRefFile",
                             "File with the terms to be used for Obfuscation",
                             TypeConverters.toString)
    refFileFormat = Param(Params._dummy(), "refFileFormat",
                          "Format of the reference file",
                          TypeConverters.toString)
    refSep = Param(Params._dummy(), "refSep",
                   "Sep character in refFile",
                   TypeConverters.toString)
    selectiveObfuscationModesPath = Param(Params._dummy(),"selectiveObfuscationModesPath",
                                          "Dictionary path where is the json that contains the selective obfuscation modes",
                                          TypeConverters.toString)
    entityCasingModesPath = Param(Params._dummy(),"entityCasingModesPath",
                                  "Dictionary path where is the json that contains the entity casing modes",
                                  TypeConverters.toString)

    @keyword_only
    def __init__(self):
        super(DeIdentification, self).__init__(classname="com.johnsnowlabs.nlp.annotators.deid.DeIdentification")

    def setRegexPatternsDictionary(self, path, read_as=ReadAs.TEXT, options=None):
        """Sets dictionary with regular expression patterns that match some protected entity

        Parameters
        ----------
        path : str
            Path where the dictionary is
        read_as: ReadAs
            Format of the file
        options: dict
            Dictionary with the options to read the file.

        """
        if options is None:
            options = {"delimiter": " "}
        return self._set(regexPatternsDictionary=ExternalResource(path, read_as, options))

    def setRegexPatternsDictionaryAsJsonString(self, json):
        """Sets dictionary with regular expression patterns as JSON that match some protected entity
        Parameters
        ----------
        json : str
            regex(s) as JSON format.
        """
        return self._set(regexPatternsDictionaryAsJsonString=json)

    def setCombineRegexPatterns(self, value):
        """Sets whether you want to use regex both loaded regex file and default regex file. If the value is 'True',
        both file will be used,
        if the value is false either loaded file or default file will be used
        Default: False.
        Parameters
        ----------
        s : bool
            Whether to combine regex files or not. If the value is 'True',
        both file will be used. Default: False.
        """
        return self._set(combineRegexPatterns=value)

    def setObfuscateRefFile(self, f):
        """Set file with the terms to be used for Obfuscation

        Parameters
        ----------
        f : str
            File with the terms to be used for Obfuscation
        """
        return self._set(obfuscateRefFile=f)

    def setRefFileFormat(self, f):
        """Sets format of the reference file

        Parameters
        ----------
        f : str
            Format of the reference file
        """
        return self._set(refFileFormat=f)

    def setRefSep(self, c):
        """Sets separator character in refFile

        Parameters
        ----------
        c : str
            Separator character in refFile
        """
        return self._set(refSep=c)

    def setSelectiveObfuscationModes(self, path):
        """Sets a Json path which has a dictionary of modes to enable multi-mode deIdentification.
            'obfuscate': Replace the values with random values.
            'mask_same_length_chars': Replace the name with the asterisks with same length minus two plus brackets on both end.
            'mask_entity_labels': Replace the values with the entity value.
            'mask_fixed_length_chars': Replace the name with the asterisks with fixed length. You can also invoke "setFixedMaskLength()"
            'skip': Skip the values (intact)
        The entities which have not been given in dictionary will deidentify according to setMode()
        Parameters
        ----------
        path : str
            Dictionary path where is the json that contains the selective obfuscation modes
        """
        return self._set(selectiveObfuscationModesPath=path)

    def setEntityCasingModes(self, path):
        """Sets a Json path which has a dictionary of modes to select casing modes.
            'lowercase': Converts all characters to lower case using the rules of the default locale.
            'uppercase': Converts all characters to upper case using the rules of the default locale.
            'capitalize': Converts the first character to upper case and converts others to lower case.
            'titlecase': Converts the first character in every token to upper case and converts others to lower case.
        Parameters
        ----------
        path : str
            Dictionary path where is the json that contains the entity casing modes.
        """
        return self._set(entityCasingModesPath=path)

    def _create_model(self, java_model: str):
        """Creates the model using the java model.

        Args:
            java_model (str): The name of the java model.
        """
        return DeIdentificationModel(java_model=java_model)


class DeIdentificationModel(AnnotatorModelInternal, DeIdentificationParams):
    """ The DeIdentificationModel model can obfuscate or mask the entities that contains personal information. These can be set with a file of
    regex patterns with setRegexPatternsDictionary, where each line is a mapping of entity to regex

    .. note::
        If the mode is set to 'obfuscate', the DeIdentificationModel utilizes **java.security.SecureRandom** for generating fake data.
        You can select a generation algorithm by configuring the system environment variable **SPARK_NLP_JSL_SEED_ALGORITHM**.
        The chosen algorithm may impact the generation of fake data, performance, and potential blocking issues.
        For information about standard RNG algorithm names, refer to the SecureRandom section in the `Number Generation Algorithms
        <https://docs.oracle.com/javase/8/docs/technotes/guides/security/StandardNames.html#SecureRandom>`_.
        The default algorithm is 'SHA1PRNG'.

    ========================================= ======================
    Input Annotation types                    Output Annotation type
    ========================================= ======================
    ``DOCUMENT, CHUNK, TOKEN``                ``DOCUMENT``
    ========================================= ======================

    The configuration params for that module are in class DeidentificationParams.

    Parameters
    ----------
    regexEntities
        Keep the regex entities used in the regexPatternDictionary


    Examples
    --------

    >>> import sparknlp
    >>> from sparknlp.base import *
    >>> from sparknlp_jsl.common import *
    >>> from sparknlp.annotator import *
    >>> from sparknlp.training import *
    >>> import sparknlp_jsl
    >>> from sparknlp_jsl.base import *
    >>> from sparknlp_jsl.annotator import *
    >>> from pyspark.ml import Pipeline
    >>> documentAssembler = DocumentAssembler() \\
    ...     .setInputCol("text") \\
    ...     .setOutputCol("document")
    ...
    >>>  sentenceDetector = SentenceDetector() \\
    ...     .setInputCols(["document"]) \\
    ...     .setOutputCol("sentence") \\
    ...     .setUseAbbreviations(True)
    ...
    >>> tokenizer = Tokenizer() \\
    ...     .setInputCols(["sentence"]) \\
    ...     .setOutputCol("token")
    ...
    >> embeddings = WordEmbeddingsModel \\
    ...     .pretrained("embeddings_clinical", "en", "clinical/models") \\
    ...     .setInputCols(["sentence", "token"]) \\
    ...     .setOutputCol("embeddings")
    ...
     Ner entities
    >>> clinical_sensitive_entities = MedicalNerModel \\
    ...     .pretrained("ner_deid_enriched", "en", "clinical/models") \\
    ...     .setInputCols(["sentence", "token", "embeddings"]).setOutputCol("ner")
    ...
    >>> nerConverter = NerConverter() \\
    ...     .setInputCols(["sentence", "token", "ner"]) \\
    ...     .setOutputCol("ner_con")
    ...
     Deidentification
    >>> deIdentification = DeIdentificationModel.pretrained("deidentify_large", "en", "clinical/models") \\
    ...     .setInputCols(["ner_chunk", "token", "sentence"]) \\
    ...     .setOutputCol("dei") \\
    ...     .setMode("obfuscate") \\
    ...     .setDateFormats(Array("MM/dd/yy","yyyy-MM-dd")) \\
    ...     .setObfuscateDate(True) \\
    ...     .setDateTag("DATE") \\
    ...     .setDays(5) \\
    ...     .setObfuscateRefSource("both")
    >>> data = spark.createDataFrame([
    ...     ["# 7194334 Date : 01/13/93 PCP : Oliveira , 25 years-old , Record date : 2079-11-09."]
    ...     ]).toDF("text")
    >>> pipeline = Pipeline(stages=[
    ...     documentAssembler,
    ...     sentenceDetector,
    ...     tokenizer,
    ...     embeddings,
    ...     clinical_sensitive_entities,
    ...     nerConverter,
    ...     deIdentification
    ... ])
    >>> result = pipeline.fit(data).transform(data)
    >>> result.select("dei.result").show(truncate = False)
     +--------------------------------------------------------------------------------------------------+
     |result                                                                                            |
     +--------------------------------------------------------------------------------------------------+
     |[# 01010101 Date : 01/18/93 PCP : Dr. Gregory House , <AGE> years-old , Record date : 2079-11-14.]|
     +--------------------------------------------------------------------------------------------------+
    """

    inputAnnotatorTypes = [AnnotatorType.DOCUMENT, AnnotatorType.TOKEN, AnnotatorType.CHUNK]
    outputAnnotatorType = AnnotatorType.DOCUMENT
    name = "DeIdentificationModel"

    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.deid.DeIdentificationModel", java_model=None):
        super(DeIdentificationModel, self).__init__(
            classname=classname,
            java_model=java_model
        )

    regexEntities = Param(Params._dummy(), "regexEntities",
                          "Keep the regex entities used in the regexPatternDictionary",
                          TypeConverters.toListString)

    def getRegexEntities(self):
        """Return the regexEntities value.
        """
        return self.getOrDefault(self.regexEntities)

    @staticmethod
    def pretrained(name="deidentify_enriched_clinical", lang="en", remote_loc="clinical/models"):
        """Downloads and loads a pretrained model.

        Parameters
        ----------
        name : str, optional
            Name of the pretrained model, by default "deidentify_enriched_clinical"
        lang : str, optional
            Language of the pretrained model, by default "en"
        remote_loc : str, optional
            Optional remote address of the resource, by default clinical/models. Will use
            Spark NLPs repositories otherwise.

        Returns
        -------
        DeIdentificationModel
            The restored model
        """
        from sparknlp_jsl.pretrained import InternalResourceDownloader
        return InternalResourceDownloader.downloadModel(DeIdentificationModel, name, lang, remote_loc,
                                                        j_dwn='InternalsPythonResourceDownloader')
