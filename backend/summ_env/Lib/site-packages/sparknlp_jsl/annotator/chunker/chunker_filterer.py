from sparknlp_jsl.annotator.filtering_params import FilteringParams
from sparknlp_jsl.common import *


class ChunkFiltererApproach(AnnotatorApproachInternal, FilteringParams):
    """Trains a ``ChunkFilterer`` annotator.
    ChunkFiltererApproach can filter chunks coming from CHUNK annotations.
    Filters can be set via white list and black list or a regular expression.
    White list criteria is enabled by default. To use regex, `criteria` has to be set to `regex`.
    Additionally, It can filter chunks according to the confidence of the chunk in the metadata.
    
    ==============================  ======================
    Input Annotation types          Output Annotation type
    ==============================  ======================
        ``DOCUMENT, CHUNK``             ``CHUNK``
    ==============================  ======================

    Parameters
    ----------
    whiteList: list
        If defined, list of entities to process. The rest will be ignored.
    blackList: list
        If defined, list of entities to ignore. The rest will be processed.
    caseSensitive: bool
        Determines whether the definitions of the white listed and black listed entities are case sensitive. Default: True.
        If the filterValue is 'entity', 'caseSensitive' is always False.
    regex: str
       If defined, list of regex to process the chunks (Default: []).
    criteria: str
           Tag representing what is the criteria to filter the chunks. Possibles values are:
           - isIn: Filter by the chunk
           - regex: Filter using a regex
    filterValue: str
        If equal to "entity", use the ner label to filter. If set to "result",
        use the `result` attribute of the annotation to filter.
    entitiesConfidenceResource: str
        Path to a CSV file containing the entity pairs to remove chunks based on
        the confidance level. The CSV file should have two columns: entity and
        confidenceThreshold. The entity column should contain the entity name
        and the confidenceThreshold column should contain the confidence
        threshold to filter the chunks.

    Examples
    --------
    >>> import sparknlp
    >>> from sparknlp.base import *
    >>> from sparknlp_jsl.common import *
    >>> from sparknlp.annotator import *
    >>> from sparknlp.training import *
    >>> import sparknlp_jsl
    >>> from sparknlp_jsl.base import *
    >>> from sparknlp_jsl.annotator import *
    >>> from pyspark.ml import Pipeline
    >>> data = spark.createDataFrame([["Has a past history of gastroenteritis and stomach pain, however patient ..."]]).toDF("text")
    >>> docAssembler = DocumentAssembler().setInputCol("text").setOutputCol("document")
    >>> sentenceDetector = SentenceDetector().setInputCols(["document"]).setOutputCol("sentence")
    >>> tokenizer = Tokenizer().setInputCols(["sentence"]).setOutputCol("token")
    >>> posTagger = PerceptronModel.pretrained() \\
    ...    .setInputCols(["sentence", "token"]) \\
    ...    .setOutputCol("pos")
    >>> chunker = Chunker() \\
    ...   .setInputCols(["pos", "sentence"]) \\
    ...   .setOutputCol("chunk") \\
    ...   .setRegexParsers(["(<NN>)+"])
    ...
    >>> chunkerFilter = ChunkFiltererApproach() \\
    ...   .setInputCols(["sentence","chunk"]) \\
    ...   .setOutputCol("filtered") \\
    ...   .setCriteria("isin") \\
    ...   .setWhiteList(["gastroenteritis"])
    ...
    >>> pipeline = Pipeline(stages=[
    ...   docAssembler,
    ...   sentenceDetector,
    ...   tokenizer,
    ...   posTagger,
    ...   chunker,
    ...   chunkerFilter])
    ...
    >>> model = pipeline.fit(data)
    """
    inputAnnotatorTypes = [AnnotatorType.DOCUMENT, AnnotatorType.CHUNK]
    outputAnnotatorType = AnnotatorType.CHUNK

    name = "ChunksFilterApproach"

    entitiesConfidenceResource = Param(Params._dummy(),
                                       "entitiesConfidenceResource",
                                       "Path to csv with  entity pairs to remove based on the confidence level",
                                       typeConverter=TypeConverters.identity)
    entitiesConfidenceResourceAsJsonString = Param(Params._dummy(),
                                                "entitiesConfidenceResourceAsJsonString",
                                                "string given as JSON with entity pairs to remove chunks based on the confidence level",
                                                typeConverter=TypeConverters.toString)

    def setFilterEntity(self, filter_by:str):
        """Sets the filterValue parameter.

        If equal to "entity", use the ner label to filter. If set to "result",
        use the `result` attribute of the annotation to filter.

        Parameters
        ----------
        filter_by : str
           possibles values result|entity.
        """
        return self._set(filterValue=filter_by)

    def setEntitiesConfidenceResource(self, path:str, read_as:str=ReadAs.TEXT, options:dict=None):
        """Sets the entitiesConfidenceResource parameter.

        Parameters
        ----------
        path : str
              Path to csv with  entity pairs to remove based on the confidence level
        read_as : str
                Read file as 'TEXT', 'SPARK', or 'BINARY'.
        options : dict
                Options for reading the file.
        """
        if options is None:
            options = {"delimiter": ","}
        return self._set(entitiesConfidenceResource=ExternalResource(path, read_as, options))

    def setEntitiesConfidenceResourceAsJsonString(self, json):
        """Sets the entitiesConfidenceResource parameter as JSON String.

        Parameters
        ----------
        json : str
            string given as JSON with entity pairs to remove chunks based on the confidence level
        """
        return self._set(entitiesConfidenceResourceAsJsonString=json)

    @keyword_only
    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.chunker.ChunkFiltererApproach"):
        super(ChunkFiltererApproach, self).__init__(classname=classname)

    def _create_model(self, java_model:str):
        """Creates the model using the java model.

        Args:
            java_model (str): The name of the java model.
        """
        return ChunkFilterer(java_model=java_model)


class ChunkFilterer(AnnotatorModelInternal, FilteringParams):
    """ChunkFilterer can filter chunks coming from CHUNK annotations.
    Filters can be set via white list and black list or a regular expression.
    White list criteria is enabled by default. To use regex, `criteria` has to be set to `regex`.
    Additionally, It can filter chunks according to the confidence of the chunk in the metadata.
    
    ==============================  ======================
    Input Annotation types          Output Annotation type
    ==============================  ======================
       ``DOCUMENT, CHUNK``                ``CHUNK``
    ==============================  ======================

    Parameters
    ----------
    whiteList: list
        If defined, list of entities to process. The rest will be ignored.
    blackList: list
        If defined, list of entities to ignore. The rest will be processed.
    caseSensitive: bool
        Determines whether the definitions of the white listed and black listed entities are case sensitive. Default: True.
        If the filterValue is 'entity', 'caseSensitive' is always False.
    regex: list
       If defined, list of regex to process the chunks (Default: []).
    criteria: str
           Tag representing what is the criteria to filter the chunks. Possibles values are:
           - isIn: Filter by the chunk
           - regex: Filter by using a regex
    entitiesConfidence: dict[str, float]
        Pairs (entity,confidenceThreshold) to filter the chunks with  entities
        which have confidence lower than the confidence threshold.
    filterValue: str
        Possible values are 'result' and 'entity'.
        If the value is 'result', It filters according to the result of the Annotation.
        If the value is 'entity', It filters according to the entity field in the metadata of the Annotation.

    Examples
    --------

       >>> import sparknlp
       >>> from sparknlp.base import *
       >>> from sparknlp_jsl.common import *
       >>> from sparknlp.annotator import *
       >>> from sparknlp.training import *
       >>> import sparknlp_jsl
       >>> from sparknlp_jsl.base import *
       >>> from sparknlp_jsl.annotator import *
       >>> from pyspark.ml import Pipeline
       >>> data = spark.createDataFrame([["Has a past history of gastroenteritis and stomach pain, however patient ..."]]).toDF("text")
       >>> docAssembler = DocumentAssembler().setInputCol("text").setOutputCol("document")
       >>> sentenceDetector = SentenceDetector().setInputCols(["document"]).setOutputCol("sentence")
       >>> tokenizer = Tokenizer().setInputCols(["sentence"]).setOutputCol("token")
       >>> posTagger = PerceptronModel.pretrained() \\
       ...    .setInputCols(["sentence", "token"]) \\
       ...    .setOutputCol("pos")
       >>> chunker = Chunker() \\
       ...   .setInputCols(["pos", "sentence"]) \\
       ...   .setOutputCol("chunk") \\
       ...   .setRegexParsers(["(<NN>)+"])
       ...
       >>> chunkerFilter = ChunkFilterer() \\
       ...   .setInputCols(["sentence","chunk"]) \\
       ...   .setOutputCol("filtered") \\
       ...   .setCriteria("isin") \\
       ...   .setWhiteList(["gastroenteritis"])
       ...
       >>> pipeline = Pipeline(stages=[
       ...   docAssembler,
       ...   sentenceDetector,
       ...   tokenizer,
       ...   posTagger,
       ...   chunker,
       ...   chunkerFilter])
       ...
       >>> result = pipeline.fit(data).transform(data)
       >>> result.selectExpr("explode(chunk)").show(truncate=False)


       >>> result.selectExpr("explode(chunk)").show(truncate=False)
       +---------------------------------------------------------------------------------+
       |col                                                                              |
       +---------------------------------------------------------------------------------+
       |{chunk, 11, 17, history, {sentence -> 0, chunk -> 0}, []}                        |
       |{chunk, 22, 36, gastroenteritis, {sentence -> 0, chunk -> 1}, []}                |
       |{chunk, 42, 53, stomach pain, {sentence -> 0, chunk -> 2}, []}                   |
       |{chunk, 64, 70, patient, {sentence -> 0, chunk -> 3}, []}                        |
       |{chunk, 81, 110, stomach pain now.We don't care, {sentence -> 0, chunk -> 4}, []}|
       |{chunk, 118, 132, gastroenteritis, {sentence -> 0, chunk -> 5}, []}              |
       +---------------------------------------------------------------------------------+

       >>> result.selectExpr("explode(filtered)").show(truncate=False)
       +-------------------------------------------------------------------+
       |col                                                                |
       +-------------------------------------------------------------------+
       |{chunk, 22, 36, gastroenteritis, {sentence -> 0, chunk -> 1}, []}  |
       |{chunk, 118, 132, gastroenteritis, {sentence -> 0, chunk -> 5}, []}|
       +-------------------------------------------------------------------+
         """
    inputAnnotatorTypes = [AnnotatorType.DOCUMENT, AnnotatorType.CHUNK]
    outputAnnotatorType = AnnotatorType.CHUNK

    name = "ChunkFilterer"

    def setFilterEntity(self, filter_by:str):
        """Sets the filterValue parameter.

        If equal to "entity", use the ner label to filter. If set to "result",
        use the `result` attribute of the annotation to filter.

        Parameters
        ----------
        filter_by : str
           possibles values result|entity.
        """
        return self._set(filterValue=filter_by)

    entitiesConfidence = Param(Params._dummy(),
                               "entitiesConfidence",
                               "Pairs (entity,confidenceThreshold) to filter the chunks with  entities which have confidence lower than the confidence threshold.",
                               typeConverter = TypeConverters.identity)

    def setEntitiesConfidence(self, entities_confidence:dict):
        """Sets the entitiesConfidence parameter.

        Parameters
        ----------
        entities_confidence : dict[str, float]
            Pairs (entity,confidenceThreshold) to filter the chunks with  entities
            which have confidence lower than the confidence threshold.
        """
        self._call_java("setEntitiesConfidence", entities_confidence)
        return self

    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.chunker.ChunkFilterer", java_model=None):
        super(ChunkFilterer, self).__init__(
            classname=classname,
            java_model=java_model
        )
