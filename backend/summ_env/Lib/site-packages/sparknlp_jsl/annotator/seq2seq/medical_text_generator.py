from sparknlp_jsl.common import *
from sparknlp_jsl.annotator.qa.beam_search_params import BeamSearchParams

class MedicalTextGenerator(AnnotatorModelInternal, BeamSearchParams, HasBatchedAnnotate, HasCaseSensitiveProperties, HasEngine):
    """
    MedicalTextGenerator is a GPT based model for text generation.

    ==========================================  ======================
    Input Annotation types                      Output Annotation type
    ==========================================  ======================
    ``DOCUMENT, DOCUMENT``                      ``CHUNK``
    ==========================================  ======================

    Parameters
    ----------
    maxNewTokens
        Maximum number of of new tokens to generate, by default 30
    maxContextLength
        Maximum length of context text
    configProtoBytes
        ConfigProto from tensorflow, serialized into byte array.
    doSample
        Whether or not to use sampling; use greedy decoding otherwise, by default False
    topK
        The number of highest probability vocabulary tokens to consider, by default 1
    noRepeatNgramSize
        The number of tokens that can't be repeated in the same order. Useful for preventing loops.
        The default is 0.
    ignoreTokenIds
        A list of token ids which are ignored in the decoder's output, by
        default []
    randomSeed: int
        Random seed. Set to positive integer to get reproducible results, by default None.
    customPrompt
        Custom prompt template. The only available variable is {DOCUMENT} and it is populated with
        the contents of the input document
    Examples
    --------
    >>> data = spark.createDataFrame([["Covid 18 is "]]).toDF("prompt")
    >>> document_assembler = DocumentAssembler()\
    ...   .setInputCol("prompt")\
    ...   .setOutputCol("document_prompt")
    ...
    >>> med_text_generator = sparknlp_jsl.annotators.qa.MedicalTextGenerator\
    ...   .pretrained()\
    ...   .setInputCols(["document_prompt"])\
    ...   .setMaxNewTokens(100)\
    ...   .setOutputCol("answer")\
    >>> pipeline = Pipeline(stages=[document_assembler, med_text_generator])
    >>> pipeline\
    ...   .fit(data)\
    ...   .select("answer.result")\
    ...   .show(truncate=False)
    +-------+
    |result |
    +-------+
    |[Convid 19 is a pandemic that has affected millions of people worldwide.]  |
    +-------+
    """

    name = "MedicalTextGenerator"

    inputAnnotatorTypes = [AnnotatorType.DOCUMENT]

    outputAnnotatorType = AnnotatorType.DOCUMENT

    stopAtEos = Param(Params._dummy(), "stopAtEos",
                           "Stop text generation when the end-of-sentence token is encountered.",
                           typeConverter=TypeConverters.toBoolean)

    configProtoBytes = Param(Params._dummy(),
                             "configProtoBytes",
                             "ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()",
                             TypeConverters.toListInt)

    useCache = Param(Params._dummy(), "useCache", "Cache internal state of the model to improve performance",
                     typeConverter=TypeConverters.toBoolean)

    maxTextLength = Param(Params._dummy(), "maxTextLength", "Max text length to process", typeConverter=TypeConverters.toInt)

    modelType = Param(Params._dummy(), "modelType",
                         "Model type",
                         typeConverter=TypeConverters.toString)

    mlFrameworkType = Param(Params._dummy(), "mlFrameworkType",
                      "ML framework type",
                      typeConverter=TypeConverters.toString)

    customPrompt = Param(Params._dummy(), "customPrompt",
                         "Custom prompt template",
                         typeConverter=TypeConverters.toString)
    def setMaxTextLength(self, value):
        """Set max text length to process

        Parameters
        ----------
        value : int
            max text length
        """
        return self._set(maxTextLength=value)


    def setUseCache(self, value):
        """Cache internal state of the model to improve performance

        Parameters
        ----------
        value : bool
            Whether or not to use cache
        """
        return self._set(useCache=value)

    def setStopAtEos(self, b):
        """Stop text generation when the end-of-sentence token is encountered.

        Parameters
        ----------
        b : bool
            whether to stop at end-of-sentence token or not
        """
        return self._set(stopAtEos=b)

    def setCustomPrompt(self, value):
        """Sets the custom prompt template. The only available variable is {DOCUMENT}, which is populated with the
        contents of the input document

        Parameters
        ----------
        value : str
            prompt template
        """
        return self._set(customPrompt=value)


    def setAdditionalTokens(self, additionalTokens):
        """Set additional tokens

        Parameters
        ----------
        value : dict[int, str]
            additional tokens
        """
        self._call_java("setAdditionalTokens", additionalTokens)
        return self

    def getAdditionalTokens(self):
        """
        Get additional tokens

        Return:
            dict[int, str]
        """
        additional_tokens_str = self._call_java("getAdditionalTokensStr")
        additional_tokens = {}
        for t in additional_tokens_str.split("\n"):
            parts = t.split("\t\t")
            if len(parts) == 2:
                additional_tokens[parts[0]] = parts[1]

        return additional_tokens

    def setConfigProtoBytes(self, b):
        """Sets configProto from tensorflow, serialized into byte array.

        Parameters
        ----------
        b : List[int]
            ConfigProto from tensorflow, serialized into byte array
        """
        return self._set(configProtoBytes=b)

    @keyword_only
    def __init__(self, classname="com.johnsnowlabs.nlp.annotators.seq2seq.MedicalTextGenerator", java_model=None):
        super(MedicalTextGenerator, self).__init__(
            classname=classname,
            java_model=java_model
        )

        self._setDefault(
            batchSize=4,
            caseSensitive=True,
            stopAtEos=False,
            useCache=True,
            maxTextLength=512,
            customPrompt="",
            modelType="Unknown",
            mlFrameworkType="tensorflow"
        )

    @staticmethod
    def loadSavedModel(folder, spark_session, model_type):
        """Loads a locally saved model.

        Parameters
        ----------
        folder : str
            Folder of the saved model
        spark_session : pyspark.sql.SparkSession
            The current SparkSession
        model_type : str
            The type of the model
        Returns
        -------
        MedicalTextGenerator
            The restored model
        """
        from sparknlp_jsl.internal import _MedicalTextGeneratorLoader
        jModel = _MedicalTextGeneratorLoader(folder, spark_session._jsparkSession, model_type)._java_obj
        return MedicalTextGenerator(java_model=jModel)

    @staticmethod
    def pretrained(name="text_generator_biomedical_biogpt_base", lang="en", remote_loc="clinical/models"):
        """Downloads and loads a pretrained model.

        Parameters
        ----------
        name : str, optional
            Name of the pretrained model, by default "text_generator_biomedical_biogpt_base"
        lang : str, optional
            Language of the pretrained model, by default "en"
        remote_loc : str, optional
            Optional remote address of the resource, by default None. Will use
            Spark NLPs repositories otherwise.

        Returns
        -------
        MedicalTextGenerator
            The restored model
        """
        from sparknlp_jsl.pretrained import InternalResourceDownloader
        return InternalResourceDownloader.downloadModel(MedicalTextGenerator, name, lang, remote_loc,
                                                        j_dwn='InternalsPythonResourceDownloader')
