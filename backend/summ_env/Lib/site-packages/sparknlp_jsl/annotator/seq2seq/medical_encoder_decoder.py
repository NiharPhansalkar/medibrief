from sparknlp_jsl.common import *


class MedicalEncoderDecoder(
    AnnotatorModelInternal, HasBatchedAnnotate, HasCaseSensitiveProperties, HasEngine
):
    name = "MedicalEncoderDecoder"

    inputAnnotatorTypes = [AnnotatorType.DOCUMENT]

    outputAnnotatorType = AnnotatorType.DOCUMENT

    configProtoBytes = Param(
        Params._dummy(),
        "configProtoBytes",
        "ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString()",
        TypeConverters.toListInt
    )

    doSample = Param(
        Params._dummy(),
        "doSample",
        "Whether or not to use sampling; use greedy decoding otherwise",
        typeConverter=TypeConverters.toBoolean
    )

    topK = Param(
        Params._dummy(),
        "topK",
        "The number of highest probability vocabulary tokens to keep for top-k-filtering",
        typeConverter=TypeConverters.toInt
    )

    ignoreTokenIds = Param(
        Params._dummy(),
        "ignoreTokenIds",
        "A list of token ids which are ignored in the decoder's output",
        typeConverter=TypeConverters.toListInt
    )

    maxNewTokens = Param(
        Params._dummy(),
        "maxNewTokens",
        "Maximum number of new tokens to be generated",
        typeConverter=TypeConverters.toInt
    )

    stopAtEos = Param(
        Params._dummy(),
        "stopAtEos",
        "Stop text generation when the end-of-sentence token is encountered.",
        typeConverter=TypeConverters.toBoolean
    )

    task = Param(
        Params._dummy(),
        "task",
        "Transformer's task",
        typeConverter=TypeConverters.toString
    )

    useCache = Param(
        Params._dummy(),
        "useCache",
        "Use caching to enhance performance",
        typeConverter=TypeConverters.toBoolean
    )

    randomSeed = Param(
        Params._dummy(), "randomSeed", "Random seed", typeConverter=TypeConverters.toInt
    )

    maxTextLength = Param(
        Params._dummy(),
        "maxTextLength",
        "Max text length to process",
        typeConverter=TypeConverters.toInt
    )

    refineSummary = Param(
        Params._dummy(),
        "refineSummary",
        "Set true to perform refined summarization at increased computation cost.",
        typeConverter=TypeConverters.toBoolean
    )
    refineSummaryTargetLength = Param(
        Params._dummy(),
        "refineSummaryTargetLength",
        "Target length of summarizations in Tokens (delimited by whitespace). Takes only effect when refineSummary=True",
        typeConverter=TypeConverters.toInt
    )
    refineChunkSize = Param(
        Params._dummy(),
        "refineChunkSize",
        "How large should refined chunks Be. Should be equal to LLM context window size in tokens. Takes only effect when refineSummary=True",
        typeConverter=TypeConverters.toInt
    )
    refineMaxAttempts = Param(
        Params._dummy(),
        "refineMaxAttempts",
        "How many times should chunks be re-summarized while they are above SummaryTargetLength before stopping. Takes only effect when refineSummary=True",
        typeConverter=TypeConverters.toInt
    )

    mlFrameworkType = Param(
        Params._dummy(),
        "mlFrameworkType",
        "ML framework type",
        typeConverter=TypeConverters.toString
    )

    def setRefineSummary(self, b):
        """Set true to perform refined summarization at increased computation cost.

        Parameters
        ----------
        b : bool
            whether to perform refined summaries  or not
        """
        return self._set(refineSummary=b)


    def setRefineSummaryTargetLength(self, i):
        """Target length of summarizations in Tokens (delimited by whitespace). Takes only effect when refineSummary=True

        Parameters
        ----------
        i : int
            Target length of summarizations in Tokens (delimited by whitespace). Takes only effect when refineSummary=True
        """
        return self._set(refineSummaryTargetLength=i)

    def setRefineChunkSize(self, i):
        """How large should refined chunks Be. Should be equal to LLM context window size in tokens. Takes only effect when refineSummary=True

        Parameters
        ----------
        i : int
            How large should refined chunks Be. Should be equal to LLM context window size in tokens. Takes only effect when refineSummary=True
        """
        return self._set(refineChunkSize=i)
    def setRefineMaxAttempts(self, i):
        """How many times should chunks be re-summarized while they are above SummaryTargetLength before stopping. Takes only effect when refineSummary=True

        Parameters
        ----------
        i : int
            How many times should chunks be re-summarized while they are above SummaryTargetLength before stopping. Takes only effect when refineSummary=True
        """
        return self._set(refineMaxAttempts=i)





    noRepeatNgramSize = Param(Params._dummy(), "noRepeatNgramSize",
                              "If set to int > 0, all ngrams of that size can only occur once",
                              typeConverter=TypeConverters.toInt)

    def setConfigProtoBytes(self, b):
        """Sets configProto from tensorflow, serialized into byte array.

        Parameters
        ----------
        b : List[int]
            ConfigProto from tensorflow, serialized into byte array
        """
        return self._set(configProtoBytes=b)

    def setStopAtEos(self, b):
        """Stop text generation when the end-of-sentence token is encountered.

        Parameters
        ----------
        b : bool
            whether to stop at end-of-sentence token or not
        """
        return self._set(stopAtEos=b)

    def setIgnoreTokenIds(self, value):
        """A list of token ids which are ignored in the decoder's output.

        Parameters
        ----------
        value : List[int]
            The words to be filtered out
        """
        return self._set(ignoreTokenIds=value)

    def setDoSample(self, value):
        """Sets whether or not to use sampling, use greedy decoding otherwise.

        Parameters
        ----------
        value : bool
            Whether or not to use sampling; use greedy decoding otherwise
        """
        return self._set(doSample=value)

    def setTopK(self, value):
        """Sets the number of highest probability vocabulary tokens to keep for
        top-k-filtering.

        Parameters
        ----------
        value : int
            Number of highest probability vocabulary tokens to keep
        """
        return self._set(topK=value)

    def setMaxNewTokens(self, value):
        """Sets the maximum number of new tokens to be generated

        Parameters
        ----------
        value : int
            the maximum number of new tokens to be generated
        """
        return self._set(maxNewTokens=value)

    def setNoRepeatNgramSize(self, value):
        """Sets size of n-grams that can only occur once.

        If set to int > 0, all ngrams of that size can only occur once.

        Parameters
        ----------
        value : int
            N-gram size can only occur once
        """
        return self._set(noRepeatNgramSize=value)

    def setRandomSeed(self, value):
        """Set random seed

        Parameters
        ----------
        value : int
            random seed
        """
        return self._set(randomSeed=value)

    def setMaxTextLength(self, value):
        """Set max text length to process

        Parameters
        ----------
        value : int
            max text length
        """
        return self._set(maxTextLength=value)

    @keyword_only
    def __init__(self, classname, java_model=None):
        super(MedicalEncoderDecoder, self).__init__(
            classname=classname, java_model=java_model
        )
        self._setDefault(
            doSample=False,
            topK=3,
            ignoreTokenIds=[],
            maxNewTokens=30,
            task="",
            useCache=False,
            maxTextLength=1024,
            stopAtEos=False,
            noRepeatNgramSize=0,
            mlFrameworkType="tensorflow"
        )
