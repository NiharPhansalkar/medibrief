from pyspark.ml import PipelineModel
from sparkocr.transformers import *
from sparkocr.enums import *
from sparkocr.utils import to_pil_image
import pandas as pd
from lxml import etree
from PIL import Image, ImageDraw, ImageFont
import re
import os
import random
from collections import OrderedDict


label2color = dict()
label2rgb = dict()
colors = ['aqua', 'aquamarine', 'black', 'blanchedalmond', 'blue', 'blueviolet', 'brown', 'burlywood', 'cadetblue',
          'chartreuse', 'chocolate', 'cornflowerblue', 'crimson', 'darkblue', 'darkcyan', 'darkgoldenrod', 'darkgray',
          'darkgreen', 'darkkhaki', 'darkmagenta',
          'darkolivegreen', 'darkorange', 'darkred', 'darksalmon', 'darkseagreen', 'darkslateblue', 'darkslategray',
          'darkturquoise', 'darkviolet', 'deeppink', 'deepskyblue', 'dimgray', 'dodgerblue', 'fuchsia', 'firebrick',
          'gold', 'goldenrod', 'green', 'greenyellow', 'hotpink',
          'indianred', 'indigo', 'indianred', 'khaki', 'lightblue', 'lightcoral', 'lightgoldenrodyellow', 'lightgreen',
          'lightgray', 'lightgrey', 'lightpink', 'limegreen', 'lightsalmon', 'magenta', 'mediumblue', 'mediumorchid',
          'mediumpurple', 'mediumspringgreen', 'mediumseagreen', 'mediumvioletred',
          'midnightblue', 'navy', 'olive', 'orange', 'orangered', 'peru', 'pink', 'purple', 'rebeccapurple', 'red',
          'rosybrown', 'saddlebrown', 'salmon', 'sandybrown', 'seagreen', 'sienna', 'violet', 'tan', 'tomato',
          'thistle',
          'yellow', 'yellowgreen']

colors_rgb = [(0, 255, 255), (127, 255, 212), (0, 0, 0), (255, 235, 205), (0, 0, 255), (138, 43, 226), (165, 42, 42),
              (222, 184, 135), (95, 158, 160), (127, 255, 0), (210, 105, 30), (100, 149, 237), (220, 20, 60),
              (0, 0, 139), (0, 139, 139), (184, 134, 11), (169, 169, 169), (0, 100, 0), (189, 183, 107), (139, 0, 139),
              (85, 107, 47), (255, 140, 0), (139, 0, 0), (233, 150, 122), (143, 188, 143), (72, 61, 139), (47, 79, 79),
              (0, 206, 209), (148, 0, 211), (255, 20, 147), (0, 191, 255), (105, 105, 105), (30, 144, 255),
              (255, 0, 255), (178, 34, 34), (255, 215, 0), (218, 165, 32), (0, 128, 0), (173, 255, 47), (255, 105, 180),
              (205, 92, 92), (75, 0, 130), (205, 92, 92), (240, 230, 140), (173, 216, 230), (240, 128, 128),
              (250, 250, 210), (144, 238, 144), (211, 211, 211), (211, 211, 211), (255, 182, 193), (50, 205, 50),
              (255, 160, 122), (255, 0, 255), (0, 0, 205), (186, 85, 211), (147, 112, 219), (0, 250, 154),
              (60, 179, 113), (199, 21, 133),
              (25, 25, 112), (0, 0, 128), (128, 128, 0), (255, 165, 0), (255, 69, 0), (205, 133, 63), (255, 192, 203),
              (128, 0, 128), (102, 51, 153), (255, 0, 0), (188, 143, 143), (139, 69, 19), (250, 128, 114),
              (244, 164, 96), (46, 139, 87), (160, 82, 45), (238, 130, 238), (210, 180, 140), (255, 99, 71),
              (216, 191, 216),
              (255, 255, 0), (154, 205, 50)]

pretrained_model = ("image_handwritten_detector_gsa0803", "en", "public/ocr/models")

def __ocr_pipeline(spark, pdf_path, ner_pipeline, resolution=400, pageIteratorLevel=PageIteratorLevel.SYMBOL, pageSegMode=PageSegmentationMode.SPARSE_TEXT_OSD, confidenceThreshold=70, text_type = "printed"):
    # Read Pdf as image
    pdf_to_image = PdfToImage() \
        .setInputCol("content") \
        .setOutputCol("image_raw") \
        .setResolution(resolution)

    ocr = ImageToText() \
        .setInputCol("image_raw") \
        .setOutputCol("text") \
        .setIgnoreResolution(False) \
        .setPageIteratorLevel(pageIteratorLevel) \
        .setPageSegMode(pageSegMode) \
        .setWithSpaces(True) \
        .setConfidenceThreshold(confidenceThreshold)

    hocr = ImageToHocr() \
        .setInputCol("image_raw") \
        .setOutputCol("hocr") \
        .setIgnoreResolution(False) \
        .setOcrParams(["preserve_interword_spaces=0"]) \
        .setPageIteratorLevel(pageIteratorLevel) \
        .setPageSegMode(pageSegMode)

    if text_type == "handwritten" or text_type == "both":
           handwritten_detector = ImageHandwrittenDetector() \
           .pretrained(*pretrained_model) \
           .setInputCol("image_raw") \
           .setOutputCol("handwritten_regions") \
           .setScoreThreshold(0.4) 
           pipeline_stages = [pdf_to_image, handwritten_detector, ocr, hocr] + ner_pipeline.stages
    
    else:
           pipeline_stages = [pdf_to_image, ocr, hocr] + ner_pipeline.stages

    ocr_pipeline = PipelineModel(stages=pipeline_stages)

    print("OCR pipeline is running...")

    pdfs = spark.read.format("binaryFile").load(pdf_path)
    result = ocr_pipeline.transform(pdfs).cache()

    return result


def __hocr_to_dataframe(hocr):
    with open("hocr_content.xml", 'w', encoding='utf-8') as f:
        f.write(str(hocr))
    doc = etree.parse("hocr_content.xml")
    words = []
    word_conf = []
    fonts = []
    sizes = []
    font = -1
    size = -1
    for path in doc.xpath('//*'):

        try:
            if 'ocr_line' in path.values():
                a = float(path.values()[2].split(';')[3].split()[1])
                b = float(path.values()[2].split(';')[4].split()[1])
                font = round((a + b) / 2, 2)
                size = float(path.values()[2].split(';')[2].split()[1])

            if 'ocrx_word' in path.values():
                conf = [x for x in path.values() if 'x_wconf' in x][0]
                word_conf.append((conf.split('bbox ')[1].split(";")[0].split()))
                words.append(path.text)
                fonts.append(font)
                sizes.append(int(size))
        except:
            pass
    df_return = pd.DataFrame({'word': words,
                              'bbox': word_conf,
                              'borders': fonts,
                              'size': sizes})
    try:
        df_return = df_return[df_return['word'].str.strip() != ''].reset_index(drop=True)
    except:
        pass
    os.remove("hocr_content.xml")
    return df_return


def __get_token_df(text):
    try:
        tokens, borders = zip(*[(m.group(0), (m.start(), m.end() - 1)) for m in re.finditer(r'\S+', text)])
        tuples = [(x, y[0], y[1]) for x, y in zip(tokens, borders)]
    except:
        tuples = [('-', 0, 0)]
    df = pd.DataFrame(tuples, columns=['token', 'start', 'end'])
    return df


def __get_mapping(text, hocr=None):
    hdf = __hocr_to_dataframe(hocr)
    token_df = __get_token_df(text)
    token_df['start'] = token_df['start'].astype(int)
    token_df['end'] = token_df['end'].astype(int)
    token_df = pd.concat([hdf, token_df], axis=1)[['token', 'start', 'end', 'bbox', 'borders', 'size']]
    token_df['h'] = token_df.bbox.apply(lambda x: int(x[3]) - int(x[1]) if type(x) is not float else 0)
    return token_df


def __get_coordinates_frame(ent_dict_list, text, hocr):
    token_df = __get_mapping(text, hocr)
    new_ent_dict_list = []

    for entity in ent_dict_list:
        if "\n" in entity["chunk"]:
            part_0_chunk = entity["chunk"].split("\n")[0]
            part_1_chunk = entity["chunk"].split("\n")[1]
            part_0_begin = entity["begin"]
            part_0_end = part_0_begin + entity["chunk"].find("\n") - 1
            part_1_begin = part_0_begin + entity["chunk"].find("\n") + 1
            part_1_end = entity["end"]
            new_ent_dict_list.append({'begin': part_0_begin,
                                      'end': part_0_end,
                                      'chunk': part_0_chunk,
                                      'ner_label': entity["ner_label"],
                                      'sentence_id': entity["sentence_id"]})
            new_ent_dict_list.append({'begin': part_1_begin,
                                      'end': part_1_end,
                                      'chunk': part_1_chunk,
                                      'ner_label': entity["ner_label"],
                                      'sentence_id': entity["sentence_id"]})

        else:
            new_ent_dict_list.append({'begin': entity["begin"],
                                      'end': entity["end"],
                                      'chunk': entity["chunk"],
                                      'ner_label': entity["ner_label"],
                                      'sentence_id': entity["sentence_id"]})

    for i, ent in enumerate(new_ent_dict_list):
        ix = list(
            set(token_df[(token_df.start >= ent['begin']) | (token_df.end >= ent['begin'])].index).intersection(
                set(token_df[(token_df.start <= ent['end'] + 1) | (token_df.end <= ent['end'] + 1)].index)))
        coords = token_df.loc[ix, 'bbox'].values
        if len(coords) > 0:
            xo_list = []
            x1_list = []
            yo_list = []
            y1_list = []
            for box in coords:
                try:
                    xo_list.append(int(box[0]))
                    yo_list.append(int(box[1]))
                    x1_list.append(int(box[2]))
                    y1_list.append(int(box[3]))
                except:
                    xo_list.append(0)
                    yo_list.append(0)
                    x1_list.append(0)
                    y1_list.append(0)
            ent['coord'] = (min(xo_list), min(yo_list), max(x1_list), max(y1_list))
        else:
            ent['coord'] = []
    coord_df_pipe = pd.DataFrame(new_ent_dict_list)
    return coord_df_pipe


def __save_color_chart(style, color_chart_path='color_chart.png'):
    global label2color, label2rgb
    if style == "colored":
        label2color_sorted = OrderedDict(sorted(label2color.items()))

    # highlight
    else:
        label2color_sorted = OrderedDict(sorted(label2rgb.items()))

    n = len(label2color_sorted)
    cols = 4
    rows = ((n - 1) // cols) + 1
    cell_height = 30
    cell_width = 250
    img_height = cell_height * rows
    img_width = cell_width * cols
    i = Image.new("RGB", (img_width, img_height), (0, 0, 0))
    a = ImageDraw.Draw(i)

    for idx, name in enumerate(label2color_sorted):
        y0 = cell_height * (idx // cols)
        y1 = y0 + cell_height
        x0 = cell_width * (idx % cols)
        x1 = x0 + (cell_width / 4)
        a.rectangle([x0, y0, x1, y1], fill=label2color_sorted[name], outline="black")
        a.text((x1 + 1, y0 + 10), name, fill='white')

    i.save(color_chart_path)
    print(f"Color chart of the entity labels is saved to {color_chart_path}.")


def __conv_rgba_to_rgb(_rgba):
    _rgb = Image.new('RGB', _rgba.size, (255, 255, 255))
    _rgb.paste(_rgba, mask=_rgba.split()[3])
    return _rgb


def __save_file(image_list, file_name, style, save_dir, text_type):
    print("Saving started...")
    if text_type == "printed" or text_type == "both":
      images = [__conv_rgba_to_rgb(_f) for _f in image_list]
    else:
      images = image_list
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    images[0].save(f"{save_dir}/{file_name}_{style}.pdf", "PDF", resolution=72.0, save_all=True,
                   append_images=images[1:])
    print(f"File saved to {save_dir}/{file_name}_{style}.pdf successfully.")


def __draw_colored_box(img_pil_deid, coord_df, box_color, label, label_color, text_band = None):
    overlay = Image.new('RGBA', img_pil_deid.size, (0, 0, 0, 0))  # transparent
    draw = ImageDraw.Draw(overlay)  # Create a context for drawing things on it.
    for i, row in coord_df.iterrows():
        draw.rectangle((row['coord'][:2], row['coord'][2:]), fill=box_color, outline=box_color, width=5)
        # Add the text band
        if text_band:
          ppd_position = tuple([row['coord'][0] + 10, row['coord'][1] + 10])
          draw.text(ppd_position, text=text_band, fill="black") 
        
        if label:
            label_position = tuple([row['coord'][0], row['coord'][1] - 10])
            draw.text(label_position, text=row['ner_label'], fill=label_color)
        else:
            pass

    # Alpha composite these two images together to obtain the desired result.
    img = Image.alpha_composite(img_pil_deid, overlay)
    img = img.convert("RGBA")  # Remove alpha for saving in jpg format.

    return img

def __colored_box(result, file_name, style, text_band, chunk_col, save_dir="Colored_Box", box_color=(0,0,0), label=False, label_color="black",
                  display_result=False, text_type='printed'):
    print("Drawing colored box...")
    box_color += (255,)
    image_list = []  # append highlighted images
    res_pd = result.selectExpr("pagenum", f"{chunk_col}.begin", f"{chunk_col}.end", f"{chunk_col}.result",
                               f"{chunk_col}.metadata").toPandas()
    res_pd = res_pd.explode(["begin", "end", "result", "metadata"])
    res_pd = res_pd.rename(columns={"result": "chunk"})
    res_pd["ner_label"] = res_pd.metadata.apply(lambda x: x["entity"])
    res_pd["sentence_id"] = res_pd.metadata.apply(lambda x: x["sentence"])
    res_pd.drop("metadata", axis=1, inplace=True)
    file_length = result.count()
    text_list = result.select('text').collect()
    hocr_list = result.select('hocr').collect()

    for row in range(0, file_length):
        text = text_list[row][0]
        hocr = hocr_list[row][0]
        ent_dict_list = [
            {'begin': x.begin, 'end': x.end, 'chunk': x.chunk, 'ner_label': x.ner_label,
             'sentence_id': x.sentence_id}
            for y, x in res_pd[res_pd.pagenum == row].iterrows()]
        coord_df = __get_coordinates_frame(ent_dict_list, text, hocr)
        img_deid = result.select('image_raw').collect()[row][0]
        img_pil_deid = to_pil_image(img_deid, img_deid.mode)
        img_pil_deid = img_pil_deid.convert("RGBA")

        if text_type == 'printed':
          image_list.append(__draw_colored_box(img_pil_deid, coord_df, box_color, label, label_color, text_band))

        elif text_type == "handwritten":
          regions = get_handwritten_regions(result)
          img_pil_deid = draw_boxes_on_image_handwritten_region(img_pil_deid, regions, style, outline_width = box_color, outline_color = box_color, text_band = text_band)
          image_list.append(img_pil_deid)

        elif text_type == 'both':
          regions = get_handwritten_regions(result)
          handwritten_img = draw_boxes_on_image_handwritten_region(img_pil_deid, regions, style, outline_width=box_color, outline_color=box_color, text_band=text_band)
          img_pil_deid = handwritten_img.convert("RGBA")
          img_pil_deid = __draw_colored_box(img_pil_deid, coord_df, box_color, label, label_color, text_band)
      
          image_list.append(img_pil_deid)
      




    __save_file(image_list, file_name, style, save_dir, text_type)

    if display_result:
        for i in image_list:
            display(i)
    else:
        pass


def __highlight(img_pil_deid, coord_df, label, label_color, black_list):
    # Make a blank image the same size as the image for the rectangle, initialized
    # to a fully transparent (0% opaque) version of the tint color, then draw a
    # semi-transparent version of the square on it.

    global label2rgb, colors
    transparency = .25  # Degree of transparency, 0-100%
    opacity = int(255 * transparency)
    overlay = Image.new('RGBA', img_pil_deid.size, (255, 255, 0, 0))  # yellow
    draw = ImageDraw.Draw(overlay)  # Create a context for drawing things on it.
    for i, row in coord_df.iterrows():
        try:
            pred_label = row['ner_label'].lower()

        except:
            raise Exception(row['ner_label'].lower(), 'label is not in the classes of ner model.')

        try:
            if pred_label in black_list:
                draw.rectangle((row['coord'][:2], row['coord'][2:]), fill=label2rgb[pred_label] + (255,),
                               outline=label2rgb[pred_label] + (255,), width=5)
            else:
                draw.rectangle((row['coord'][:2], row['coord'][2:]), fill=label2rgb[pred_label] + (opacity,),
                               outline=label2rgb[pred_label] + (opacity,), width=5)

        except:
            print("Error in this row:", row)
            continue

        if label:
            label_position = tuple([row['coord'][0], row['coord'][1] - 10])
            draw.text(label_position, text=row['ner_label'], fill=label_color)
        else:
            pass

    # Alpha composite these two images together to obtain the desired result.
    img = Image.alpha_composite(img_pil_deid, overlay)
    img = img.convert("RGBA")  # Remove alpha for saving in jpg format.

    return img


def __highlighted_box(result, file_name, style, chunk_col, black_list, save_dir="Highlighted_Box",
                      label=False,
                      label_color="red", color_chart_path="color_chart.png", display_result=False, text_type='printed'):
    global label2rgb, colors_rgb
    print("Highlighting...")
    image_list = []  # append highlighted images
    black_list = list(set([i.lower() for i in black_list]))  # lowercase black_list
    res_pd = result.selectExpr("pagenum", f"{chunk_col}.begin", f"{chunk_col}.end", f"{chunk_col}.result",
                               f"{chunk_col}.metadata").toPandas()
    res_pd = res_pd.explode(["begin", "end", "result", "metadata"])
    res_pd = res_pd.rename(columns={"result": "chunk"})
    res_pd["ner_label"] = res_pd.metadata.apply(lambda x: x["entity"])
    res_pd["sentence_id"] = res_pd.metadata.apply(lambda x: x["sentence"])
    res_pd.drop("metadata", axis=1, inplace=True)
    random.shuffle(colors_rgb)
    classes = res_pd.ner_label.str.lower().unique()
    label2rgb = dict(zip(classes, colors_rgb))
    file_length = result.count()
    text_list = result.select('text').collect()
    hocr_list = result.select('hocr').collect()

    for row in range(0, file_length):
        text = text_list[row][0]
        hocr = hocr_list[row][0]
        ent_dict_list = [
            {'begin': x.begin, 'end': x.end, 'chunk': x.chunk, 'ner_label': x.ner_label,
             'sentence_id': x.sentence_id}
            for y, x in res_pd[res_pd.pagenum == row].iterrows()]
        coord_df = __get_coordinates_frame(ent_dict_list, text, hocr)
        img_deid = result.select('image_raw').collect()[row][0]
        img_pil_deid = to_pil_image(img_deid, img_deid.mode)
        img_pil_deid = img_pil_deid.convert("RGBA")

        if text_type == 'printed':
          image_list.append(__highlight(img_pil_deid, coord_df, label, label_color, black_list))
          
        elif text_type == "handwritten":
          regions = get_handwritten_regions(result)
          img_pil_deid = draw_boxes_on_image_handwritten_region(img_pil_deid, regions, style, outline_width = None, outline_color = None, text_band = None)
          image_list.append(img_pil_deid)

        elif text_type == 'both':
          regions = get_handwritten_regions(result)
          handwritten_img = draw_boxes_on_image_handwritten_region(img_pil_deid, regions, style, outline_width = None, outline_color = None, text_band = None)
          printed_img = __highlight(img_pil_deid, coord_df, label, label_color, black_list)

          # Resize both images to match the larger dimensions
          if printed_img.size[0] != handwritten_img.size[0] or printed_img.size[1] != handwritten_img.size[1]:
              printed_img = printed_img.resize(handwritten_img.size, Image.ANTIALIAS)
          
          # Convert handwritten image to RGBA mode if it's in RGB mode
          if handwritten_img.mode == 'RGB':
            handwritten_img = handwritten_img.convert('RGBA')

          transparency = 0.5

          # Create a new image with the same dimensions and mode
          res = Image.blend(printed_img, handwritten_img, transparency)
          image_list.append(res)


    __save_file(image_list, file_name, style, save_dir, text_type)
    __save_color_chart("highlight", color_chart_path)

    if display_result:
        for i in image_list:
            display(i)
    else:
        pass


def __draw_outline(img_pil_deid, coord_df, label, label_color, black_list, outline_width = 2, outline_color = None):
    global label2color, colors
    draw = ImageDraw.Draw(img_pil_deid)

    for i, row in coord_df.iterrows():
        try:
            pred_label = row['ner_label'].lower()

        except:
            raise Exception(row['ner_label'].lower(), 'label is not in the classes of the ner model.')

        try:
            if pred_label in black_list:
                draw.rectangle((row['coord'][:2], row['coord'][2:]), outline=label2color[pred_label], width=outline_width,
                               fill=label2color[pred_label])

            else:
                if outline_color is None:
                  draw.rectangle((row['coord'][:2], row['coord'][2:]), outline=label2color[pred_label], width=outline_width)
                else:
                  draw.rectangle((row['coord'][:2], row['coord'][2:]), outline=outline_color, width=outline_width)
        except:
            print("Error in this row:", row)
            continue

        if label:
            label_position = tuple([row['coord'][0], row['coord'][1] - 10])
            draw.text(label_position, text=row['ner_label'], fill=label_color)
        else:
            pass

    return img_pil_deid


def __bounding_box(result, file_name, style, outline_width, outline_color, chunk_col, black_list, save_dir="Bounding_Box", label=False,
                  label_color="red", color_chart_path="color_chart.png", display_result=False, text_type='printed'):
    global label2color, colors
    print("Drawing bounding box...")
    image_list = []  # append images
    black_list = list(set([i.lower() for i in black_list]))  # lowercase black_list
    res_pd = result.selectExpr("pagenum", f"{chunk_col}.begin", f"{chunk_col}.end", f"{chunk_col}.result",
                               f"{chunk_col}.metadata").toPandas()
    res_pd = res_pd.explode(["begin", "end", "result", "metadata"])
    res_pd = res_pd.rename(columns={"result": "chunk"})
    res_pd["ner_label"] = res_pd.metadata.apply(lambda x: x["entity"])
    res_pd["sentence_id"] = res_pd.metadata.apply(lambda x: x["sentence"])
    res_pd.drop("metadata", axis=1, inplace=True)
    random.shuffle(colors)
    classes = res_pd.ner_label.str.lower().unique()
    label2color = dict(zip(classes, colors))
    file_length = result.count()
    text_list = result.select('text').collect()
    hocr_list = result.select('hocr').collect()

    for row in range(0, file_length):
        text = text_list[row][0]
        hocr = hocr_list[row][0]
        ent_dict_list = [
            {'begin': x.begin, 'end': x.end, 'chunk': x.chunk, 'ner_label': x.ner_label,
             'sentence_id': x.sentence_id}
            for y, x in res_pd[res_pd.pagenum == row].iterrows()]
        coord_df = __get_coordinates_frame(ent_dict_list, text, hocr)
        img_deid = result.select('image_raw').collect()[row][0]
        img_pil_deid = to_pil_image(img_deid, img_deid.mode)
        img_pil_deid = img_pil_deid.convert("RGBA")
        
        if text_type == 'printed':
            if outline_color is not None:
                img_pil_deid = __draw_outline(img_pil_deid, coord_df, label, label_color, black_list, outline_width, outline_color)
            else:
                img_pil_deid = __draw_outline(img_pil_deid, coord_df, label, label_color, black_list, outline_width=outline_width, outline_color = "red")
            image_list.append(img_pil_deid)

        elif text_type == 'handwritten':
            regions = get_handwritten_regions(result)
            if outline_color is not None:
                img_pil_deid = draw_boxes_on_image_handwritten_region(img_pil_deid, regions, style, outline_width, outline_color, text_band = None)
            else:
                img_pil_deid = draw_boxes_on_image_handwritten_region(img_pil_deid, regions, style, outline_width, outline_color = "red", text_band = None)
            image_list.append(img_pil_deid)
        
        elif text_type == 'both':
          regions = get_handwritten_regions(result)
          if outline_color is not None:
                handwritten_img = draw_boxes_on_image_handwritten_region(img_pil_deid, regions, style, outline_width, outline_color, text_band = None)
                img_pil_deid = handwritten_img.convert("RGBA")
                img_pil_deid = __draw_outline(img_pil_deid, coord_df, label, label_color, black_list, outline_width, outline_color)
          else:
                handwritten_img = draw_boxes_on_image_handwritten_region(img_pil_deid, regions, style, outline_width, outline_color = "red", text_band = None)
                img_pil_deid = handwritten_img.convert("RGBA")
                img_pil_deid = __draw_outline(img_pil_deid, coord_df, label, label_color, black_list, outline_width=outline_width)   

          image_list.append(img_pil_deid)


    __save_file(image_list, file_name, style, save_dir, text_type)
    __save_color_chart("colored", color_chart_path)

    if display_result:
        for i in image_list:
            display(i)
    else:
        pass

def get_handwritten_regions(result):

    handwritten_regions = []
    for row in result.collect():
        regions = row['handwritten_regions']
        handwritten_regions.extend(regions)
    
    return handwritten_regions

def draw_boxes_on_image_handwritten_region(image_data, regions, style, outline_width, outline_color, text_band):
        overlay = Image.new('RGBA', image_data.size, (0, 0, 0, 0))  # transparent overlay
        draw = ImageDraw.Draw(overlay)  # Create a context for drawing things on it.
   
        for region in regions:
            x, y, w, h = region['x'], region['y'], region['width'], region['height']  # Assuming handwritten_regions contain x, y, width, height
            if style == "bounding_box":
              draw.rectangle([x, y, x + w, y + h], outline=outline_color, width = outline_width)  # Draw a red rectangle on the overlay

            elif style == "colored_box":
              draw.rectangle([x, y, x + w, y + h], outline=outline_color, fill=outline_color, width=5)  # Draw a red rectangle on the overlay
              if text_band:
                draw.text([x + 10, y + 10, x + w + 10, y + h + 10], text=text_band, fill="black")
            elif style == "highlight":
              transparency = .25
              opacity = int(255 * transparency)
              draw.rectangle([x, y, x + w, y + h], fill= (255, 0, 0) + (opacity,),
                               outline= (255, 0, 0) + (opacity,), width=5)
              
        img = Image.alpha_composite(image_data, overlay)  # Alpha composite the images
        img = img.convert("RGB")  # Convert to RGB to retain transparency
   
        return img

