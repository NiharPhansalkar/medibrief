from sparknlp_jsl.structured_deidentification import StructuredDeidentification

#### ------------------------------------- StructuredDeidentification ------------------------------------- ####

def structured_deidentifier(
            spark,
            input_file_path=None,
            output_file_path="deidentified.csv",
            separator=",",
            columns_dict=None,
            ref_source="faker",
            obfuscateRefFile=None,
            columns_seed=None,
            shift_days=None,
            date_formats=None,
            language="en"
        ):
        """This method is used to deidentify structured data. It takes the input as a file path and returns a deidentified dataframe and a file in csv/json format.

        Parameters:
        ----------

        input_file_path (str): The path of the input file.
        output_file_path (str): The path of the output file.
        separator (str): The seperator of the input csv file.
        columns_dict (dict): A dictionary that contains the column names and the tags that should be used for deidentification.
        ref_source (str): The source of the reference file. It can be "faker" or "file".
        obfuscateRefFile (str): The path of the reference file for obfuscation.
        columns_seed (int): The seed value for the random number generator.
        shift_days (int): The number of days to be shifted.
        date_formats (list): A list of date formats.
        language (str): The language used to select faker entities. Options: 'en'(English), 'de'(German), 'es'(Spanish), 'fr'(French), 'ar'(Arabic) or 'ro'(Romanian). Default:'en'.

        Returns:
        -------
        Spark DataFrame: A deidentified dataframe.
        csv/json file: A deidentified file.

        """


        if not columns_dict:
            columns_dict = {"NAME": "NAME", "AGE": "AGE"}

        if not date_formats:
            date_formats = ["dd/MM/yyyy", "dd-MM-yyyy", "d/M/yyyy", "dd-MM-yyyy", "d-M-yyyy"]

        obfuscator_args = {
            "spark": spark,
            "columns": columns_dict,
            "obfuscateRefSource": ref_source,
            "columnsSeed": columns_seed,
            "dateFormats": date_formats,
            "language": language
        }

        if ref_source == "file" or ref_source == "both":
            obfuscator_args["obfuscateRefFile"] = obfuscateRefFile

        if ref_source == "faker" and shift_days is not None:
            obfuscator_args["days"] = shift_days

        obfuscator = StructuredDeidentification(**obfuscator_args)

        try:
            input_file_type = input_file_path.split(".")[-1]

            data = spark.read.csv(
                input_file_path, header=True, sep=separator
            )

            if input_file_type == "csv":
                data = data

            elif input_file_type == "json":
                data = spark.read.format(input_file_type).load(
                    input_file_path
                )

        except ValueError:
            raise ValueError("You entered an invalid file path or file format...")

        results_df = obfuscator.obfuscateColumns(data)
        results_df_pd = results_df.toPandas()

        if input_file_type == "csv":
            results_df_pd.to_csv(f"{output_file_path}", index=False)
            print(
                f"Deidentifcation successfully completed and the results saved as '{output_file_path}' !"
            )

        elif input_file_type == "json":
            results_df_pd.to_json(f"{output_file_path}", orient="records")
            print(
                f"Deidentifcation successfully completed and the results saved as '{output_file_path}' !"
            )

        return results_df
