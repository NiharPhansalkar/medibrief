import unittest

from pyspark.sql import SparkSession
from sparknlp_jsl.utils.risk_adjustment_utils import RiskAdjustmentUtil


class RiskAdjustmentUtilTestCase(unittest.TestCase):

    def setUp(self):

        self.spark = SparkSession.builder \
            .appName("Internal RiskAdjustmentUtil Tests") \
            .master("local[*]") \
            .config("spark.driver.memory","4G") \
            .config("spark.driver.maxResultSize", "2G") \
            .config("spark.jars", "lib/sparknlp-jsl.jar,lib/sparknlp.jar") \
            .getOrCreate()

    def test_hcc_labels(self):
        result = RiskAdjustmentUtil.HCC_labels("hcc", "28", 2024, ["HCC1", "HCC37", "HCC321"])
        assert (type(result) is dict)
        assert (result["HCC1"] == "HIV/AIDS" and result["HCC37"] == "Diabetes with Chronic Complications")
        assert (not result.__contains__("HCC321"))

    def test_diff_between_HCCs(self):
        result = RiskAdjustmentUtil.diff_between_HCCs("rxhcc", "08", 2022, ["RXHCC77", "RXHCC262"], ["RXHCC1", "RXHCC78", "RXHCC261"])
        assert (type(result) is dict)
        assert (result["added_list"] == ["RXHCC1", "RXHCC78", "RXHCC261"])
        assert (result["deleted_list"] == ["RXHCC77"])

    def test_hcc_from_icd(self):
        result = RiskAdjustmentUtil.HCC_from_ICD("hcc", "28", 2024, ["A021", "I209", "E103559"])
        assert (type(result) is dict)
        assert (len(result["A021"]) == 1 and list(result["A021"]).__contains__("HCC2"))
        assert (not result.__contains__("I209"))
        assert (len(result["E103559"]) == 2)


if __name__ == '__main__':
    unittest.main()
